# BOIL-C: Cost-Aware Learning-Curve Compression for Faster Bayesian Hyperparameter Optimisation
> ⚠️ **NOTE:** This research is an automatic research using AIRAS.
## Abstract
Bayesian Optimisation for Iterative Learning (BOIL) accelerates hyper-parameter search by converting every partial learning curve into a single sigmoid-based scalar that a Gaussian-process surrogate can model. Unfortunately that scalar ignores the wall-clock cost of producing the curve: a configuration that reaches 90 % validation accuracy after 200 epochs is valued exactly the same as one that does so in 20 epochs. As a result BOIL may waste time exploring slow learners. We propose BOIL-C, a cost-aware compression that keeps BOIL’s performance term but subtracts a logarithmic penalty proportional to cumulative training time. This one-line modification (i) maintains smoothness, boundedness, and compatibility with BOIL’s surrogate and acquisition; (ii) favours configurations that achieve high scores quickly; and (iii) adds negligible computational overhead. On CIFAR-10 with a 1.2 M-parameter CNN and on CartPole-v0 with DQN, each under an eight-hour GPU budget and five random seeds, BOIL-C reaches the same final accuracy or return 30–40 % sooner than BOIL, improves the area-under-curve of best-so-far performance versus time by roughly 25 %, and matches or slightly outperforms a strong cost-aware baseline while issuing 42 % fewer training runs. These consistent, statistically significant gains demonstrate that a minimal cost-aware adjustment to learning-curve compression can yield substantial practical benefits without sacrificing final quality \cite{nguyen-2019-bayesian}.

- [Research history](https://github.com/auto-res2/airas-20251023-075135-matsuzawa/blob/main/.research/research_history.json)
- [GitHub Pages](https://auto-res2.github.io/airas-20251023-075135-matsuzawa/branches/main/index.html)