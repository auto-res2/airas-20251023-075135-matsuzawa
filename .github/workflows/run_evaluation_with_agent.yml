name: Run Evaluation with Agent

on:
  workflow_dispatch:
    inputs:
      agent_type:
        description: 'The AI agent to use for the evaluation cycle'
        required: true
        type: choice
        options:
          - open_code
          - claude_code
      run_ids:
        description: 'A JSON array of run_ids to process'
        required: true
      experiment_iteration:
        description: "Iteration count"
        required: true
      model_name:
        description: "Model to use for opencode"
        default: 'anthropic/claude-sonnet-4-5-20250929'

permissions:
  id-token: write
  contents: write

defaults:
  run:
    shell: bash

env:
  RESULTS_DIR: ".research/iteration${{ github.event.inputs.experiment_iteration }}"

  SYNC_COMMAND: "uv sync"
  EVALUATION_COMMAND: |
    set -e
    echo "=== [EVALUATION] Start at $(date -u) ==="

    uv run python -m src.evaluate \
      results_dir="$RESULTS_DIR" \
      run_ids='${{ inputs.run_ids }}'
    echo "=== [EVALUATION] PASSED at $(date -u) ==="
  PROMPT: |
    You are a fully autonomous AI research assistant.
    Your task is to ensure the evaluation script runs successfully to completion to generate comparison figures and aggregated metrics, by executing, analyzing, fixing, and re-validating it.
    You have been granted full tool access.

    Guiding Principles:
    - Scope: Do not perform any Git operations like commit or push. Your sole responsibility is to make the code runnable.
    - Method: When fixing errors, you MUST only modify existing files; do not create or delete any files.
    - Autonomy: Execute all steps autonomously. Do not ask for permission.

    Procedure:
    1.  Initial Setup: First, run `bash -c "$SYNC_COMMAND"` to install dependencies.
    2.  Run Evaluation: Execute `bash -c "$EVALUATION_COMMAND"` to generate comparison figures and aggregated metrics.
    3.  Analyze & Fix Loop: If evaluation fails, you MUST analyze the error, use your tools to fix the code, and then re-run evaluation. Repeat this cycle until it succeeds.
        A successful run is only confirmed when a message starting with `=== [EVALUATION] PASSED` is present in the output log.
    4. Visual Quality Check: Once evaluation succeeds, you MUST:
        a. Locate all generated figures in `$RESULTS_DIR`:
          - Per-run figures in `$RESULTS_DIR/{run_id}/*.pdf` (or .png)
          - Comparison figures in `$RESULTS_DIR/comparison/*.pdf` (or .png)

        b. Read and analyze EACH figure by directly viewing the PDF/PNG files (do NOT use JSON files):
          - Use the Read tool to open and view each generated PDF/PNG file
          - Visually inspect each figure for quality issues:
            - Font sizes: Are axis labels, titles, legends readable? (not too small/large)
            - Scale consistency: Do comparison plots use consistent Y-axis ranges?
            - Color distinction: Are different lines/bars easily distinguishable?
            - Layout: Is tight_layout applied? Are elements overlapping?
            - Annotations: Are important values annotated on the figures?
            - Resolution: Is the figure clear and publication-quality?

        c. If ANY figure has visual quality issues:
          - Identify the specific problem (e.g., "font size too small", "inconsistent Y-axis scales")
          - Modify `src/evaluate.py` to fix the visualization code
          - Re-run the entire evaluation from step 2
          - Repeat until ALL figures pass visual quality checks

        d. Only exit successfully when:
          - Evaluation completes without errors
          - ALL generated figures meet publication-quality standards

jobs:
  setup:
    name: Setup Workspace
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          python-version: "3.11"
          enable-cache: false

      - name: Prepare results dir
        run: mkdir -p "$RESULTS_DIR"

      - name: Upload workspace artifact
        uses: actions/upload-artifact@v4
        with:
          name: workspace
          path: .

  run_opencode:
    name: Run Agent (Open Code)
    needs: setup
    if: github.event.inputs.agent_type == 'open_code'
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - name: Download workspace artifact
        uses: actions/download-artifact@v4
        with:
          name: workspace
          path: .
      
      - name: Setup Node.js, bun, and opencode
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      - run: npm install -g bun
      - run: curl -fsSL https://opencode.ai/install | bash

      - name: Run OpenCode
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        run: |
          for i in {1..10}; do
            opencode run --model "${{ github.event.inputs.model_name }}" "$PROMPT" && break
            if [ $i -lt 10 ]; then
              echo "Attempt $i failed with exit code $?. Retrying in $((2**i)) seconds..."
              sleep $((2**i))
            else
              echo "All 10 attempts failed."
              exit 1
            fi
          done

      - name: Upload final workspace
        uses: actions/upload-artifact@v4
        with:
          name: workspace-final
          path: .

  run_claude:
    name: Run Agent (Claude Code)
    needs: setup
    if: github.event.inputs.agent_type == 'claude_code'
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - name: Download workspace artifact
        uses: actions/download-artifact@v4
        with:
          name: workspace
          path: .

      - name: Run Claude Code
        uses: anthropics/claude-code-action@v1
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          prompt: ${{ env.PROMPT }}
          claude_args: "--max-turns=100 --allowed-tools Bash,Write,Edit,MultiEdit,TodoWrite"

      - name: Upload final workspace
        uses: actions/upload-artifact@v4
        with:
          name: workspace-final
          path: .

  finalize:
    name: Finalize and Commit
    needs: [run_opencode, run_claude]
    if: always() && (needs.run_opencode.result == 'success' || needs.run_claude.result == 'success')
    runs-on: ubuntu-latest
    steps:
      - name: Download final workspace
        uses: actions/download-artifact@v4
        with:
          name: workspace-final
          path: .

      - name: Commit and push evaluation results
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git remote set-url origin "https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}"
          git add "$RESULTS_DIR"
          git add --update .

          if ! git diff --staged --quiet; then
            git commit -m "[CI] Commit evaluation results (iteration ${{ github.event.inputs.experiment_iteration }})"
            for i in {1..5}; do
              git pull --rebase && git push && break
              echo "Push failed on attempt $i. Retrying in $((2**i)) seconds..."
              sleep $((2**i))
            done
          else
            echo "No changes were made by the agent or the evaluation."
          fi
