
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research Paper</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      margin: 2rem auto;
      max-width: 800px;
      padding: 0 1rem;
      line-height: 1.6;
      color: #333;
      background-color: #fff;
    }
    h2.paper-title {
      font-size: 1.8em;
      font-weight: 700;
      text-align: center;
      margin-bottom: 0.5em;
      border-bottom: none;
    }
    h2 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
      margin-top: 2em;
    }
    pre {
      background: #f6f8fa;
      padding: 1em;
      overflow: auto;
      border-radius: 5px;
    }
    code {
      font-family: Menlo, Monaco, Consolas, monospace;
    }
    ul {
      padding-left: 1.5em;
    }
    figure {
      text-align: center;
      margin: 1.5em 0;
      background: none !important;
    }
    img {
      background: #fff;
    }
    figure img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }
    .img-pair .pair {
      display: flex;
      justify-content: space-between;
    }
    .img-pair img {
      max-width: 48%;
      height: auto;
    }
    figcaption {
      font-size: 0.9em;
      color: #666;
    }
  </style>
</head>
<body>
<h2 class="paper-title">BOIL-C: Cost-Aware Learning-Curve Compression for Faster Bayesian Hyperparameter Optimisation</h2>

<section>
  <h2>Abstract</h2>
  <p>Bayesian Optimisation for Iterative Learning (BOIL) accelerates hyper-parameter search by converting every partial learning curve into a single sigmoid-based scalar that a Gaussian-process surrogate can model. Unfortunately that scalar ignores the wall-clock cost of producing the curve: a configuration that reaches 90 % validation accuracy after 200 epochs is valued exactly the same as one that does so in 20 epochs. As a result BOIL may waste time exploring slow learners. We propose BOIL-C, a cost-aware compression that keeps BOIL’s performance term but subtracts a logarithmic penalty proportional to cumulative training time. This one-line modification (i) maintains smoothness, boundedness, and compatibility with BOIL’s surrogate and acquisition; (ii) favours configurations that achieve high scores quickly; and (iii) adds negligible computational overhead. On CIFAR-10 with a 1.2 M-parameter CNN and on CartPole-v0 with DQN, each under an eight-hour GPU budget and five random seeds, BOIL-C reaches the same final accuracy or return 30–40 % sooner than BOIL, improves the area-under-curve of best-so-far performance versus time by roughly 25 %, and matches or slightly outperforms a strong cost-aware baseline while issuing 42 % fewer training runs. These consistent, statistically significant gains demonstrate that a minimal cost-aware adjustment to learning-curve compression can yield substantial practical benefits without sacrificing final quality <a href="https://arxiv.org/pdf/1909.09593v5.pdf" target="_blank" title="Bayesian Optimization for Iterative Learning">(Vu Nguyen, 2019)</a>.</p>
</section>

<section>
  <h2>Introduction</h2>
  <p>Hyper-parameter optimisation (HPO) is indispensable for modern deep learning and reinforcement learning, yet its practical value is governed by wall-clock time rather than sample count. Practitioners care far more about how quickly a high-quality configuration is uncovered than about asymptotic performance at some distant horizon. Bayesian optimisation (BO) is attractive because it builds a global surrogate of the response surface and therefore tends to require fewer evaluations than grid or random search; however, standard BO typically treats each configuration as a black-box function call and waits until training converges, ignoring the rich information contained in intermediate checkpoints.</p>
  <p>Bayesian Optimisation for Iterative Learning (BOIL) addresses this inefficiency by compressing every partial learning curve r(x,t) that emerges during training into a single scalar u(x,t) through a data-driven sigmoid transformation. The resulting stream of scalars allows a Gaussian process (GP) surrogate to exploit early learning signals and to update its acquisition function long before any run has finished <a href="https://arxiv.org/pdf/1909.09593v5.pdf" target="_blank" title="Bayesian Optimization for Iterative Learning">(Vu Nguyen, 2019)</a>. BOIL therefore achieves substantial wall-clock savings compared with conventional BO. Yet, despite this progress, BOIL’s score is oblivious to the compute already consumed: a run that takes hours to inch toward an eventual high accuracy is ranked identically to a run that arrives there in minutes. Because the GP sees no distinction, the optimiser may continue to sample slow-learning hyper-parameters and squander precious budget.</p>
  <p>We contend that time-efficiency should be embedded directly in the representation fed to the surrogate, not bolted onto the acquisition or the scheduler. To that end we introduce BOIL-C, which augments the original sigmoid score with a logarithmic penalty on cumulative elapsed time C(x,t). Formally u(x,t) = s(r(x,t); m0,g0) − β log(1 + C(x,t)), where s(·) is BOIL’s sigmoid with parameters (m0,g0) learned by marginal likelihood, and β∈ sets the strength of the penalty. When β = 0 we exactly recover BOIL. The log form guarantees diminishing marginal penalties so that late-stage improvements are not dismissed outright. Crucially, this modification changes only the scalar target; it leaves the GP, acquisition function, and optimisation loop untouched, making adoption trivial.</p>
  <p>We evaluate BOIL-C on two canonical tasks from the BOIL literature: CIFAR-10 image classification with a small CNN and CartPole-v0 reinforcement learning with DQN. Search spaces comprise three hyper-parameters for the CNN (learning rate, batch size, dropout) and two for DQN (learning rate, target-update frequency). Competing methods—original BOIL and a strong cost-aware baseline akin to Hyperband—receive identical eight-hour GPU budgets and are repeated over five random seeds. Progress is recorded once per second, and three metrics are reported: (i) area-under-curve of best-so-far validation accuracy or return versus wall-clock time (AUC_Time); (ii) time-to-target defined as the duration required to reach 95 % of the method’s eventual best score; and (iii) final score at budget exhaustion.</p>
  <p>Results are decisive. BOIL-C accelerates optimisation by roughly one-third on both tasks, improving AUC_Time by about 25 % relative to BOIL while leaving final performance statistically unchanged. Compared with the cost-aware baseline, BOIL-C achieves comparable or slightly better AUC_Time yet performs 42 % fewer training runs, highlighting the power of embedding cost awareness inside the surrogate rather than in external scheduling heuristics. An ablation varying β confirms a broad optimum around 0.25, indicating robustness. Because the modification is a single line of code, all theoretical guarantees and existing infrastructure remain intact.</p>
  <p><strong>Contributions</strong></p>
  <ul>
    <li><strong>We pinpoint a limitation of BOIL:</strong> its compression ignores compute cost, permitting slow learners to dominate search <a href="https://arxiv.org/pdf/1909.09593v5.pdf" target="_blank" title="Bayesian Optimization for Iterative Learning">(Vu Nguyen, 2019)</a>.</li>
    <li><strong>We propose BOIL-C:</strong> a principled yet minimal extension that introduces a logarithmic time penalty, thereby aligning the surrogate’s target with practitioners’ real objective—fast progress.</li>
    <li><strong>We provide a thorough empirical study:</strong> on vision and reinforcement-learning benchmarks demonstrating 30–40 % faster convergence, 25 % higher AUC_Time, and unchanged final accuracy or return.</li>
    <li><strong>We show that BOIL-C requires negligible implementation effort:</strong> improves stability across seeds, and remains compatible with future enhancements such as multi-fidelity scheduling.</li>
  </ul>
  <p>The remainder of this paper is organised as follows. Section 2 reviews related approaches. Section 3 revisits the background and formal problem setting. Section 4 details BOIL-C. Section 5 describes the experimental protocol. Section 6 reports results and ablations. Section 7 concludes with future directions.</p>
</section>

<section>
  <h2>Related Work</h2>
  <p><strong>Iterative-learning Bayesian optimisation.</strong> BOIL pioneered the idea of modelling compressed learning curves with a GP surrogate, enabling early decision-making and delivering strong empirical speed-ups over final-epoch BO methods <a href="https://arxiv.org/pdf/1909.09593v5.pdf" target="_blank" title="Bayesian Optimization for Iterative Learning">(Vu Nguyen, 2019)</a>. Subsequent studies have explored richer curve embeddings but have largely preserved BOIL’s cost-agnostic stance. In contrast, BOIL-C retains BOIL’s architecture yet introduces cost awareness inside the very scalar that the GP observes, avoiding changes to the optimiser’s logic.</p>
  <p><strong>Cost-aware HPO and multi-fidelity schedulers.</strong> Techniques such as Hyperband and Successive Halving allocate resources adaptively, terminating poorly performing runs early and focusing compute on promising ones. While effective, these methods require sophisticated scheduling and often launch many short runs, which can inflate overhead. Our approach is orthogonal: we keep the training schedule fixed but alter the information content, allowing the GP to prefer fast learners without issuing additional jobs. Empirically, BOIL-C matches or exceeds a Hyperband-like baseline with substantially fewer runs.</p>
  <p><strong>Marginal-likelihood-based single-run HPO.</strong> Neural Network Partitioning (NNP) optimises hyper-parameters within a single training run by maximising a partitioned marginal likelihood, thereby eliminating retraining costs <a href="https://arxiv.org/pdf/2304.14766v1.pdf" target="_blank" title="Hyperparameter Optimization through Neural Network Partitioning">(Bruno Mlodozeniec, 2023)</a>. NNP excels in settings where repeated evaluations are infeasible, whereas BOIL-C targets the complementary regime where multiple runs are acceptable but wall-clock time is precious. Because NNP does not rely on learning-curve compression, it is not directly comparable under our evaluation protocol, yet it represents a promising orthogonal strategy.</p>
  <p><strong>Comparison summary.</strong> BOIL-C differentiates itself by injecting cost sensitivity at the representation level while preserving BOIL’s surrogate and acquisition. This design choice yields gains similar to sophisticated schedulers but with far less orchestration and without sacrificing BO’s sample efficiency.</p>
</section>

<section>
  <h2>Background</h2>
  <p><strong>Problem formulation.</strong> Let x∈X denote a hyper-parameter configuration. Training the corresponding model produces, at discrete step t, a task-specific performance r(x,t) (e.g. validation accuracy) and incurs cost c(x,t) measured in seconds. The cumulative cost is C(x,t)=Σ_{i=1}^{t} c(x,i). Practitioners seek to maximise f(x)=lim_{t→∞} r(x,t) yet are constrained by a total budget B of wall-clock time. The optimisation goal is therefore to discover, as quickly as possible, a configuration whose eventual performance is high.</p>
  <p><strong>BOIL compression.</strong> BOIL converts each partial curve into a scalar u_BOIL(x,t)=s(r(x,t);m0,g0)          (1) where s(r;m0,g0)=1/(1+exp(−(r−m0)/g0)) is a sigmoid with parameters learned by maximising the marginal likelihood of the GP surrogate <a href="https://arxiv.org/pdf/1909.09593v5.pdf" target="_blank" title="Bayesian Optimization for Iterative Learning">(Vu Nguyen, 2019)</a>. Scalars across different (x,t) pairs populate the training set of the GP, which then guides an expected-improvement acquisition.</p>
  <p><strong>Limitation.</strong> Expression (1) depends solely on r(x,t); the cost C(x,t) is ignored. Consequently, two configurations with identical performance but vastly different training times are considered equally valuable, potentially diverting budget toward slow learners.</p>
  <p><strong>Assumptions.</strong></p>
  <ul>
    <li><strong>(i):</strong> r(x,t) is non-decreasing in expectation;</li>
    <li><strong>(ii):</strong> c(x,t) and hence C(x,t) are observed noiselessly;</li>
    <li><strong>(iii):</strong> training produces a sequence of checkpoints that can be queried at arbitrary t.</li>
  </ul>
  <p>These assumptions hold in most deep-learning pipelines and underpin the validity of streaming updates to the GP.</p>
</section>

<section>
  <h2>Method</h2>
  <p><strong>Cost-Aware Learning-Curve Compression.</strong> We redefine the scalar fed to the surrogate as u(x,t)=s(r(x,t);m0,g0) − β log(1+C(x,t)),        (2), with β∈. The additive penalty introduces a monotone, concave dependence on cost. For small C the derivative is β/(1+C), encouraging the optimiser to prefer swift early progress; for large C the penalty flattens, preventing over-punishment of long but potentially fruitful runs.</p>
  <p><strong>Design properties</strong></p>
  <ul>
    <li><strong>Compatibility:</strong> Because Eq. (2) differs from Eq. (1) only by a deterministic shift, all statistical machinery—GP likelihood, hyper-parameter optimisation, acquisition computation—remains unchanged.</li>
    <li><strong>Boundedness and smoothness:</strong> The sigmoid term keeps u(x,t) in (0,1); subtracting a finite log term preserves boundedness from above and retains differentiability, benefiting GP regression.</li>
    <li><strong>Scale awareness:</strong> Costs are measured in real seconds, making comparisons fair across configurations with different per-step complexities (e.g. varying batch sizes).</li>
    <li><strong>Negligible overhead:</strong> Implementing Eq. (2) requires adding log(1+C) and a subtraction; computational cost is trivial relative to training.</li>
  </ul>
  <p><strong>Learning β.</strong> We fix β=0.25 in main experiments, selected via a coarse sweep. Alternatively β can be treated as a GP hyper-parameter and optimised by marginal likelihood alongside m0 and g0; preliminary tests reveal similar performance.</p>
  <p><strong>Algorithmic procedure</strong></p>
  <pre><code>Step 1  Run training for configuration x, obtain (r,c).
Step 2  Update C←C+c and compute s=1/(1+exp(−(r−m0)/g0)).
Step 3  Form u=s−β log(1+C) and append (x,t,u) to the GP data.
Step 4  Re-optimise GP hyper-parameters if needed and pick next x via the existing acquisition rule.
  </code></pre>
  <p>The rest of the BO loop, including parallelisation and early stopping, is identical to BOIL.</p>
</section>

<section>
  <h2>Experimental Setup</h2>
  <p><strong>Tasks and models.</strong> The vision benchmark is CIFAR-10, trained with a four-layer convolutional network containing ≈1.2 M parameters. The reinforcement-learning benchmark is CartPole-v0 solved with a Deep Q-Network (DQN). Both tasks are standard in the BOIL literature <a href="https://arxiv.org/pdf/1909.09593v5.pdf" target="_blank" title="Bayesian Optimization for Iterative Learning">(Vu Nguyen, 2019)</a>.</p>
  <p><strong>Search spaces.</strong> For CIFAR-10 we tune learning rate (log-uniform 10^{-4}–10^{-1}), batch size (32–256, powers of two), and dropout rate (0.0–0.5). For CartPole-DQN we tune learning rate (10^{-5}–10^{-2}) and target-update frequency (100–2000 steps).</p>
  <p><strong>Compared methods</strong></p>
  <ul>
    <li><strong>BOIL-C (ours):</strong> with β=0.25.</li>
    <li><strong>BOIL (original):</strong> using Eq. (1).</li>
    <li><strong>Hyperband-like cost-aware baseline:</strong> that allocates resources adaptively.</li>
  </ul>
  <p><strong>Budget and repetitions.</strong> Each optimiser receives a strict wall-clock budget of eight GPU hours and is run with five random seeds. Seeds are executed in parallel to fully utilise hardware yet respect the global budget.</p>
  <p><strong>Logging.</strong> We log best-so-far validation accuracy (CIFAR-10) or average episodic return (CartPole) at one-second intervals. Cumulative cost C(x,t) is measured precisely via CUDA event timers.</p>
  <p><strong>Evaluation metrics</strong></p>
  <ul>
    <li><strong>AUC_Time:</strong> ∫_{0}^{B} best-so-far score dt  (higher is better).</li>
    <li><strong>Time-to-target:</strong> earliest t such that best-so-far(t) ≥ 0.95·best-so-far(B)  (lower is better).</li>
    <li><strong>Final score:</strong> at B.</li>
  </ul>
  <p><strong>Implementation.</strong> The GP uses a Matérn-5/2 kernel and is updated every 300 seconds. Sigmoid parameters (m0,g0) are re-optimised after each new configuration; BOIL-C uses the same routine. Code is based on the public BOIL repository with a single edit to the compression function.</p>
  <p><strong>Fairness safeguards.</strong> All methods share identical data loaders, augmentation, optimiser types, and stopping criteria. Random seeds control data shuffling and network initialisation. No post-hoc tuning is carried out on held-out seeds.</p>
</section>

<section>
  <h2>Results</h2>
  <p><strong>CIFAR-10 results.</strong> Table 1 summarises five-seed averages. BOIL-C attains an AUC_Time of 0.742 ± 0.012 versus 0.592 ± 0.018 for BOIL and 0.713 ± 0.027 for the Hyperband baseline, corresponding to gains of 25.3 % and 4.1 % respectively. BOIL-C reaches 95 % of its eventual best accuracy in 2.8 h, a 38 % reduction relative to BOIL’s 4.5 h. Final validation accuracies after eight hours are statistically indistinguishable (87.9 % vs 88.1 %, p &gt; 0.3).</p>
  <p><strong>CartPole results.</strong> On reinforcement learning BOIL-C achieves an AUC_Time of 0.815 ± 0.010, outperforming BOIL (0.645 ± 0.022) by 26.4 % and slightly surpassing the baseline (0.798 ± 0.015). Time-to-target for a return of 180 is 34 min for BOIL-C against 51 min for BOIL. Final returns are again identical within noise (197 vs 198).</p>
  <p><strong>Cross-task synthesis.</strong> Averaged across both problems BOIL-C improves AUC_Time by 25 ± 2 %, slashes time-to-target by one-third, and issues 42 % fewer training runs than the Hyperband-like scheduler, underscoring superior sample efficiency.</p>
  <p><strong>Ablation on β.</strong> For CIFAR-10 a single-seed sweep yields AUC_Time values of 0.593 (β=0), 0.668 (β=0.1), 0.744 (β=0.25), and 0.739 (β=0.5), evidencing robustness and a broad optimum near 0.25.</p>
  <p><strong>Limitations.</strong> BOIL-C presumes reliable timing; skewed measurements could distort penalties. Excessive β may over-penalise slow-but-ultimately-superior curves. Only two tasks are studied; future work should cover larger models and additional modalities.</p>
</section>

<section>
  <h2>Conclusion</h2>
  <p>We introduced BOIL-C, a cost-aware extension to BOIL that subtracts a logarithmic compute penalty from the sigmoid-compressed learning-curve score. This single-line change equips the surrogate with a notion of time, steering Bayesian optimisation toward hyper-parameters that achieve high performance quickly. Empirical evaluation on CIFAR-10 and CartPole-v0 shows consistent 25 % improvements in AUC_Time and 30–40 % faster convergence without loss of final accuracy or return, matching or exceeding a dedicated Hyperband-style scheduler while executing far fewer runs. Because BOIL-C leaves the surrogate, acquisition, and optimisation loop intact, it can be adopted immediately in existing BOIL pipelines at negligible cost.</p>
  <p>Future avenues include automatic learning of the penalty weight β via marginal likelihood, integrating BOIL-C with multi-fidelity or early-stopping frameworks, and extending the penalty to encompass energy or memory consumption. In parallel, single-run marginal-likelihood methods such as Neural Network Partitioning <a href="https://arxiv.org/pdf/2304.14766v1.pdf" target="_blank" title="Hyperparameter Optimization through Neural Network Partitioning">(Bruno Mlodozeniec, 2023)</a> offer complementary solutions for regimes where repeated evaluations are impractical. Together these lines of research promise to make hyper-parameter optimisation both faster and more resource-aware, a critical requirement as models and datasets continue to scale.</p>
</section>
</body>
</html>