@misc{airas2025,
  author    = {Toma Tanaka and Takumi Matsuzawa and Yuki Yoshino and Ilya Horiguchi and Shiro Takagi and Ryutaro Yamauchi and Wataru Kumagai},
  title     = {{AIRAS}},
  year      = {2025},
  publisher = {GitHub},
  url       = {https://github.com/airas-org/airas}
}

% ===========================================
% REQUIRED CITATIONS
% These papers must be cited in the manuscript
% ===========================================

@article{mlodozeniec-2023-hyperparameter,
 abstract = {Well-tuned hyperparameters are crucial for obtaining good generalization
behavior in neural networks. They can enforce appropriate inductive biases,
regularize the model and improve performance -- especially in the presence of
limited data. In this work, we propose a simple and efficient way for
optimizing hyperparameters inspired by the marginal likelihood, an optimization
objective that requires no validation data. Our method partitions the training
data and a neural network model into $K$ data shards and parameter partitions,
respectively. Each partition is associated with and optimized only on specific
data shards. Combining these partitions into subnetworks allows us to define
the ``out-of-training-sample" loss of a subnetwork, i.e., the loss on data
shards unseen by the subnetwork, as the objective for hyperparameter
optimization. We demonstrate that we can apply this objective to optimize a
variety of different hyperparameters in a single training run while being
significantly computationally cheaper than alternative methods aiming to
optimize the marginal likelihood for neural networks. Lastly, we also focus on
optimizing hyperparameters in federated learning, where retraining and
cross-validation are particularly challenging.},
 arxiv_url = {https://arxiv.org/pdf/2304.14766v1.pdf},
 author = {Bruno Mlodozeniec and Matthias Reisser and Christos Louizos},
 title = {Hyperparameter Optimization through Neural Network Partitioning},
 year = {2023}
}

@article{nguyen-2019-bayesian,
 abstract = {The performance of deep (reinforcement) learning systems crucially depends on
the choice of hyperparameters. Their tuning is notoriously expensive, typically
requiring an iterative training process to run for numerous steps to
convergence. Traditional tuning algorithms only consider the final performance
of hyperparameters acquired after many expensive iterations and ignore
intermediate information from earlier training steps. In this paper, we present
a Bayesian optimization (BO) approach which exploits the iterative structure of
learning algorithms for efficient hyperparameter tuning. We propose to learn an
evaluation function compressing learning progress at any stage of the training
process into a single numeric score according to both training success and
stability. Our BO framework is then balancing the benefit of assessing a
hyperparameter setting over additional training steps against their computation
cost. We further increase model efficiency by selectively including scores from
different training steps for any evaluated hyperparameter set. We demonstrate
the efficiency of our algorithm by tuning hyperparameters for the training of
deep reinforcement learning agents and convolutional neural networks. Our
algorithm outperforms all existing baselines in identifying optimal
hyperparameters in minimal time.},
 arxiv_url = {https://arxiv.org/pdf/1909.09593v5.pdf},
 author = {Vu Nguyen and Sebastian Schulze and Michael A Osborne},
 github_url = {https://github.com/ntienvu/BOIL},
 title = {Bayesian Optimization for Iterative Learning},
 year = {2019}
}

% ===========================================
% REFERENCE CANDIDATES
% Additional reference papers for context
% ===========================================

@article{author-year-pattern,
 title = {Pattern recognition and machine learning}
}

% ===========================================
% REQUIRED CITATIONS
% These papers must be cited in the manuscript
% ===========================================

@article{mlodozeniec-2023-hyperparameter,
 abstract = {Well-tuned hyperparameters are crucial for obtaining good generalization
behavior in neural networks. They can enforce appropriate inductive biases,
regularize the model and improve performance -- especially in the presence of
limited data. In this work, we propose a simple and efficient way for
optimizing hyperparameters inspired by the marginal likelihood, an optimization
objective that requires no validation data. Our method partitions the training
data and a neural network model into $K$ data shards and parameter partitions,
respectively. Each partition is associated with and optimized only on specific
data shards. Combining these partitions into subnetworks allows us to define
the ``out-of-training-sample" loss of a subnetwork, i.e., the loss on data
shards unseen by the subnetwork, as the objective for hyperparameter
optimization. We demonstrate that we can apply this objective to optimize a
variety of different hyperparameters in a single training run while being
significantly computationally cheaper than alternative methods aiming to
optimize the marginal likelihood for neural networks. Lastly, we also focus on
optimizing hyperparameters in federated learning, where retraining and
cross-validation are particularly challenging.},
 arxiv_url = {https://arxiv.org/pdf/2304.14766v1.pdf},
 author = {Bruno Mlodozeniec and Matthias Reisser and Christos Louizos},
 title = {Hyperparameter Optimization through Neural Network Partitioning},
 year = {2023}
}

@article{nguyen-2019-bayesian,
 abstract = {The performance of deep (reinforcement) learning systems crucially depends on
the choice of hyperparameters. Their tuning is notoriously expensive, typically
requiring an iterative training process to run for numerous steps to
convergence. Traditional tuning algorithms only consider the final performance
of hyperparameters acquired after many expensive iterations and ignore
intermediate information from earlier training steps. In this paper, we present
a Bayesian optimization (BO) approach which exploits the iterative structure of
learning algorithms for efficient hyperparameter tuning. We propose to learn an
evaluation function compressing learning progress at any stage of the training
process into a single numeric score according to both training success and
stability. Our BO framework is then balancing the benefit of assessing a
hyperparameter setting over additional training steps against their computation
cost. We further increase model efficiency by selectively including scores from
different training steps for any evaluated hyperparameter set. We demonstrate
the efficiency of our algorithm by tuning hyperparameters for the training of
deep reinforcement learning agents and convolutional neural networks. Our
algorithm outperforms all existing baselines in identifying optimal
hyperparameters in minimal time.},
 arxiv_url = {https://arxiv.org/pdf/1909.09593v5.pdf},
 author = {Vu Nguyen and Sebastian Schulze and Michael A Osborne},
 github_url = {https://github.com/ntienvu/BOIL},
 title = {Bayesian Optimization for Iterative Learning},
 year = {2019}
}

% ===========================================
% REFERENCE CANDIDATES
% Additional reference papers for context
% ===========================================

@article{author-year-pattern,
 title = {Pattern recognition and machine learning}
}

% ===========================================
% REQUIRED CITATIONS
% These papers must be cited in the manuscript
% ===========================================

@article{mlodozeniec-2023-hyperparameter,
 abstract = {Well-tuned hyperparameters are crucial for obtaining good generalization
behavior in neural networks. They can enforce appropriate inductive biases,
regularize the model and improve performance -- especially in the presence of
limited data. In this work, we propose a simple and efficient way for
optimizing hyperparameters inspired by the marginal likelihood, an optimization
objective that requires no validation data. Our method partitions the training
data and a neural network model into $K$ data shards and parameter partitions,
respectively. Each partition is associated with and optimized only on specific
data shards. Combining these partitions into subnetworks allows us to define
the ``out-of-training-sample" loss of a subnetwork, i.e., the loss on data
shards unseen by the subnetwork, as the objective for hyperparameter
optimization. We demonstrate that we can apply this objective to optimize a
variety of different hyperparameters in a single training run while being
significantly computationally cheaper than alternative methods aiming to
optimize the marginal likelihood for neural networks. Lastly, we also focus on
optimizing hyperparameters in federated learning, where retraining and
cross-validation are particularly challenging.},
 arxiv_url = {https://arxiv.org/pdf/2304.14766v1.pdf},
 author = {Bruno Mlodozeniec and Matthias Reisser and Christos Louizos},
 title = {Hyperparameter Optimization through Neural Network Partitioning},
 year = {2023}
}

@article{nguyen-2019-bayesian,
 abstract = {The performance of deep (reinforcement) learning systems crucially depends on
the choice of hyperparameters. Their tuning is notoriously expensive, typically
requiring an iterative training process to run for numerous steps to
convergence. Traditional tuning algorithms only consider the final performance
of hyperparameters acquired after many expensive iterations and ignore
intermediate information from earlier training steps. In this paper, we present
a Bayesian optimization (BO) approach which exploits the iterative structure of
learning algorithms for efficient hyperparameter tuning. We propose to learn an
evaluation function compressing learning progress at any stage of the training
process into a single numeric score according to both training success and
stability. Our BO framework is then balancing the benefit of assessing a
hyperparameter setting over additional training steps against their computation
cost. We further increase model efficiency by selectively including scores from
different training steps for any evaluated hyperparameter set. We demonstrate
the efficiency of our algorithm by tuning hyperparameters for the training of
deep reinforcement learning agents and convolutional neural networks. Our
algorithm outperforms all existing baselines in identifying optimal
hyperparameters in minimal time.},
 arxiv_url = {https://arxiv.org/pdf/1909.09593v5.pdf},
 author = {Vu Nguyen and Sebastian Schulze and Michael A Osborne},
 github_url = {https://github.com/ntienvu/BOIL},
 title = {Bayesian Optimization for Iterative Learning},
 year = {2019}
}

% ===========================================
% REFERENCE CANDIDATES
% Additional reference papers for context
% ===========================================

@article{author-year-pattern,
 title = {Pattern recognition and machine learning}
}

% ===========================================
% REQUIRED CITATIONS
% These papers must be cited in the manuscript
% ===========================================

@article{mlodozeniec-2023-hyperparameter,
 abstract = {Well-tuned hyperparameters are crucial for obtaining good generalization
behavior in neural networks. They can enforce appropriate inductive biases,
regularize the model and improve performance -- especially in the presence of
limited data. In this work, we propose a simple and efficient way for
optimizing hyperparameters inspired by the marginal likelihood, an optimization
objective that requires no validation data. Our method partitions the training
data and a neural network model into $K$ data shards and parameter partitions,
respectively. Each partition is associated with and optimized only on specific
data shards. Combining these partitions into subnetworks allows us to define
the ``out-of-training-sample" loss of a subnetwork, i.e., the loss on data
shards unseen by the subnetwork, as the objective for hyperparameter
optimization. We demonstrate that we can apply this objective to optimize a
variety of different hyperparameters in a single training run while being
significantly computationally cheaper than alternative methods aiming to
optimize the marginal likelihood for neural networks. Lastly, we also focus on
optimizing hyperparameters in federated learning, where retraining and
cross-validation are particularly challenging.},
 arxiv_url = {https://arxiv.org/pdf/2304.14766v1.pdf},
 author = {Bruno Mlodozeniec and Matthias Reisser and Christos Louizos},
 title = {Hyperparameter Optimization through Neural Network Partitioning},
 year = {2023}
}

@article{nguyen-2019-bayesian,
 abstract = {The performance of deep (reinforcement) learning systems crucially depends on
the choice of hyperparameters. Their tuning is notoriously expensive, typically
requiring an iterative training process to run for numerous steps to
convergence. Traditional tuning algorithms only consider the final performance
of hyperparameters acquired after many expensive iterations and ignore
intermediate information from earlier training steps. In this paper, we present
a Bayesian optimization (BO) approach which exploits the iterative structure of
learning algorithms for efficient hyperparameter tuning. We propose to learn an
evaluation function compressing learning progress at any stage of the training
process into a single numeric score according to both training success and
stability. Our BO framework is then balancing the benefit of assessing a
hyperparameter setting over additional training steps against their computation
cost. We further increase model efficiency by selectively including scores from
different training steps for any evaluated hyperparameter set. We demonstrate
the efficiency of our algorithm by tuning hyperparameters for the training of
deep reinforcement learning agents and convolutional neural networks. Our
algorithm outperforms all existing baselines in identifying optimal
hyperparameters in minimal time.},
 arxiv_url = {https://arxiv.org/pdf/1909.09593v5.pdf},
 author = {Vu Nguyen and Sebastian Schulze and Michael A Osborne},
 github_url = {https://github.com/ntienvu/BOIL},
 title = {Bayesian Optimization for Iterative Learning},
 year = {2019}
}

% ===========================================
% REFERENCE CANDIDATES
% Additional reference papers for context
% ===========================================

@article{author-year-pattern,
 title = {Pattern recognition and machine learning}
}

% ===========================================
% REQUIRED CITATIONS
% These papers must be cited in the manuscript
% ===========================================

@article{mlodozeniec-2023-hyperparameter,
 abstract = {Well-tuned hyperparameters are crucial for obtaining good generalization
behavior in neural networks. They can enforce appropriate inductive biases,
regularize the model and improve performance -- especially in the presence of
limited data. In this work, we propose a simple and efficient way for
optimizing hyperparameters inspired by the marginal likelihood, an optimization
objective that requires no validation data. Our method partitions the training
data and a neural network model into $K$ data shards and parameter partitions,
respectively. Each partition is associated with and optimized only on specific
data shards. Combining these partitions into subnetworks allows us to define
the ``out-of-training-sample" loss of a subnetwork, i.e., the loss on data
shards unseen by the subnetwork, as the objective for hyperparameter
optimization. We demonstrate that we can apply this objective to optimize a
variety of different hyperparameters in a single training run while being
significantly computationally cheaper than alternative methods aiming to
optimize the marginal likelihood for neural networks. Lastly, we also focus on
optimizing hyperparameters in federated learning, where retraining and
cross-validation are particularly challenging.},
 arxiv_url = {https://arxiv.org/pdf/2304.14766v1.pdf},
 author = {Bruno Mlodozeniec and Matthias Reisser and Christos Louizos},
 title = {Hyperparameter Optimization through Neural Network Partitioning},
 year = {2023}
}

@article{nguyen-2019-bayesian,
 abstract = {The performance of deep (reinforcement) learning systems crucially depends on
the choice of hyperparameters. Their tuning is notoriously expensive, typically
requiring an iterative training process to run for numerous steps to
convergence. Traditional tuning algorithms only consider the final performance
of hyperparameters acquired after many expensive iterations and ignore
intermediate information from earlier training steps. In this paper, we present
a Bayesian optimization (BO) approach which exploits the iterative structure of
learning algorithms for efficient hyperparameter tuning. We propose to learn an
evaluation function compressing learning progress at any stage of the training
process into a single numeric score according to both training success and
stability. Our BO framework is then balancing the benefit of assessing a
hyperparameter setting over additional training steps against their computation
cost. We further increase model efficiency by selectively including scores from
different training steps for any evaluated hyperparameter set. We demonstrate
the efficiency of our algorithm by tuning hyperparameters for the training of
deep reinforcement learning agents and convolutional neural networks. Our
algorithm outperforms all existing baselines in identifying optimal
hyperparameters in minimal time.},
 arxiv_url = {https://arxiv.org/pdf/1909.09593v5.pdf},
 author = {Vu Nguyen and Sebastian Schulze and Michael A Osborne},
 github_url = {https://github.com/ntienvu/BOIL},
 title = {Bayesian Optimization for Iterative Learning},
 year = {2019}
}

% ===========================================
% REFERENCE CANDIDATES
% Additional reference papers for context
% ===========================================

@article{benton-2020-learning,
 abstract = {Invariances to translations have imbued convolutional neural networks with
powerful generalization properties. However, we often do not know a priori what
invariances are present in the data, or to what extent a model should be
invariant to a given symmetry group. We show how to \emph{learn} invariances
and equivariances by parameterizing a distribution over augmentations and
optimizing the training loss simultaneously with respect to the network
parameters and augmentation parameters. With this simple procedure we can
recover the correct set and extent of invariances on image classification,
regression, segmentation, and molecular property prediction from a large space
of augmentations, on training data alone.},
 arxiv_url = {https://arxiv.org/pdf/2010.11882v2.pdf},
 author = {Gregory Benton and Marc Finzi and Pavel Izmailov and Andrew Gordon Wilson},
 title = {Learning invariances in neural networks from training data},
 year = {2020}
}