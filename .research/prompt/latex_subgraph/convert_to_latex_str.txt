
LLM Name: gpt-5-2025-08-07
Input:

You are a LaTeX expert.
Your task is to convert each section of a research paper into plain LaTeX **content only**, without including any section titles or metadata.

Below are the paper sections. For each one, convert only the **content** into LaTeX:

---
Section: title

BOIL-C: Cost-Aware Learning-Curve Compression for Faster Bayesian Hyperparameter Optimisation

---

---
Section: abstract

Bayesian Optimisation for Iterative Learning (BOIL) accelerates hyper-parameter search by feeding a Gaussian-process surrogate with single-number summaries of partial learning curves. Unfortunately these summaries ignore the wall-clock time required to reach a given accuracy, so BOIL often oversamples configurations that learn slowly yet eventually perform well, wasting compute under tight budgets. We propose BOIL-C, a one-line modification that subtracts a logarithmic cost penalty from BOIL’s sigmoid score: u(x,t)=s(r(x,t);m0,g0)−β·log(1+C(x,t)), where r is the observed performance at step t and C the cumulative training time. The surrogate, acquisition function and optimisation loop remain unchanged. Experiments on CIFAR-10 with a 1.2 M-parameter CNN and on CartPole-v0 with DQN compare BOIL-C to the original BOIL and to Hyperband under identical eight-hour GPU budgets and five seeds. BOIL-C matches or slightly improves final accuracy/return yet increases the area-under-best-so-far-score versus time by ≈25 % and reaches target quality 30–40 % faster. An ablation shows robust gains for β∈; paired t-tests confirm significant improvements in time-efficiency without harming asymptotic performance. Thus a principled, minimal compute penalty restores time awareness to BOIL and delivers meaningful real-world savings.

---

---
Section: introduction

Hyper-parameter optimisation (HPO) is indispensable for state-of-the-art performance in deep learning and reinforcement learning, but its computational cost remains prohibitive. Bayesian Optimisation for Iterative Learning (BOIL) addresses this by compressing partial learning curves into scalars consumed by a Gaussian-process (GP) surrogate, enabling the optimiser to exploit intermediate training signals instead of waiting for full convergence \cite{nguyen-2019-bayesian}. Despite its efficiency, BOIL’s scalar score depends solely on accuracy and stability, ignoring the wall-clock cost spent to achieve that accuracy. Consequently, BOIL may repeatedly select configurations that learn slowly but ultimately perform well, squandering precious compute when a fixed budget is imposed.

We introduce BOIL-C, a cost-aware learning-curve compression scheme that augments BOIL’s score with a logarithmic penalty on cumulative training time. The modification is minimal—a single subtraction—yet it endows the GP surrogate with an explicit preference for configurations that achieve high accuracy quickly. Designing such a penalty is challenging: if the cost term is too strong, the optimiser may discard ultimately superior but slower configurations; if too weak, search behaviour remains unchanged. BOIL-C solves this by using a sub-linear log(1+C) penalty whose magnitude is controlled by one coefficient β.

We validate BOIL-C on two representative tasks: CIFAR-10 image classification with a 1.2 M-parameter convolutional network and CartPole-v0 reinforcement learning with DQN. We compare against BOIL and the cost-aware bandit baseline Hyperband under identical eight-hour GPU budgets and five independent seeds. Performance is measured both at budget exhaustion and throughout the run via the area under the best-so-far curve versus time (AUC-Time). BOIL-C achieves the same or slightly higher final accuracy/return while delivering≈25 % higher AUC-Time and attaining target quality 30–40 % sooner.

Contributions:
• We propose BOIL-C, a cost-aware scalar compression that requires only a one-line change to BOIL.
• We present thorough experiments on vision and RL tasks showing consistent improvements in anytime performance without sacrificing final quality.
• We provide an ablation over β and statistical tests confirming the significance and robustness of the observed gains.
• BOIL-C complements broader efficiency efforts such as partition-based HPO \cite{mlodozeniec-2023-hyperparameter} and demonstrates how minimal, principled modifications can yield tangible compute savings.

---

---
Section: related_work

Learning-curve-aware Bayesian optimisation BOIL compresses partial training trajectories into sigmoid scores that quantify accuracy and stability, allowing a GP surrogate to reason about progress and stop unpromising runs early \cite{nguyen-2019-bayesian}. BOIL-C preserves this framework but augments the score with an explicit cost term. Alternative learning-curve models predict future accuracy directly, yet they typically require bespoke surrogates and do not inject cost at the compression stage.

Cost-aware resource allocation Hyperband and successors allocate budgets adaptively, trading early stopping against exploration through non-parametric bandit rules. These schedulers are inherently time-aware, often achieving strong anytime performance, but they forgo parametric surrogates and thus require many full or partial trainings. BOIL-C bridges this gap by embedding compute considerations inside a GP-based BO framework, combining sample efficiency with time awareness.

Alternative HPO objectives Partition-based optimisation approximates marginal likelihood by splitting data and parameters, enabling validation-free tuning \cite{mlodozeniec-2023-hyperparameter}. Although orthogonal to compute penalties, such objectives could be combined with BOIL-C. Work on learning invariances jointly with model parameters \cite{benton-2020-learning} illustrates the wider trend of integrating auxiliary objectives directly into training; BOIL-C follows the same philosophy for wall-clock cost.

Compared to these approaches, BOIL-C is distinguished by its simplicity: one extra term in the compression suffices to convert BOIL into a cost-aware optimiser while leaving the surrogate and acquisition untouched.

---

---
Section: background

Problem setting Given a configuration x and training step t, let r(x,t) denote the validation performance and c(x,i) the wall-clock seconds consumed by step i. The cumulative cost is C(x,t)=∑_{i=1}^{t}c(x,i). Under a fixed budget B, the optimiser iteratively (1) trains a chosen configuration, (2) compresses the partial curve to a scalar, (3) updates the GP surrogate, and (4) selects the next configuration via expected improvement.

BOIL compression The original BOIL maps r(x,t) to a sigmoid score s(r(x,t);m0,g0)=1/(1+exp(−(r−m0)/g0)), where m0 and g0 are learned by maximising GP marginal likelihood \cite{nguyen-2019-bayesian}. This score is agnostic to elapsed time, making the optimiser blind to computational efficiency.

Design goal We seek a modified scalar that (i) rewards accuracy, (ii) penalises cost, (iii) grows smoothly, and (iv) preserves BOIL’s GP machinery. A logarithmic penalty satisfies these criteria: it is zero at zero cost, sub-linear, and numerically stable via log(1+C).

---

---
Section: method

Cost-aware compression For each partial run we compute
u(x,t)=s(r(x,t);m0,g0)−β·log(1+C(x,t)),
where β∈ weighs compute against accuracy. The term log(1+C) ensures diminishing penalisation and avoids instability at small costs.

Surrogate and acquisition The GP surrogate, Matérn-5/2 kernel, observation noise, and expected-improvement acquisition remain exactly as in BOIL. Only the target values fed to the surrogate change from s to u. Parameters m0 and g0 continue to be learned by marginal likelihood; β is fixed to 0.25 unless stated, but could also be estimated jointly.

Implementation The modification amounts to replacing one line in the compression routine (see listing in the code appendix). No other code changes are required.

---

---
Section: experimental_setup

Datasets and models (1) CIFAR-10 classification using a four-layer CNN with ≈1.2 M parameters; standard random-crop and flip augmentation, 45 k/5 k/10 k train/val/test split. (2) CartPole-v0 reinforcement learning with DQN, following the original BOIL setup.

Search spaces CIFAR-10: learning rate log-uniform in , batch size ∈{32,64,128}, dropout uniform in . CartPole: learning rate and target-update frequency as in \cite{nguyen-2019-bayesian}.

Budgets and hardware Each optimiser (BOIL-C, BOIL, Hyperband) receives eight physical GPU-hours and is run with five independent seeds on NVIDIA A100 hardware. Seeds are distributed across available GPUs to exhaust the budget. We record the best-so-far validation accuracy (or return) every second and integrate these traces to obtain AUC-Time.

Training protocol CIFAR-10 models train for up to 200 epochs with SGD (momentum 0.9, weight decay 5×10⁻⁴) and a cosine schedule. Checkpoints are saved each epoch. Hyper-parameter trials are orchestrated by Optuna with a TPE sampler and median pruner; BOIL variants share GP hyper-parameters (Matérn-5/2 kernel, noise 10⁻³) and acquisition settings.

Evaluation metrics Primary: AUC-Time (higher is better). Secondary: final validation and test accuracy (CIFAR-10) or return (CartPole) at budget exhaustion. Fairness: identical budgets, seeds, and search spaces across methods.

---

---
Section: results

CIFAR-10 BOIL-C achieves a final test accuracy of 0.922 versus 0.9183 for BOIL. More importantly, BOIL-C improves AUC-Time by 25.8 % and reaches 90 % validation accuracy in 27.3 min versus 45.8 min for BOIL. Hyperband attains similar AUC-Time but requires roughly twice as many partial trainings.

CartPole-v0 BOIL-C matches BOIL’s final return (199.2 vs 198.8) yet delivers a 32 % higher AUC-Time and converges 30 % sooner. It slightly outperforms Hyperband while evaluating 43 % fewer full training runs.

Ablation on β Sweeping β on CIFAR-10 yields AUC-Time values of 5.9×10⁴ (β=0, BOIL), 6.8×10⁴ (β=0.15), 7.45×10⁴ (β=0.25), and 7.3×10⁴ (β=0.40), demonstrating robustness for β∈.

Statistical significance Paired t-tests over five seeds show significant AUC-Time improvements: t=5.12, p=0.003 (CIFAR-10) and t=4.41, p=0.005 (CartPole). No significant difference appears in final accuracy/return (p>0.4), confirming that BOIL-C accelerates convergence without loss of peak quality.

Limitations BOIL-C introduces one hyper-parameter β that may need tuning across domains; extremely long plateaus might benefit from alternative penalty shapes. Compute measurements are assumed reliable; excessive measurement noise could dilute the penalty.

Figures
Figure 1: Confusion matrix of the final BOIL-C CIFAR-10 model (filename: proposed-Small-CNN-1.2M-CIFAR-10_confusion_matrix.pdf). Higher diagonal entries indicate better classification.
Figure 2: Best-so-far validation accuracy versus time for BOIL-C on CIFAR-10 (filename: proposed-Small-CNN-1.2M-CIFAR-10_learning_curve.pdf). Higher curves indicate better and earlier performance.
Figure 3: Confusion matrix of the final BOIL CIFAR-10 model (filename: comparative-1-Small-CNN-1.2M-CIFAR-10_confusion_matrix.pdf). Higher diagonal entries indicate better classification.
Figure 4: Best-so-far validation accuracy versus time for BOIL on CIFAR-10 (filename: comparative-1-Small-CNN-1.2M-CIFAR-10_learning_curve.pdf). Higher curves indicate better and earlier performance.
Figure 5: Final test accuracy across methods (filename: comparison_accuracy_bar_chart.pdf). Higher bars are better.
Figure 6: Distribution of final test accuracy across seeds (filename: comparison_accuracy_boxplot.pdf). Higher boxes are better.
Figure 7: Relative AUC-Time improvement of BOIL-C over BOIL (filename: comparison_relative_improvement_bar_chart.pdf). Higher bars indicate greater efficiency gains.

---

---
Section: conclusion

We presented BOIL-C, a cost-aware extension of BOIL that subtracts a logarithmic penalty in cumulative compute from the learning-curve scalar fed to the GP surrogate. This one-line change preserves BOIL’s modelling and acquisition logic while introducing a principled preference for fast-learning configurations. Across CIFAR-10 and CartPole-v0, BOIL-C maintains or slightly improves final accuracy/return yet delivers≈25 % higher AUC-Time and 30–40 % faster time-to-target quality, outperforming the original BOIL and matching or surpassing Hyperband with far fewer trainings. These gains are statistically significant and robust across β∈.

Future work includes learning β jointly with the sigmoid parameters, exploring alternative cost penalties, and applying BOIL-C to larger parameter spaces and tasks. Because BOIL-C is orthogonal to objectives such as partition-based marginal-likelihood optimisation \cite{mlodozeniec-2023-hyperparameter} and retains compatibility with BOIL’s GP framework \cite{nguyen-2019-bayesian}, it offers a practical drop-in upgrade for time-efficient hyper-parameter search in compute-constrained settings.

---


## LaTeX Formatting Rules:
- Use \subsection{...} for any subsections within this section.
    - Subsection titles should be distinct from the section name;
    - Do not use '\subsection{  }', or other slight variations. Use more descriptive and unique titles.
    - Avoid excessive subdivision. If a subsection is brief or overlaps significantly with another, consider merging them for clarity and flow.

- For listing contributions, use the LaTeX \begin{itemize}...\end{itemize} format.
    - Each item should start with a short title in \textbf{...} format.
    - Avoid using -, *, or other Markdown bullet styles.

- When including tables, use the `tabularx` environment with `\textwidth` as the target width.
    - At least one column must use the `X` type to enable automatic width adjustment and line breaking.
    - Include `\hline` at the top, after the header, and at the bottom. Avoid vertical lines unless necessary.
    - To left-align content in `X` columns, define `
ewcolumntype{Y}{>{
aggedrightrraybackslash}X}` using the `array` package.

- When writing pseudocode, use the `algorithm` and `algorithmicx` LaTeX environments.
    - Only include pseudocode in the `Method` section. Pseudocode is not allowed in any other sections.
    - Prefer the `\begin{algorithmic}` environment using **lowercase commands** such as `\State`, `\For`, and `\If`, to ensure compatibility and clean formatting.
    - Pseudocode must represent actual algorithms or procedures with clear logic. Do not use pseudocode to simply rephrase narrative descriptions or repeat what has already been explained in text.
        - Good Example:
        ```latex
        \State Compute transformed tokens: \(	ilde{T} \leftarrow W\,T\)
        \State Update: \(T_{new} \leftarrow 	ilde{T} + \mu\,T_{prev}\)
        ```
- Figures and images are ONLY allowed in the "Results" section.
    - Use LaTeX float option `[H]` to force placement.

- All figures must be inserted using the following LaTeX format, using a `width` that reflects the filename:
    ```latex
    \includegraphics[width=\linewidth]{ images/filename.pdf }
    ```
    The `<appropriate-width>` must be selected based on the filename suffix:
    - If the filename ends with _pair1.pdf or _pair2.pdf, use 0.48\linewidth as the width of each subfigure environment and place the figures side by side using `subcaption` package.
    - Otherwise (default), use 0.7\linewidth

- **Escaping special characters**:
    - LaTeX special characters (`#`, `$`, `%`, `&`, `~`, `_`, `^`, `{`, `}`, `\`) must be escaped with a leading backslash when they appear in plain text (e.g., `data\_set`, `C\&C`).
    - Underscores **must always be escaped** (`\_`) outside math mode, even in filenames (e.g., memory\_profiler), code-style words, itemize lists, or citation contexts.

- Always use ASCII hyphens (`-`) instead of en-dashes (`–`) or em-dashes (`—`) to avoid spacing issues in hyphenated terms.
- Do not include any of these higher-level commands such as \documentclass{...}, \begin{document}, and \end{document}.
    - Additionally, avoid including section-specific commands such as \begin{abstract}, \section{  }, or any other similar environment definitions.
- Do not modify citation placeholders:
    - Citation placeholders appear in the format [citation_key], where citation_key contains underscores, numbers, and text (e.g., [vaswani_2017_attention], [smith_2023_deep]).
    - You must preserve these placeholders EXACTLY as they appear in the input text.
- If no LaTeX conversion is needed, output the content as-is without status messages like [Unchanged].
Output:
{
    "title": "BOIL-C: Cost-Aware Learning-Curve Compression for Faster Bayesian Hyperparameter Optimisation",
    "abstract": "Bayesian Optimisation for Iterative Learning (BOIL) accelerates hyper-parameter search by feeding a Gaussian-process surrogate with single-number summaries of partial learning curves. Unfortunately these summaries ignore the wall-clock time required to reach a given accuracy, so BOIL often oversamples configurations that learn slowly yet eventually perform well, wasting compute under tight budgets. We propose BOIL-C, a one-line modification that subtracts a logarithmic cost penalty from BOIL's sigmoid score: \\(u(x,t)=s(r(x,t);m_0,g_0)-\\beta\\cdot\\log(1+C(x,t))\\), where \\(r\\) is the observed performance at step \\(t\\) and \\(C\\) the cumulative training time. The surrogate, acquisition function and optimisation loop remain unchanged. Experiments on CIFAR-10 with a 1.2 M-parameter CNN and on CartPole-v0 with DQN compare BOIL-C to the original BOIL and to Hyperband under identical eight-hour GPU budgets and five seeds. BOIL-C matches or slightly improves final accuracy/return yet increases the area-under-best-so-far-score versus time by \\(\\approx 25\\,\\%\\) and reaches target quality 30-40\\,\\% faster. An ablation shows robust gains for \\(\\beta\\in\\); paired t-tests confirm significant improvements in time-efficiency without harming asymptotic performance. Thus a principled, minimal compute penalty restores time awareness to BOIL and delivers meaningful real-world savings.",
    "introduction": "Hyper-parameter optimisation (HPO) is indispensable for state-of-the-art performance in deep learning and reinforcement learning, but its computational cost remains prohibitive. Bayesian Optimisation for Iterative Learning (BOIL) addresses this by compressing partial learning curves into scalars consumed by a Gaussian-process (GP) surrogate, enabling the optimiser to exploit intermediate training signals instead of waiting for full convergence \\cite{nguyen-2019-bayesian}. Despite its efficiency, BOIL's scalar score depends solely on accuracy and stability, ignoring the wall-clock cost spent to achieve that accuracy. Consequently, BOIL may repeatedly select configurations that learn slowly but ultimately perform well, squandering precious compute when a fixed budget is imposed.\n\nWe introduce BOIL-C, a cost-aware learning-curve compression scheme that augments BOIL's score with a logarithmic penalty on cumulative training time. The modification is minimal - a single subtraction - yet it endows the GP surrogate with an explicit preference for configurations that achieve high accuracy quickly. Designing such a penalty is challenging: if the cost term is too strong, the optimiser may discard ultimately superior but slower configurations; if too weak, search behaviour remains unchanged. BOIL-C solves this by using a sub-linear \\(\\log(1+C)\\) penalty whose magnitude is controlled by one coefficient \\(\\beta\\).\n\nWe validate BOIL-C on two representative tasks: CIFAR-10 image classification with a 1.2 M-parameter convolutional network and CartPole-v0 reinforcement learning with DQN. We compare against BOIL and the cost-aware bandit baseline Hyperband under identical eight-hour GPU budgets and five independent seeds. Performance is measured both at budget exhaustion and throughout the run via the area under the best-so-far curve versus time (AUC-Time). BOIL-C achieves the same or slightly higher final accuracy/return while delivering \\(\\approx 25\\,\\%\\) higher AUC-Time and attaining target quality 30-40\\,\\% sooner.\n\n\\begin{itemize}\n\\item \\textbf{Cost-aware compression:} We propose BOIL-C, a cost-aware scalar compression that requires only a one-line change to BOIL.\n\\item \\textbf{Anytime gains:} We present thorough experiments on vision and RL tasks showing consistent improvements in anytime performance without sacrificing final quality.\n\\item \\textbf{Robustness and significance:} We provide an ablation over \\(\\beta\\) and statistical tests confirming the significance and robustness of the observed gains.\n\\item \\textbf{Complementary to other efficiencies:} BOIL-C complements broader efficiency efforts such as partition-based HPO \\cite{mlodozeniec-2023-hyperparameter} and demonstrates how minimal, principled modifications can yield tangible compute savings.\n\\end{itemize}",
    "related_work": "\\subsection{Learning-curve-aware Bayesian optimisation}\nBOIL compresses partial training trajectories into sigmoid scores that quantify accuracy and stability, allowing a GP surrogate to reason about progress and stop unpromising runs early \\cite{nguyen-2019-bayesian}. BOIL-C preserves this framework but augments the score with an explicit cost term. Alternative learning-curve models predict future accuracy directly, yet they typically require bespoke surrogates and do not inject cost at the compression stage.\n\n\\subsection{Cost-aware resource allocation}\nHyperband and successors allocate budgets adaptively, trading early stopping against exploration through non-parametric bandit rules. These schedulers are inherently time-aware, often achieving strong anytime performance, but they forgo parametric surrogates and thus require many full or partial trainings. BOIL-C bridges this gap by embedding compute considerations inside a GP-based BO framework, combining sample efficiency with time awareness.\n\n\\subsection{Alternative HPO objectives}\nPartition-based optimisation approximates marginal likelihood by splitting data and parameters, enabling validation-free tuning \\cite{mlodozeniec-2023-hyperparameter}. Although orthogonal to compute penalties, such objectives could be combined with BOIL-C. Work on learning invariances jointly with model parameters \\cite{benton-2020-learning} illustrates the wider trend of integrating auxiliary objectives directly into training; BOIL-C follows the same philosophy for wall-clock cost.\n\nCompared to these approaches, BOIL-C is distinguished by its simplicity: one extra term in the compression suffices to convert BOIL into a cost-aware optimiser while leaving the surrogate and acquisition untouched.",
    "background": "\\subsection{Problem setting}\nGiven a configuration \\(x\\) and training step \\(t\\), let \\(r(x,t)\\) denote the validation performance and \\(c(x,i)\\) the wall-clock seconds consumed by step \\(i\\). The cumulative cost is \\(C(x,t)=\\sum_{i=1}^{t}c(x,i)\\). Under a fixed budget \\(B\\), the optimiser iteratively (1) trains a chosen configuration, (2) compresses the partial curve to a scalar, (3) updates the GP surrogate, and (4) selects the next configuration via expected improvement.\n\n\\subsection{BOIL compression}\nThe original BOIL maps \\(r(x,t)\\) to a sigmoid score \\(s(r(x,t);m_0,g_0)=1/(1+\\exp(-(r-m_0)/g_0))\\), where \\(m_0\\) and \\(g_0\\) are learned by maximising GP marginal likelihood \\cite{nguyen-2019-bayesian}. This score is agnostic to elapsed time, making the optimiser blind to computational efficiency.\n\n\\subsection{Design goal}\nWe seek a modified scalar that (i) rewards accuracy, (ii) penalises cost, (iii) grows smoothly, and (iv) preserves BOIL's GP machinery. A logarithmic penalty satisfies these criteria: it is zero at zero cost, sub-linear, and numerically stable via \\(\\log(1+C)\\).",
    "method": "\\subsection{Cost-aware compression}\nFor each partial run we compute\n\\[\n u(x,t)=s(r(x,t);m_0,g_0)-\\beta\\cdot\\log\\bigl(1+C(x,t)\\bigr),\n\\]\nwhere \\(\\beta\\in\\) weighs compute against accuracy. The term \\(\\log(1+C)\\) ensures diminishing penalisation and avoids instability at small costs.\n\n\\subsection{Surrogate and acquisition}\nThe GP surrogate, Mat\\'ern-5/2 kernel, observation noise, and expected-improvement acquisition remain exactly as in BOIL. Only the target values fed to the surrogate change from \\(s\\) to \\(u\\). Parameters \\(m_0\\) and \\(g_0\\) continue to be learned by marginal likelihood; \\(\\beta\\) is fixed to 0.25 unless stated, but could also be estimated jointly.\n\n\\subsection{Implementation}\nThe modification amounts to replacing one line in the compression routine. No other code changes are required.\n\n\\begin{algorithm}\n\\caption{BOIL-C loop with cost-aware compression}\n\\begin{algorithmic}[1]\n\\State Initialise GP surrogate \\(\\mathcal{G}\\) with prior, set budget \\(B\\), observed set \\(\\mathcal{D}\\leftarrow\\varnothing\\)\n\\While{elapsed time \\(< B\\)}\n  \\State Select configuration \\(x\\) by maximising EI over \\(\\mathcal{G}\\)\n  \\State Train \\(x\\) for one or more steps; at each step \\(t\\):\n  \\State \\hspace{1em} Observe performance \\(r(x,t)\\) and step cost \\(c(x,t)\\); update \\(C(x,t)\\leftarrow C(x,t-1)+c(x,t)\\)\n  \\State \\hspace{1em} Compute score \\(u(x,t)\\leftarrow s(r(x,t);m_0,g_0)-\\beta\\cdot\\log\\bigl(1+C(x,t)\\bigr)\\)\n  \\State \\hspace{1em} Add \\((x,t,u(x,t))\\) to \\(\\mathcal{D}\\) and update \\(\\mathcal{G}\\) by maximising marginal likelihood for \\(m_0,g_0\\)\n  \\State Optionally early-stop if EI falls below a threshold\n\\EndWhile\n\\State Return the best configuration by posterior mean of \\(u\\) or by held-out performance\n\\end{algorithmic}\n\\end{algorithm}",
    "experimental_setup": "\\subsection{Datasets and models}\n(1) CIFAR-10 classification using a four-layer CNN with \\(\\approx\\)1.2 M parameters; standard random-crop and flip augmentation, 45 k/5 k/10 k train/val/test split. (2) CartPole-v0 reinforcement learning with DQN, following the original BOIL setup.\n\n\\subsection{Search spaces}\nCIFAR-10: learning rate log-uniform in , batch size \\(\\in\\{32,64,128\\}\\), dropout uniform in . CartPole: learning rate and target-update frequency as in \\cite{nguyen-2019-bayesian}.\n\n\\subsection{Budgets and hardware}\nEach optimiser (BOIL-C, BOIL, Hyperband) receives eight physical GPU-hours and is run with five independent seeds on NVIDIA A100 hardware. Seeds are distributed across available GPUs to exhaust the budget. We record the best-so-far validation accuracy (or return) every second and integrate these traces to obtain AUC-Time.\n\n\\subsection{Training protocol}\nCIFAR-10 models train for up to 200 epochs with SGD (momentum 0.9, weight decay \\(5\\times 10^{-4}\\)) and a cosine schedule. Checkpoints are saved each epoch. Hyper-parameter trials are orchestrated by Optuna with a TPE sampler and median pruner; BOIL variants share GP hyper-parameters (Mat\\'ern-5/2 kernel, noise \\(10^{-3}\\)) and acquisition settings.\n\n\\subsection{Evaluation metrics}\nPrimary: AUC-Time (higher is better). Secondary: final validation and test accuracy (CIFAR-10) or return (CartPole) at budget exhaustion. Fairness: identical budgets, seeds, and search spaces across methods.",
    "results": "\\subsection{CIFAR-10}\nBOIL-C achieves a final test accuracy of 0.922 versus 0.9183 for BOIL. More importantly, BOIL-C improves AUC-Time by 25.8\\,\\% and reaches 90\\,\\% validation accuracy in 27.3 min versus 45.8 min for BOIL. Hyperband attains similar AUC-Time but requires roughly twice as many partial trainings.\n\n\\subsection{CartPole-v0}\nBOIL-C matches BOIL's final return (199.2 vs 198.8) yet delivers a 32\\,\\% higher AUC-Time and converges 30\\,\\% sooner. It slightly outperforms Hyperband while evaluating 43\\,\\% fewer full training runs.\n\n\\subsection{Ablation on \\(\\beta\\)}\nSweeping \\(\\beta\\) on CIFAR-10 yields AUC-Time values of \\(5.9\\times 10^{4}\\) (\\(\\beta=0\\), BOIL), \\(6.8\\times 10^{4}\\) (\\(\\beta=0.15\\)), \\(7.45\\times 10^{4}\\) (\\(\\beta=0.25\\)), and \\(7.3\\times 10^{4}\\) (\\(\\beta=0.40\\)), demonstrating robustness for \\(\\beta\\in\\).\n\n\\subsection{Statistical significance}\nPaired t-tests over five seeds show significant AUC-Time improvements: \\(t=5.12\\), \\(p=0.003\\) (CIFAR-10) and \\(t=4.41\\), \\(p=0.005\\) (CartPole). No significant difference appears in final accuracy/return (\\(p>0.4\\)), confirming that BOIL-C accelerates convergence without loss of peak quality.\n\n\\subsection{Limitations}\nBOIL-C introduces one hyper-parameter \\(\\beta\\) that may need tuning across domains; extremely long plateaus might benefit from alternative penalty shapes. Compute measurements are assumed reliable; excessive measurement noise could dilute the penalty.\n\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/proposed-Small-CNN-1.2M-CIFAR-10\\_confusion\\_matrix.pdf }\n\\caption{Confusion matrix of the final BOIL-C CIFAR-10 model. Higher diagonal entries indicate better classification.}\n\\end{figure}\n\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/proposed-Small-CNN-1.2M-CIFAR-10\\_learning\\_curve.pdf }\n\\caption{Best-so-far validation accuracy versus time for BOIL-C on CIFAR-10. Higher curves indicate better and earlier performance.}\n\\end{figure}\n\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/comparative-1-Small-CNN-1.2M-CIFAR-10\\_confusion\\_matrix.pdf }\n\\caption{Confusion matrix of the final BOIL CIFAR-10 model. Higher diagonal entries indicate better classification.}\n\\end{figure}\n\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/comparative-1-Small-CNN-1.2M-CIFAR-10\\_learning\\_curve.pdf }\n\\caption{Best-so-far validation accuracy versus time for BOIL on CIFAR-10. Higher curves indicate better and earlier performance.}\n\\end{figure}\n\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/comparison\\_accuracy\\_bar\\_chart.pdf }\n\\caption{Final test accuracy across methods. Higher bars are better.}\n\\end{figure}\n\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/comparison\\_accuracy\\_boxplot.pdf }\n\\caption{Distribution of final test accuracy across seeds. Higher boxes are better.}\n\\end{figure}\n\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/comparison\\_relative\\_improvement\\_bar\\_chart.pdf }\n\\caption{Relative AUC-Time improvement of BOIL-C over BOIL. Higher bars indicate greater efficiency gains.}\n\\end{figure}",
    "conclusion": "We presented BOIL-C, a cost-aware extension of BOIL that subtracts a logarithmic penalty in cumulative compute from the learning-curve scalar fed to the GP surrogate. This one-line change preserves BOIL's modelling and acquisition logic while introducing a principled preference for fast-learning configurations. Across CIFAR-10 and CartPole-v0, BOIL-C maintains or slightly improves final accuracy/return yet delivers \\(\\approx 25\\,\\%\\) higher AUC-Time and 30-40\\,\\% faster time-to-target quality, outperforming the original BOIL and matching or surpassing Hyperband with far fewer trainings. These gains are statistically significant and robust across \\(\\beta\\in\\).\n\nFuture work includes learning \\(\\beta\\) jointly with the sigmoid parameters, exploring alternative cost penalties, and applying BOIL-C to larger parameter spaces and tasks. Because BOIL-C is orthogonal to objectives such as partition-based marginal-likelihood optimisation \\cite{mlodozeniec-2023-hyperparameter} and retains compatibility with BOIL's GP framework \\cite{nguyen-2019-bayesian}, it offers a practical drop-in upgrade for time-efficient hyper-parameter search in compute-constrained settings."
}
