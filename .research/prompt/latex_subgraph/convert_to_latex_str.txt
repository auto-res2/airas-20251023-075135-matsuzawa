
LLM Name: gpt-5-2025-08-07
Input:

You are a LaTeX expert.
Your task is to convert each section of a research paper into plain LaTeX **content only**, without including any section titles or metadata.

Below are the paper sections. For each one, convert only the **content** into LaTeX:

---
Section: title

BOIL-C: Cost-Aware Learning-Curve Compression for Faster Bayesian Hyperparameter Optimisation

---

---
Section: abstract

Bayesian Optimisation for Iterative Learning (BOIL) accelerates hyper-parameter search by converting every partial learning curve into a single sigmoid-based scalar that a Gaussian-process surrogate can model. Unfortunately that scalar ignores the wall-clock cost of producing the curve: a configuration that reaches 90 % validation accuracy after 200 epochs is valued exactly the same as one that does so in 20 epochs. As a result BOIL may waste time exploring slow learners. We propose BOIL-C, a cost-aware compression that keeps BOIL’s performance term but subtracts a logarithmic penalty proportional to cumulative training time. This one-line modification (i) maintains smoothness, boundedness, and compatibility with BOIL’s surrogate and acquisition; (ii) favours configurations that achieve high scores quickly; and (iii) adds negligible computational overhead. On CIFAR-10 with a 1.2 M-parameter CNN and on CartPole-v0 with DQN, each under an eight-hour GPU budget and five random seeds, BOIL-C reaches the same final accuracy or return 30–40 % sooner than BOIL, improves the area-under-curve of best-so-far performance versus time by roughly 25 %, and matches or slightly outperforms a strong cost-aware baseline while issuing 42 % fewer training runs. These consistent, statistically significant gains demonstrate that a minimal cost-aware adjustment to learning-curve compression can yield substantial practical benefits without sacrificing final quality \cite{nguyen-2019-bayesian}.

---

---
Section: introduction

Hyper-parameter optimisation (HPO) is indispensable for modern deep learning and reinforcement learning, yet its practical value is governed by wall-clock time rather than sample count. Practitioners care far more about how quickly a high-quality configuration is uncovered than about asymptotic performance at some distant horizon. Bayesian optimisation (BO) is attractive because it builds a global surrogate of the response surface and therefore tends to require fewer evaluations than grid or random search; however, standard BO typically treats each configuration as a black-box function call and waits until training converges, ignoring the rich information contained in intermediate checkpoints.
Bayesian Optimisation for Iterative Learning (BOIL) addresses this inefficiency by compressing every partial learning curve r(x,t) that emerges during training into a single scalar u(x,t) through a data-driven sigmoid transformation. The resulting stream of scalars allows a Gaussian process (GP) surrogate to exploit early learning signals and to update its acquisition function long before any run has finished \cite{nguyen-2019-bayesian}. BOIL therefore achieves substantial wall-clock savings compared with conventional BO. Yet, despite this progress, BOIL’s score is oblivious to the compute already consumed: a run that takes hours to inch toward an eventual high accuracy is ranked identically to a run that arrives there in minutes. Because the GP sees no distinction, the optimiser may continue to sample slow-learning hyper-parameters and squander precious budget.
We contend that time-efficiency should be embedded directly in the representation fed to the surrogate, not bolted onto the acquisition or the scheduler. To that end we introduce BOIL-C, which augments the original sigmoid score with a logarithmic penalty on cumulative elapsed time C(x,t). Formally
u(x,t) = s(r(x,t); m0,g0) − β log(1 + C(x,t)),
where s(·) is BOIL’s sigmoid with parameters (m0,g0) learned by marginal likelihood, and β∈ sets the strength of the penalty. When β = 0 we exactly recover BOIL. The log form guarantees diminishing marginal penalties so that late-stage improvements are not dismissed outright. Crucially, this modification changes only the scalar target; it leaves the GP, acquisition function, and optimisation loop untouched, making adoption trivial.
We evaluate BOIL-C on two canonical tasks from the BOIL literature: CIFAR-10 image classification with a small CNN and CartPole-v0 reinforcement learning with DQN. Search spaces comprise three hyper-parameters for the CNN (learning rate, batch size, dropout) and two for DQN (learning rate, target-update frequency). Competing methods—original BOIL and a strong cost-aware baseline akin to Hyperband—receive identical eight-hour GPU budgets and are repeated over five random seeds. Progress is recorded once per second, and three metrics are reported: (i) area-under-curve of best-so-far validation accuracy or return versus wall-clock time (AUC_Time); (ii) time-to-target defined as the duration required to reach 95 % of the method’s eventual best score; and (iii) final score at budget exhaustion.
Results are decisive. BOIL-C accelerates optimisation by roughly one-third on both tasks, improving AUC_Time by about 25 % relative to BOIL while leaving final performance statistically unchanged. Compared with the cost-aware baseline, BOIL-C achieves comparable or slightly better AUC_Time yet performs 42 % fewer training runs, highlighting the power of embedding cost awareness inside the surrogate rather than in external scheduling heuristics. An ablation varying β confirms a broad optimum around 0.25, indicating robustness. Because the modification is a single line of code, all theoretical guarantees and existing infrastructure remain intact.
Contributions
• We pinpoint a limitation of BOIL: its compression ignores compute cost, permitting slow learners to dominate search \cite{nguyen-2019-bayesian}.
• We propose BOIL-C, a principled yet minimal extension that introduces a logarithmic time penalty, thereby aligning the surrogate’s target with practitioners’ real objective—fast progress.
• We provide a thorough empirical study on vision and reinforcement-learning benchmarks demonstrating 30–40 % faster convergence, 25 % higher AUC_Time, and unchanged final accuracy or return.
• We show that BOIL-C requires negligible implementation effort, improves stability across seeds, and remains compatible with future enhancements such as multi-fidelity scheduling.
The remainder of this paper is organised as follows. Section 2 reviews related approaches. Section 3 revisits the background and formal problem setting. Section 4 details BOIL-C. Section 5 describes the experimental protocol. Section 6 reports results and ablations. Section 7 concludes with future directions.

---

---
Section: related_work

Iterative-learning Bayesian optimisation. BOIL pioneered the idea of modelling compressed learning curves with a GP surrogate, enabling early decision-making and delivering strong empirical speed-ups over final-epoch BO methods \cite{nguyen-2019-bayesian}. Subsequent studies have explored richer curve embeddings but have largely preserved BOIL’s cost-agnostic stance. In contrast, BOIL-C retains BOIL’s architecture yet introduces cost awareness inside the very scalar that the GP observes, avoiding changes to the optimiser’s logic.
Cost-aware HPO and multi-fidelity schedulers. Techniques such as Hyperband and Successive Halving allocate resources adaptively, terminating poorly performing runs early and focusing compute on promising ones. While effective, these methods require sophisticated scheduling and often launch many short runs, which can inflate overhead. Our approach is orthogonal: we keep the training schedule fixed but alter the information content, allowing the GP to prefer fast learners without issuing additional jobs. Empirically, BOIL-C matches or exceeds a Hyperband-like baseline with substantially fewer runs.
Marginal-likelihood-based single-run HPO. Neural Network Partitioning (NNP) optimises hyper-parameters within a single training run by maximising a partitioned marginal likelihood, thereby eliminating retraining costs \cite{mlodozeniec-2023-hyperparameter}. NNP excels in settings where repeated evaluations are infeasible, whereas BOIL-C targets the complementary regime where multiple runs are acceptable but wall-clock time is precious. Because NNP does not rely on learning-curve compression, it is not directly comparable under our evaluation protocol, yet it represents a promising orthogonal strategy.
Comparison summary. BOIL-C differentiates itself by injecting cost sensitivity at the representation level while preserving BOIL’s surrogate and acquisition. This design choice yields gains similar to sophisticated schedulers but with far less orchestration and without sacrificing BO’s sample efficiency.

---

---
Section: background

Problem formulation. Let x∈X denote a hyper-parameter configuration. Training the corresponding model produces, at discrete step t, a task-specific performance r(x,t) (e.g. validation accuracy) and incurs cost c(x,t) measured in seconds. The cumulative cost is C(x,t)=Σ_{i=1}^{t} c(x,i). Practitioners seek to maximise f(x)=lim_{t→∞} r(x,t) yet are constrained by a total budget B of wall-clock time. The optimisation goal is therefore to discover, as quickly as possible, a configuration whose eventual performance is high.
BOIL compression. BOIL converts each partial curve into a scalar
u_BOIL(x,t)=s(r(x,t);m0,g0)          (1)
where s(r;m0,g0)=1/(1+exp(−(r−m0)/g0)) is a sigmoid with parameters learned by maximising the marginal likelihood of the GP surrogate \cite{nguyen-2019-bayesian}. Scalars across different (x,t) pairs populate the training set of the GP, which then guides an expected-improvement acquisition.
Limitation. Expression (1) depends solely on r(x,t); the cost C(x,t) is ignored. Consequently, two configurations with identical performance but vastly different training times are considered equally valuable, potentially diverting budget toward slow learners.
Assumptions. Following BOIL we assume: (i) r(x,t) is non-decreasing in expectation; (ii) c(x,t) and hence C(x,t) are observed noiselessly; (iii) training produces a sequence of checkpoints that can be queried at arbitrary t. These assumptions hold in most deep-learning pipelines and underpin the validity of streaming updates to the GP.

---

---
Section: method

Cost-Aware Learning-Curve Compression. We redefine the scalar fed to the surrogate as
u(x,t)=s(r(x,t);m0,g0) − β log(1+C(x,t)),        (2)
with β∈. The additive penalty introduces a monotone, concave dependence on cost. For small C the derivative is β/(1+C), encouraging the optimiser to prefer swift early progress; for large C the penalty flattens, preventing over-punishment of long but potentially fruitful runs.
Design properties
1. Compatibility. Because Eq. (2) differs from Eq. (1) only by a deterministic shift, all statistical machinery—GP likelihood, hyper-parameter optimisation, acquisition computation—remains unchanged.
2. Boundedness and smoothness. The sigmoid term keeps u(x,t) in (0,1); subtracting a finite log term preserves boundedness from above and retains differentiability, benefiting GP regression.
3. Scale awareness. Costs are measured in real seconds, making comparisons fair across configurations with different per-step complexities (e.g. varying batch sizes).
4. Negligible overhead. Implementing Eq. (2) requires adding log(1+C) and a subtraction; computational cost is trivial relative to training.
Learning β. We fix β=0.25 in main experiments, selected via a coarse sweep. Alternatively β can be treated as a GP hyper-parameter and optimised by marginal likelihood alongside m0 and g0; preliminary tests reveal similar performance.
Algorithmic procedure
Step 1  Run training for configuration x, obtain (r,c).
Step 2  Update C←C+c and compute s=1/(1+exp(−(r−m0)/g0)).
Step 3  Form u=s−β log(1+C) and append (x,t,u) to the GP data.
Step 4  Re-optimise GP hyper-parameters if needed and pick next x via the existing acquisition rule.
The rest of the BO loop, including parallelisation and early stopping, is identical to BOIL.

---

---
Section: experimental_setup

Tasks and models. The vision benchmark is CIFAR-10, trained with a four-layer convolutional network containing ≈1.2 M parameters. The reinforcement-learning benchmark is CartPole-v0 solved with a Deep Q-Network (DQN). Both tasks are standard in the BOIL literature \cite{nguyen-2019-bayesian}.
Search spaces. For CIFAR-10 we tune learning rate (log-uniform 10^{-4}–10^{-1}), batch size (32–256, powers of two), and dropout rate (0.0–0.5). For CartPole-DQN we tune learning rate (10^{-5}–10^{-2}) and target-update frequency (100–2000 steps).
Compared methods
• BOIL-C (ours) with β=0.25.
• BOIL (original) using Eq. (1).
• Hyperband-like cost-aware baseline that allocates resources adaptively.
Budget and repetitions. Each optimiser receives a strict wall-clock budget of eight GPU hours and is run with five random seeds. Seeds are executed in parallel to fully utilise hardware yet respect the global budget.
Logging. We log best-so-far validation accuracy (CIFAR-10) or average episodic return (CartPole) at one-second intervals. Cumulative cost C(x,t) is measured precisely via CUDA event timers.
Evaluation metrics
1. AUC_Time = ∫_{0}^{B} best-so-far score dt  (higher is better).
2. Time-to-target = earliest t such that best-so-far(t) ≥ 0.95·best-so-far(B)  (lower is better).
3. Final score at B.
Mean and standard error (SEM) across seeds are reported. Statistical significance is assessed with paired t-tests.
Implementation. The GP uses a Matérn-5/2 kernel and is updated every 300 seconds. Sigmoid parameters (m0,g0) are re-optimised after each new configuration; BOIL-C uses the same routine. Code is based on the public BOIL repository with a single edit to the compression function.
Fairness safeguards. All methods share identical data loaders, augmentation, optimiser types, and stopping criteria. Random seeds control data shuffling and network initialisation. No post-hoc tuning is carried out on held-out seeds.

---

---
Section: results

CIFAR-10 results. Table 1 summarises five-seed averages. BOIL-C attains an AUC_Time of 0.742 ± 0.012 versus 0.592 ± 0.018 for BOIL and 0.713 ± 0.027 for the Hyperband baseline, corresponding to gains of 25.3 % and 4.1 % respectively. BOIL-C reaches 95 % of its eventual best accuracy in 2.8 h, a 38 % reduction relative to BOIL’s 4.5 h. Final validation accuracies after eight hours are statistically indistinguishable (87.9 % vs 88.1 %, p > 0.3).
CartPole results. On reinforcement learning BOIL-C achieves an AUC_Time of 0.815 ± 0.010, outperforming BOIL (0.645 ± 0.022) by 26.4 % and slightly surpassing the baseline (0.798 ± 0.015). Time-to-target for a return of 180 is 34 min for BOIL-C against 51 min for BOIL. Final returns are again identical within noise (197 vs 198).
Cross-task synthesis. Averaged across both problems BOIL-C improves AUC_Time by 25 ± 2 %, slashes time-to-target by one-third, and issues 42 % fewer training runs than the Hyperband-like scheduler, underscoring superior sample efficiency.
Ablation on β. For CIFAR-10 a single-seed sweep yields AUC_Time values of 0.593 (β=0), 0.668 (β=0.1), 0.744 (β=0.25), and 0.739 (β=0.5), evidencing robustness and a broad optimum near 0.25.
Limitations. BOIL-C presumes reliable timing; skewed measurements could distort penalties. Excessive β may over-penalise slow-but-ultimately-superior curves. Only two tasks are studied; future work should cover larger models and additional modalities.

---

---
Section: conclusion

We introduced BOIL-C, a cost-aware extension to BOIL that subtracts a logarithmic compute penalty from the sigmoid-compressed learning-curve score. This single-line change equips the surrogate with a notion of time, steering Bayesian optimisation toward hyper-parameters that achieve high performance quickly. Empirical evaluation on CIFAR-10 and CartPole-v0 shows consistent 25 % improvements in AUC_Time and 30–40 % faster convergence without loss of final accuracy or return, matching or exceeding a dedicated Hyperband-style scheduler while executing far fewer runs. Because BOIL-C leaves the surrogate, acquisition, and optimisation loop intact, it can be adopted immediately in existing BOIL pipelines at negligible cost.
Future avenues include automatic learning of the penalty weight β via marginal likelihood, integrating BOIL-C with multi-fidelity or early-stopping frameworks, and extending the penalty to encompass energy or memory consumption. In parallel, single-run marginal-likelihood methods such as Neural Network Partitioning \cite{mlodozeniec-2023-hyperparameter} offer complementary solutions for regimes where repeated evaluations are impractical. Together these lines of research promise to make hyper-parameter optimisation both faster and more resource-aware, a critical requirement as models and datasets continue to scale.

---


## LaTeX Formatting Rules:
- Use \subsection{...} for any subsections within this section.
    - Subsection titles should be distinct from the section name;
    - Do not use '\subsection{  }', or other slight variations. Use more descriptive and unique titles.
    - Avoid excessive subdivision. If a subsection is brief or overlaps significantly with another, consider merging them for clarity and flow.

- For listing contributions, use the LaTeX \begin{itemize}...\end{itemize} format.
    - Each item should start with a short title in \textbf{...} format.
    - Avoid using -, *, or other Markdown bullet styles.

- When including tables, use the `tabularx` environment with `\textwidth` as the target width.
    - At least one column must use the `X` type to enable automatic width adjustment and line breaking.
    - Include `\hline` at the top, after the header, and at the bottom. Avoid vertical lines unless necessary.
    - To left-align content in `X` columns, define `
ewcolumntype{Y}{>{
aggedrightrraybackslash}X}` using the `array` package.

- When writing pseudocode, use the `algorithm` and `algorithmicx` LaTeX environments.
    - Only include pseudocode in the `Method` section. Pseudocode is not allowed in any other sections.
    - Prefer the `\begin{algorithmic}` environment using **lowercase commands** such as `\State`, `\For`, and `\If`, to ensure compatibility and clean formatting.
    - Pseudocode must represent actual algorithms or procedures with clear logic. Do not use pseudocode to simply rephrase narrative descriptions or repeat what has already been explained in text.
        - Good Example:
        ```latex
        \State Compute transformed tokens: \(	ilde{T} \leftarrow W\,T\)
        \State Update: \(T_{new} \leftarrow 	ilde{T} + \mu\,T_{prev}\)
        ```
- Figures and images are ONLY allowed in the "Results" section.
    - Use LaTeX float option `[H]` to force placement.

- All figures must be inserted using the following LaTeX format, using a `width` that reflects the filename:
    ```latex
    \includegraphics[width=\linewidth]{ images/filename.pdf }
    ```
    The `<appropriate-width>` must be selected based on the filename suffix:
    - If the filename ends with _pair1.pdf or _pair2.pdf, use 0.48\linewidth as the width of each subfigure environment and place the figures side by side using `subcaption` package.
    - Otherwise (default), use 0.7\linewidth

- **Escaping special characters**:
    - LaTeX special characters (`#`, `$`, `%`, `&`, `~`, `_`, `^`, `{`, `}`, `\`) must be escaped with a leading backslash when they appear in plain text (e.g., `data\_set`, `C\&C`).
    - Underscores **must always be escaped** (`\_`) outside math mode, even in filenames (e.g., memory\_profiler), code-style words, itemize lists, or citation contexts.

- Always use ASCII hyphens (`-`) instead of en-dashes (`–`) or em-dashes (`—`) to avoid spacing issues in hyphenated terms.
- Do not include any of these higher-level commands such as \documentclass{...}, \begin{document}, and \end{document}.
    - Additionally, avoid including section-specific commands such as \begin{abstract}, \section{  }, or any other similar environment definitions.
- Do not modify citation placeholders:
    - Citation placeholders appear in the format [citation_key], where citation_key contains underscores, numbers, and text (e.g., [vaswani_2017_attention], [smith_2023_deep]).
    - You must preserve these placeholders EXACTLY as they appear in the input text.
- If no LaTeX conversion is needed, output the content as-is without status messages like [Unchanged].
Output:
{
    "title": "BOIL-C: Cost-Aware Learning-Curve Compression for Faster Bayesian Hyperparameter Optimisation",
    "abstract": "Bayesian Optimisation for Iterative Learning (BOIL) accelerates hyper-parameter search by converting every partial learning curve into a single sigmoid-based scalar that a Gaussian-process surrogate can model. Unfortunately that scalar ignores the wall-clock cost of producing the curve: a configuration that reaches 90 \\% validation accuracy after 200 epochs is valued exactly the same as one that does so in 20 epochs. As a result BOIL may waste time exploring slow learners. We propose BOIL-C, a cost-aware compression that keeps BOIL’s performance term but subtracts a logarithmic penalty proportional to cumulative training time. This one-line modification (i) maintains smoothness, boundedness, and compatibility with BOIL’s surrogate and acquisition; (ii) favours configurations that achieve high scores quickly; and (iii) adds negligible computational overhead. On CIFAR-10 with a 1.2 M-parameter CNN and on CartPole-v0 with DQN, each under an eight-hour GPU budget and five random seeds, BOIL-C reaches the same final accuracy or return 30-40 \\% sooner than BOIL, improves the area-under-curve of best-so-far performance versus time by roughly 25 \\%, and matches or slightly outperforms a strong cost-aware baseline while issuing 42 \\% fewer training runs. These consistent, statistically significant gains demonstrate that a minimal cost-aware adjustment to learning-curve compression can yield substantial practical benefits without sacrificing final quality \\cite{nguyen-2019-bayesian}.",
    "introduction": "Hyper-parameter optimisation (HPO) is indispensable for modern deep learning and reinforcement learning, yet its practical value is governed by wall-clock time rather than sample count. Practitioners care far more about how quickly a high-quality configuration is uncovered than about asymptotic performance at some distant horizon. Bayesian optimisation (BO) is attractive because it builds a global surrogate of the response surface and therefore tends to require fewer evaluations than grid or random search; however, standard BO typically treats each configuration as a black-box function call and waits until training converges, ignoring the rich information contained in intermediate checkpoints.\n\nBayesian Optimisation for Iterative Learning (BOIL) addresses this inefficiency by compressing every partial learning curve \\(r(\\mathbf{x}, t)\\) that emerges during training into a single scalar \\(u(\\mathbf{x}, t)\\) through a data-driven sigmoid transformation. The resulting stream of scalars allows a Gaussian process (GP) surrogate to exploit early learning signals and to update its acquisition function long before any run has finished \\cite{nguyen-2019-bayesian}. BOIL therefore achieves substantial wall-clock savings compared with conventional BO. Yet, despite this progress, BOIL’s score is oblivious to the compute already consumed: a run that takes hours to inch toward an eventual high accuracy is ranked identically to a run that arrives there in minutes. Because the GP sees no distinction, the optimiser may continue to sample slow-learning hyper-parameters and squander precious budget.\n\nWe contend that time-efficiency should be embedded directly in the representation fed to the surrogate, not bolted onto the acquisition or the scheduler. To that end we introduce BOIL-C, which augments the original sigmoid score with a logarithmic penalty on cumulative elapsed time \\(C(\\mathbf{x}, t)\\). Formally\n\\[\n  u(\\mathbf{x}, t) = s\\big(r(\\mathbf{x}, t); m\\_0, g\\_0\\big) - \\beta\\, \\log\\!\\big(1 + C(\\mathbf{x}, t)\\big),\n\\]\nwhere \\(s(\\cdot)\\) is BOIL’s sigmoid with parameters \\((m\\_0, g\\_0)\\) learned by marginal likelihood, and \\(\\beta\\) sets the strength of the penalty. When \\(\\beta = 0\\) we exactly recover BOIL. The log form guarantees diminishing marginal penalties so that late-stage improvements are not dismissed outright. Crucially, this modification changes only the scalar target; it leaves the GP, acquisition function, and optimisation loop untouched, making adoption trivial.\n\nWe evaluate BOIL-C on two canonical tasks from the BOIL literature: CIFAR-10 image classification with a small CNN and CartPole-v0 reinforcement learning with DQN. Search spaces comprise three hyper-parameters for the CNN (learning rate, batch size, dropout) and two for DQN (learning rate, target-update frequency). Competing methods—original BOIL and a strong cost-aware baseline akin to Hyperband—receive identical eight-hour GPU budgets and are repeated over five random seeds. Progress is recorded once per second, and three metrics are reported: (i) area-under-curve of best-so-far validation accuracy or return versus wall-clock time (AUC\\_Time); (ii) time-to-target defined as the duration required to reach 95 \\% of the method’s eventual best score; and (iii) final score at budget exhaustion.\n\nResults are decisive. BOIL-C accelerates optimisation by roughly one-third on both tasks, improving AUC\\_Time by about 25 \\% relative to BOIL while leaving final performance statistically unchanged. Compared with the cost-aware baseline, BOIL-C achieves comparable or slightly better AUC\\_Time yet performs 42 \\% fewer training runs, highlighting the power of embedding cost awareness inside the surrogate rather than in external scheduling heuristics. An ablation varying \\(\\beta\\) confirms a broad optimum around 0.25, indicating robustness. Because the modification is a single line of code, all theoretical guarantees and existing infrastructure remain intact.\n\n\\subsection{Contributions}\n\\begin{itemize}\n  \\item \\textbf{Limitation of BOIL.} We pinpoint a limitation of BOIL: its compression ignores compute cost, permitting slow learners to dominate search \\cite{nguyen-2019-bayesian}.\n  \\item \\textbf{BOIL-C proposal.} We propose BOIL-C, a principled yet minimal extension that introduces a logarithmic time penalty, thereby aligning the surrogate’s target with practitioners’ real objective - fast progress.\n  \\item \\textbf{Empirical gains.} We provide a thorough empirical study on vision and reinforcement-learning benchmarks demonstrating 30-40 \\% faster convergence, 25 \\% higher AUC\\_Time, and unchanged final accuracy or return.\n  \\item \\textbf{Practicality and compatibility.} We show that BOIL-C requires negligible implementation effort, improves stability across seeds, and remains compatible with future enhancements such as multi-fidelity scheduling.\n\\end{itemize}\n\nThe remainder of this paper is organised as follows. Section 2 reviews related approaches. Section 3 revisits the background and formal problem setting. Section 4 details BOIL-C. Section 5 describes the experimental protocol. Section 6 reports results and ablations. Section 7 concludes with future directions.",
    "related_work": "\\subsection{Iterative-learning Bayesian optimisation}\nBOIL pioneered the idea of modelling compressed learning curves with a GP surrogate, enabling early decision-making and delivering strong empirical speed-ups over final-epoch BO methods \\cite{nguyen-2019-bayesian}. Subsequent studies have explored richer curve embeddings but have largely preserved BOIL’s cost-agnostic stance. In contrast, BOIL-C retains BOIL’s architecture yet introduces cost awareness inside the very scalar that the GP observes, avoiding changes to the optimiser’s logic.\n\n\\subsection{Cost-aware HPO and multi-fidelity schedulers}\nTechniques such as Hyperband and Successive Halving allocate resources adaptively, terminating poorly performing runs early and focusing compute on promising ones. While effective, these methods require sophisticated scheduling and often launch many short runs, which can inflate overhead. Our approach is orthogonal: we keep the training schedule fixed but alter the information content, allowing the GP to prefer fast learners without issuing additional jobs. Empirically, BOIL-C matches or exceeds a Hyperband-like baseline with substantially fewer runs.\n\n\\subsection{Marginal-likelihood-based single-run HPO}\nNeural Network Partitioning (NNP) optimises hyper-parameters within a single training run by maximising a partitioned marginal likelihood, thereby eliminating retraining costs \\cite{mlodozeniec-2023-hyperparameter}. NNP excels in settings where repeated evaluations are infeasible, whereas BOIL-C targets the complementary regime where multiple runs are acceptable but wall-clock time is precious. Because NNP does not rely on learning-curve compression, it is not directly comparable under our evaluation protocol, yet it represents a promising orthogonal strategy.\n\n\\subsection{Comparison summary}\nBOIL-C differentiates itself by injecting cost sensitivity at the representation level while preserving BOIL’s surrogate and acquisition. This design choice yields gains similar to sophisticated schedulers but with far less orchestration and without sacrificing BO’s sample efficiency.",
    "background": "\\subsection{Problem formulation}\nLet \\(\\mathbf{x} \\in \\mathcal{X}\\) denote a hyper-parameter configuration. Training the corresponding model produces, at discrete step \\(t\\), a task-specific performance \\(r(\\mathbf{x}, t)\\) (e.g. validation accuracy) and incurs cost \\(c(\\mathbf{x}, t)\\) measured in seconds. The cumulative cost is \\(C(\\mathbf{x}, t) = \\sum\\limits\\_{i=1}^{t} c(\\mathbf{x}, i)\\). Practitioners seek to maximise \\(f(\\mathbf{x}) = \\lim\\limits\\_{t \\to \\infty} r(\\mathbf{x}, t)\\) yet are constrained by a total budget \\(B\\) of wall-clock time. The optimisation goal is therefore to discover, as quickly as possible, a configuration whose eventual performance is high.\n\n\\subsection{BOIL compression}\nBOIL converts each partial curve into a scalar\n\\begin{equation}\\tag{1}\n  u\\_{\\mathrm{BOIL}}(\\mathbf{x}, t) = s\\big(r(\\mathbf{x}, t); m\\_0, g\\_0\\big),\n\\end{equation}\nwhere \\(s(r; m\\_0, g\\_0) = \\frac{1}{1 + \\exp\\!\\left(-\\frac{r - m\\_0}{g\\_0}\\right)}\\) is a sigmoid with parameters learned by maximising the marginal likelihood of the GP surrogate \\cite{nguyen-2019-bayesian}. Scalars across different \\((\\mathbf{x}, t)\\) pairs populate the training set of the GP, which then guides an expected-improvement acquisition.\n\n\\subsection{Limitation}\nExpression (1) depends solely on \\(r(\\mathbf{x}, t)\\); the cost \\(C(\\mathbf{x}, t)\\) is ignored. Consequently, two configurations with identical performance but vastly different training times are considered equally valuable, potentially diverting budget toward slow learners.\n\n\\subsection{Assumptions}\nFollowing BOIL we assume: (i) \\(r(\\mathbf{x}, t)\\) is non-decreasing in expectation; (ii) \\(c(\\mathbf{x}, t)\\) and hence \\(C(\\mathbf{x}, t)\\) are observed noiselessly; (iii) training produces a sequence of checkpoints that can be queried at arbitrary \\(t\\). These assumptions hold in most deep-learning pipelines and underpin the validity of streaming updates to the GP.",
    "method": "\\subsection{Cost-aware learning-curve compression}\nWe redefine the scalar fed to the surrogate as\n\\begin{equation}\\tag{2}\n  u(\\mathbf{x}, t) = s\\big(r(\\mathbf{x}, t); m\\_0, g\\_0\\big) - \\beta\\,\\log\\!\\big(1 + C(\\mathbf{x}, t)\\big),\n\\end{equation}\nwith \\(\\beta\\). The additive penalty introduces a monotone, concave dependence on cost. For small \\(C\\) the derivative is \\(\\beta/(1+C)\\), encouraging the optimiser to prefer swift early progress; for large \\(C\\) the penalty flattens, preventing over-punishment of long but potentially fruitful runs.\n\n\\subsection{Design properties}\n\\begin{itemize}\n  \\item \\textbf{Compatibility.} Because Eq. (2) differs from Eq. (1) only by a deterministic shift, all statistical machinery - GP likelihood, hyper-parameter optimisation, acquisition computation - remains unchanged.\n  \\item \\textbf{Boundedness and smoothness.} The sigmoid term keeps \\(u(\\mathbf{x}, t)\\) in \\((0,1)\\); subtracting a finite log term preserves boundedness from above and retains differentiability, benefiting GP regression.\n  \\item \\textbf{Scale awareness.} Costs are measured in real seconds, making comparisons fair across configurations with different per-step complexities (e.g. varying batch sizes).\n  \\item \\textbf{Negligible overhead.} Implementing Eq. (2) requires adding \\(\\log(1+C)\\) and a subtraction; computational cost is trivial relative to training.\n\\end{itemize}\n\n\\subsection{Learning the penalty weight}\nWe fix \\(\\beta = 0.25\\) in main experiments, selected via a coarse sweep. Alternatively \\(\\beta\\) can be treated as a GP hyper-parameter and optimised by marginal likelihood alongside \\(m\\_0\\) and \\(g\\_0\\); preliminary tests reveal similar performance.\n\n\\subsection{Algorithmic procedure}\n\\begin{algorithm}[H]\n\\caption{BOIL-C compression and BO update}\n\\begin{algorithmic}[1]\n  \\State Given a configuration \\(\\mathbf{x}\\), observe current performance \\(r(\\mathbf{x}, t)\\) and step cost \\(c(\\mathbf{x}, t)\\)\n  \\State Update cumulative cost: \\(C \\leftarrow C + c(\\mathbf{x}, t)\\)\n  \\State Compute sigmoid score: \\(s \\leftarrow \\frac{1}{1 + \\exp\\!\\left(-\\frac{r(\\mathbf{x}, t) - m\\_0}{g\\_0}\\right)}\\)\n  \\State Form compressed value: \\(u \\leftarrow s - \\beta\\,\\log(1 + C)\\)\n  \\State Append \\((\\mathbf{x}, t, u)\\) to GP training data\n  \\State Re-optimise GP hyper-parameters if scheduled\n  \\State Select next configuration \\(\\mathbf{x}'\\) using the existing acquisition on the GP\n\\end{algorithmic}\n\\end{algorithm}\n\nThe rest of the BO loop, including parallelisation and early stopping, is identical to BOIL.",
    "experimental_setup": "\\subsection{Tasks and models}\nThe vision benchmark is CIFAR-10, trained with a four-layer convolutional network containing \\(\\approx 1.2\\) M parameters. The reinforcement-learning benchmark is CartPole-v0 solved with a Deep Q-Network (DQN). Both tasks are standard in the BOIL literature \\cite{nguyen-2019-bayesian}.\n\n\\subsection{Search spaces}\nFor CIFAR-10 we tune learning rate (log-uniform \\(10^{-4}\\)–\\(10^{-1}\\)), batch size (32–256, powers of two), and dropout rate (0.0–0.5). For CartPole-DQN we tune learning rate (\\(10^{-5}\\)–\\(10^{-2}\\)) and target-update frequency (100–2000 steps).\n\n\\subsection{Compared methods}\n\\begin{itemize}\n  \\item \\textbf{BOIL-C (ours).} \\(\\beta = 0.25\\).\n  \\item \\textbf{BOIL (original).} Using Eq. (1).\n  \\item \\textbf{Hyperband-like baseline.} Cost-aware scheduler that allocates resources adaptively.\n\\end{itemize}\n\n\\subsection{Budget and repetitions}\nEach optimiser receives a strict wall-clock budget of eight GPU hours and is run with five random seeds. Seeds are executed in parallel to fully utilise hardware yet respect the global budget.\n\n\\subsection{Logging}\nWe log best-so-far validation accuracy (CIFAR-10) or average episodic return (CartPole) at one-second intervals. Cumulative cost \\(C(\\mathbf{x}, t)\\) is measured precisely via CUDA event timers.\n\n\\subsection{Evaluation metrics}\n\\begin{itemize}\n  \\item \\(\\mathrm{AUC\\_Time} = \\int\\_{0}^{B} \\text{best-so-far score}\\, dt\\) (higher is better).\n  \\item Time-to-target: earliest \\(t\\) such that \\(\\text{best-so-far}(t) \\ge 0.95\\,\\text{best-so-far}(B)\\) (lower is better).\n  \\item Final score at \\(B\\).\n\\end{itemize}\nMean and standard error (SEM) across seeds are reported. Statistical significance is assessed with paired t-tests.\n\n\\subsection{Implementation}\nThe GP uses a Mat\\'ern-5/2 kernel and is updated every 300 seconds. Sigmoid parameters \\((m\\_0, g\\_0)\\) are re-optimised after each new configuration; BOIL-C uses the same routine. Code is based on the public BOIL repository with a single edit to the compression function.\n\n\\subsection{Fairness safeguards}\nAll methods share identical data loaders, augmentation, optimiser types, and stopping criteria. Random seeds control data shuffling and network initialisation. No post-hoc tuning is carried out on held-out seeds.",
    "results": "\\subsection{CIFAR-10 results}\nTable 1 summarises five-seed averages. BOIL-C attains an AUC\\_Time of 0.742 \\(\\pm\\) 0.012 versus 0.592 \\(\\pm\\) 0.018 for BOIL and 0.713 \\(\\pm\\) 0.027 for the Hyperband baseline, corresponding to gains of 25.3 \\% and 4.1 \\% respectively. BOIL-C reaches 95 \\% of its eventual best accuracy in 2.8 h, a 38 \\% reduction relative to BOIL’s 4.5 h. Final validation accuracies after eight hours are statistically indistinguishable (87.9 \\% vs 88.1 \\%, \\(p > 0.3\\)).\n\n\\subsection{CartPole results}\nOn reinforcement learning BOIL-C achieves an AUC\\_Time of 0.815 \\(\\pm\\) 0.010, outperforming BOIL (0.645 \\(\\pm\\) 0.022) by 26.4 \\% and slightly surpassing the baseline (0.798 \\(\\pm\\) 0.015). Time-to-target for a return of 180 is 34 min for BOIL-C against 51 min for BOIL. Final returns are again identical within noise (197 vs 198).\n\n\\subsection{Cross-task synthesis}\nAveraged across both problems BOIL-C improves AUC\\_Time by 25 \\(\\pm\\) 2 \\%, slashes time-to-target by one-third, and issues 42 \\% fewer training runs than the Hyperband-like scheduler, underscoring superior sample efficiency.\n\n\\subsection{Ablation on \\(\\beta\\)}\nFor CIFAR-10 a single-seed sweep yields AUC\\_Time values of 0.593 (\\(\\beta=0\\)), 0.668 (\\(\\beta=0.1\\)), 0.744 (\\(\\beta=0.25\\)), and 0.739 (\\(\\beta=0.5\\)), evidencing robustness and a broad optimum near 0.25.\n\n\\subsection{Limitations}\nBOIL-C presumes reliable timing; skewed measurements could distort penalties. Excessive \\(\\beta\\) may over-penalise slow-but-ultimately-superior curves. Only two tasks are studied; future work should cover larger models and additional modalities.",
    "conclusion": "We introduced BOIL-C, a cost-aware extension to BOIL that subtracts a logarithmic compute penalty from the sigmoid-compressed learning-curve score. This single-line change equips the surrogate with a notion of time, steering Bayesian optimisation toward hyper-parameters that achieve high performance quickly. Empirical evaluation on CIFAR-10 and CartPole-v0 shows consistent 25 \\% improvements in AUC\\_Time and 30-40 \\% faster convergence without loss of final accuracy or return, matching or exceeding a dedicated Hyperband-style scheduler while executing far fewer runs. Because BOIL-C leaves the surrogate, acquisition, and optimisation loop intact, it can be adopted immediately in existing BOIL pipelines at negligible cost.\n\n\\subsection{Future directions}\nFuture avenues include automatic learning of the penalty weight \\(\\beta\\) via marginal likelihood, integrating BOIL-C with multi-fidelity or early-stopping frameworks, and extending the penalty to encompass energy or memory consumption. In parallel, single-run marginal-likelihood methods such as Neural Network Partitioning \\cite{mlodozeniec-2023-hyperparameter} offer complementary solutions for regimes where repeated evaluations are impractical. Together these lines of research promise to make hyper-parameter optimisation both faster and more resource-aware, a critical requirement as models and datasets continue to scale."
}
