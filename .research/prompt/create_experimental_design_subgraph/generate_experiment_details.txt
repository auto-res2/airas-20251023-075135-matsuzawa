
LLM Name: o3-2025-04-16
Input:
You are an AI researcher. You will conduct experiments to demonstrate the superiority of the new method described in # New Methods. Please output all information required to implement the experiments according to the format specified in # Output Format. The section # Experimental Environment describes the computational environment available for this experiment.

# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "BOIL transforms every partial learning curve into a single scalar via a fixed-shape sigmoid. The score is independent of how much compute was spent to obtain the curve: a run that reaches 90% accuracy after 200 epochs receives the same utility as one that reaches 90% in 20 epochs. Consequently BOIL may keep sampling hyper-parameters that learn slowly but ultimately perform well, wasting wall-clock time.",
    "Methods": "Cost–Aware Learning-Curve Compression (BOIL-C).\nModification (one line change in the compression routine):\n    u(x,t) = s( r(x,t); m0,g0 )  –  β · log( 1 + C(x,t) )\nwhere\n• s(·) is BOIL’s original sigmoid compression,  \n• C(x,t)=∑_{i=1}^{t} c(x,i) is the cumulative observed training cost (in seconds),\n• β∈[0,1] is a small constant or learned alongside m0,g0 by marginal-likelihood maximisation.\n\nInterpretation: we keep BOIL’s performance-based score but subtract a logarithmic penalty that grows with consumed compute, favouring hyper-params that reach good scores quickly.  Only the single scalar fed to the GP changes; the surrogate, acquisition function and optimisation loop are untouched.",
    "Experimental Setup": "Datasets: CIFAR-10 image classification with a small CNN; CartPole-v0 reinforcement learning with DQN (same as BOIL).\nHyper-parameters to tune: learning-rate, batch-size, and dropout for CNN; lr and target-update for DQN.\nMethods compared:\n1) BOIL (original)\n2) BOIL-C (ours, β=0.25)\n3) Hyperband (strong cost-aware baseline)\nBudget: 8 GPU hours per method, 5 independent seeds.\nMetric: best validation accuracy (CNN) / average return (RL) reached versus wall-clock time.  Report area-under-curve (AUC) of best-so-far metric w.r.t. time.",
    "Experimental Code": "# --- key modification only ----------------------------------------------------\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# inside BOIL class, replace original compression -----------------------------\n\ndef compress_curve(sigmoid_score, cumulative_cost, beta=0.25):\n    \"\"\"Return cost-aware scalar for GP.  Inputs are scalars.\"\"\"\n    return sigmoid_score - beta * np.log1p(cumulative_cost)\n\n# example usage ---------------------------------------------------------------\n# r_t: current accuracy at epoch t, m0,g0 learned as in BOIL\nsigmoid_score = 1 / (1 + np.exp(-(r_t - m0)/g0))\nscalar_for_gp = compress_curve(sigmoid_score, cumulative_cost)\n# everything else in BOIL (GP update, acquisition, etc.) stays unchanged.",
    "Expected Result": "Across both tasks BOIL-C achieves the same final accuracy/return as BOIL but reaches it 30-40% faster in wall-clock time.  The AUC-time metric improves by ≈25% over BOIL and is on par or slightly better than Hyperband, while requiring far fewer total runs.",
    "Expected Conclusion": "Penalising training cost directly in the learning-curve compression gives BOIL the missing notion of time-efficiency with just one extra term.  The change is trivial to implement (one extra subtraction) yet shifts the search toward hyper-parameters that learn quickly, saving compute without sacrificing quality.  This demonstrates how a minimal, well-motivated modification can translate into meaningful practical gains for hyper-parameter optimisation."
}

# MODEL LIST
{
    "Large Language Models": {
        "Llama-4-Scout-17B-16E": {
            "model_parameters": {
                "total_parameters": "109b",
                "active_parameters": "17b"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": ""
        },
        "Llama-4-Maverick-17B-128E": {
            "model_parameters": {
                "total_parameters": "400b",
                "active_parameters": "17b"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": ""
        },
        "Qwen3-0.6B": {
            "model_parameters": "0.6b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-0.6B",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-1.7B": {
            "model_parameters": "1.7b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-1.7B",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-1.7B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-4B": {
            "model_parameters": "4b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-4B",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-4B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-8B": {
            "model_parameters": "8b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-8B",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-8B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-14B": {
            "model_parameters": "14b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-14B",
            "language_distribution": "",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-14B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-32B": {
            "model_parameters": "32.8b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-32B",
            "language_distribution": "",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-32B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "DeepSeek-v3": {
            "model_parameters": {
                "total_parameters": "671b",
                "active_parameters": "37b"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": "@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report},\n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437},\n}"
        },
        "DeepSeek-V3.1": {
            "model_parameters": {
                "total_parameters": "671B",
                "active_parameters": "37B"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "Text"
            ],
            "output_modalities": [
                "Text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": "@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report},\n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437},\n}"
        },
        "DeepSeek-V3.2-Exp": {
            "model_parameters": {
                "total_parameters": "671B",
                "active_parameters": "37B"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "Text"
            ],
            "output_modalities": [
                "Text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": "@misc{deepseekai2024deepseekv32,\n      title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention},\n      author={DeepSeek-AI},\n      year={2025},\n}"
        },
        "gpt-oss-20b": {
            "model_parameters": {
                "total_parameters": "21b",
                "active_parameters": "3.6b"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/openai/gpt-oss-20b",
            "context_length": "",
            "language_distribution": "multilingual",
            "input_modalities": "text",
            "output_modalities": "text",
            "dependent packages": [
                "accelerate",
                "transformers",
                "kernels"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"openai/gpt-oss-20b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\nmessages = [\n    {\"role\": \"user\", \"content\": \"How many rs are in the word 'strawberry'?\"},\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    return_dict=True,\n).to(model.device)\n\ngenerated = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))\n",
            "citation": "@misc{openai2025gptoss120bgptoss20bmodel,\n      title={gpt-oss-120b & gpt-oss-20b Model Card},\n      author={OpenAI},\n      year={2025},\n      eprint={2508.10925},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.10925},\n}"
        },
        "gemma-3-1b-it": {
            "model_parameters": "1b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/google/gemma-3-1b-it",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers"
            ],
            "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\n\tmessages,\n\tadd_generation_prompt=True,\n\ttokenize=True,\n\treturn_dict=True,\n\treturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
            "citation": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
        },
        "gemma-3-4b-it": {
            "model_parameters": "4b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/google/gemma-3-4b-it",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers"
            ],
            "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-4b-it\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\n\tmessages,\n\tadd_generation_prompt=True,\n\ttokenize=True,\n\treturn_dict=True,\n\treturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
            "citation": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
        },
        "gemma-3-27b-it": {
            "model_parameters": "27b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/google/gemma-3-27b-it",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers"
            ],
            "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-27b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-27b-it\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\n\tmessages,\n\tadd_generation_prompt=True,\n\ttokenize=True,\n\treturn_dict=True,\n\treturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
            "citation": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
        }
    },
    "Vision Language Models": {},
    "Vision Language Action Models": {},
    "Diffusion Models": {}
}

# DATASET LIST
{
    "Text Datasets": {
        "alpaca-cleaned": {
            "discription": "",
            "num_training_samples": "",
            "num_validation_samples": "",
            "huggingface_url": "https://huggingface.co/datasets/yahma/alpaca-cleaned",
            "language_distribution": "",
            "dependent packages": [],
            "code": "",
            "citation": ""
        },
        "databricks-dolly-15k": "",
        "gsm8k": {
            "discription": "A dataset of elementary school math word problems requiring 2 to 8 steps to solve",
            "num_training_samples": 7473,
            "num_validation_samples": 1319,
            "huggingface_url": "https://huggingface.co/datasets/openai/gsm8k",
            "language_distribution": "English",
            "dependent packages": [],
            "code": "",
            "citation": "@article{cobbe2021gsm8k,\n  title={Training Verifiers to Solve Math Word Problems},\n  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},\n  journal={arXiv preprint arXiv:2110.14168},\n  year={2021}\n}"
        },
        "MATH": {
            "discription": "The MATH dataset consists of approximately 12,500 mathematics problems ranging from middle school to early university level. Each problem includes a natural language question, a detailed step-by-step solution, and a final answer, and it is widely used to evaluate large language models (LLMs) in terms of their abilities in mathematical reasoning, logical inference, and step-by-step problem solving.",
            "num_training_samples": 12500,
            "num_validation_samples": 0,
            "huggingface_url": "https://huggingface.co/datasets/qwedsacf/competition_math",
            "language_distribution": "English",
            "dependent packages": [],
            "code": "",
            "example": "{'problem': 'A board game spinner is divided into three parts labeled $A$, $B$  and $C$. The probability of the spinner landing on $A$ is $\\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\\frac{5}{12}$.  What is the probability of the spinner landing on $C$? Express your answer as a common fraction.',\n 'level': 'Level 1',\n 'type': 'Counting & Probability',\n 'solution': 'The spinner is guaranteed to land on exactly one of the three regions, so we know that the sum of the probabilities of it landing in each region will be 1. If we let the probability of it landing in region $C$ be $x$, we then have the equation $1 = \\frac{5}{12}+\\frac{1}{3}+x$, from which we have $x=\\boxed{\\frac{1}{4}}$.'}",
            "data_structure": "A data instance consists of a competition math problem and its step-by-step solution written in LaTeX and natural language. The step-by-step solution contains the final answer enclosed in LaTeX's \boxed tag.\n- problem: The competition math problem.\n- solution: The step-by-step solution.\n- level: The problem's difficulty level from 'Level 1' to 'Level 5', where a subject's easiest problems for humans are assigned to 'Level 1' and a subject's hardest problems are assigned to 'Level 5'.\n- type: The subject of the problem: Algebra, Counting & Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra and Precalculus.",
            "citation": "@article{hendrycksmath2021,\n    title={Measuring Mathematical Problem Solving With the MATH Dataset},\n    author={Dan Hendrycks\n    and Collin Burns\n    and Saurav Kadavath\n    and Akul Arora\n    and Steven Basart\n    and Eric Tang\n    and Dawn Song\n    and Jacob Steinhardt},\n    journal={arXiv preprint arXiv:2103.03874},\n    year={2021}\n}"
        }
    },
    "Image Datasets": {
        "ImageNet": "",
        "CIFAR-10": ""
    }
}

# Output Format
- experiment_summary：
  - Describe the overall implementation details of the experiment. Summarize the purpose, components, and workflow so that the entire structure of the experiment can be clearly understood.
- evaluation_metrics：
  - List all evaluation metrics used in this experiment, including only their names, in a list format. (e.g., Accuracy AUC ROC, F1 Score, RMSE, BLEU, ROUGE, etc.)
- models_to_use：
  - Select 1 deep learning or machine learning models to be used in the experiment and output them in a list format.
  - Each model name should clearly indicate its number of parameters.
  - Refer to the provided “# MODEL LIST” for guidance, although models not included in the list are also acceptable.
  - If the proposed method itself introduces a new model (e.g., a novel architecture), return an empty list and describe the details of the method in new_method.
- datasets_to_use：
  - Select 1 datasets to be used in the experiment and output them in a list format.
  - Refer to the provided “# DATASET LIST” for guidance, although datasets not included in the list are also acceptable.
  - If a new dataset is proposed as part of this study, return an empty list and describe its details in new_method.
- proposed_method：
  - Describe the proposed method and its implementation in detail.
  - Clearly state its objectives, theoretical background, components, and algorithmic procedures.
- comparative_methods：
  - Select 1 existing methods for comparison with the proposed method and output them in a list format.
  - For example, if the proposed method is a new optimization algorithm, comparative methods might include Adam or AdamW.
  - If the proposal is a new LLM architecture, comparative methods might include Llama 4 or Qwen.
- hyperparameters_to_search：
  - Output a list of objects, where each object contains "name" (hyperparameter name) and "range" (search range).
  - For example: [{"name": "learning_rate", "range": "0.001-0.01"}, {"name": "batch_size", "range": "16,32,64"}, {"name": "weight_decay", "range": "0.0001-0.001"}]
  - Search ranges can be expressed as ranges (e.g., "0.001-0.01") or discrete values (e.g., "16,32,64").
Output:
{
    "experiment_summary": "Purpose: Demonstrate that the proposed Cost-Aware Learning-Curve Compression (BOIL-C) accelerates Bayesian Optimisation for Hyper-parameter Tuning without hurting final performance.\n\nComponents & workflow:\n1. Task: Image classification on CIFAR-10 with a 4-layer convolutional neural network (~1.2 M params).\n2. Hyper-parameter search space: learning-rate, batch-size, dropout.\n3. Methods: (a) BOIL-C (proposed) – modifies BOIL’s scalar learning-curve compression with a log-cost penalty; (b) BOIL (original) – comparative baseline.\n4. Each optimiser receives an identical budget of 8 physical GPU-hours and is run with 5 independent random seeds. During the search every partial training curve of the CNN is compressed to a scalar and fed to a Gaussian-process surrogate; the acquisition function chooses the next configuration.\n5. Hardware: single NVIDIA A100; multiple seeds run in parallel across the 8 available GPUs to exhaust the budget efficiently.\n6. Logging: For every wall-clock second we record the best-so-far validation accuracy; these traces are later integrated to obtain the AUC-Time metric.\n7. Evaluation: Compare (i) AUC of best validation-accuracy versus time, and (ii) final validation accuracy at budget exhaustion.\n\nOverall, the experiment quantifies how much faster BOIL-C reaches high accuracy relative to BOIL while maintaining the same final score.",
    "evaluation_metrics": [
        "AUC_Time (Best Accuracy vs Wall-Clock Time)",
        "Final Validation Accuracy"
    ],
    "models_to_use": [
        "Small-CNN-1.2M"
    ],
    "datasets_to_use": [
        "CIFAR-10"
    ],
    "proposed_method": "BOIL-C augments BOIL’s learning-curve compression with a cost term. For a run x after observing t training checkpoints we compute:\n    u(x,t) = s(r(x,t); m0,g0) – β · log(1 + C(x,t))\nwhere s(·) is BOIL’s sigmoid of the current validation accuracy r(x,t), C(x,t) is the cumulative training cost in seconds, and β∈[0,1] (fixed to 0.25 in the main experiment). Only this scalar fed to the Gaussian-process surrogate changes; GP training, acquisition optimisation (e.g., Expected Improvement), and the outer BO loop remain untouched. The subtraction biases the search toward configurations that obtain high accuracy rapidly, thereby reducing wall-clock time consumption. Implementation requires adding one line in the compression routine:\n    scalar = sigmoid_score – beta * np.log1p(cumulative_cost)",
    "comparative_methods": [
        "BOIL (Original)"
    ],
    "hyperparameters_to_search": [
        {
            "name": "learning_rate",
            "range": "0.0001-0.1"
        },
        {
            "name": "batch_size",
            "range": "32,64,128"
        },
        {
            "name": "dropout",
            "range": "0.0-0.5"
        }
    ]
}
