
LLM Name: o3-2025-04-16
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path} mode=full

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} mode=trial
```
- `run`: Experiment run_id (matching a run_id from config/run/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `mode`: Execution mode (required parameter)
  * `mode=trial`: Lightweight execution for validation (epochs=1, batches limited to 1-2, wandb.mode=disabled, optuna.n_trials=0)
  * `mode=full`: Full experiment execution (wandb.mode=online, full epochs, full Optuna trials)
  * **Code must automatically configure based on mode (e.g., `if cfg.mode == "trial": cfg.wandb.mode = "disabled"; cfg.optuna.n_trials = 0` elif `cfg.mode == "full": cfg.wandb.mode = "online"`)**

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- `run_ids`: JSON string list of run IDs to evaluate (e.g., '["run-1-proposed-bert-glue", "run-2-baseline-bert-glue"]')
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * **Optuna Integration**: If using Optuna for hyperparameter search, DO NOT log intermediate trial results to WandB - only train once with the best hyperparameters after optimization completes and log that final run
  * **Log ALL metrics to WandB comprehensively**:
    - Use `wandb.log()` at each training step/batch/epoch with ALL relevant metrics
    - Log as frequently as possible (per-batch or per-epoch) to capture training dynamics
  * **Save final/best metrics to WandB summary**:
    - Use `wandb.summary["key"] = value` for final results
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse command line arguments:
    - `results_dir`: Output directory path
    - `run_ids`: JSON string list of run IDs (parse with `json.loads(args.run_ids)`)
  * Load WandB config from `{results_dir}/config.yaml`
  * **Retrieve comprehensive experimental data from WandB API** for specified run_ids:
    ```python
    import json
    api = wandb.Api()
    run_ids = json.loads(args.run_ids)  # Parse JSON string to list
    for run_id in run_ids:
        run = api.run(f"{entity}/{project}/{run_id}")
        history = run.history()  # pandas DataFrame with ALL time-series metrics (train_loss, val_acc, etc.)
        summary = run.summary._json_dict  # Final/best metrics (best_val_acc, final_test_acc, etc.)
        config = dict(run.config)  # Run configuration (hyperparameters, model settings, etc.)
    ```
  * **STEP 1: Per-Run Processing** (for each run_id):
    - Export **comprehensive** run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
    - Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
    - Each run should have its own subdirectory with its metrics and figures
  * **STEP 2: Aggregated Analysis** (after processing all runs):
    - Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
    - Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
    - Generate comparison figures to: `{results_dir}/comparison/`:
      * Cross-run comparison charts (bar charts, box plots)
      * Performance metrics tables
      * Statistical significance tests
  * **Figure Generation Guidelines**:
    - Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
    - Use GLOBALLY UNIQUE image filenames to prevent collisions across different runs and directories**:
      * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
      * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
  * Print all generated file paths to stdout (both per-run and comparison)

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- **Mode handling**: Automatically configure based on `cfg.mode`:
  * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1, etc.
  * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 251023-test
    mode: online  # Automatically set to "disabled" in trial_mode
  ```
- `WANDB_API_KEY` environment variable is automatically available for authentication

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Mode-Based Behavior**: Code must automatically configure based on `cfg.mode` ("trial" vs "full")
   - `mode=trial`: Set `cfg.wandb.mode="disabled"`, `cfg.optuna.n_trials=0`, epochs=1, limited batches
   - `mode=full`: Set `cfg.wandb.mode="online"`, use full configuration
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: Evaluation aggregated analysis is incomplete: evaluate.py does not compute any secondary / derived metrics (e.g. improvement-rate (proposed-baseline)/baseline) as required by criterion 8.  All other core requirements appear satisfied but this missing derived-metric step violates the specification, so the code is not yet ready for immediate execution.
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "# config/config.yaml\n# Global Hydra configuration.  Each experiment must supply `run=\u003crun_id\u003e` on\n# the command-line so that the corresponding file in config/run/ is loaded.\n\ndefaults:\n  - _self_\n  - run: ???   # mandatory override \u2013 points to config/run/\u003crun_id\u003e.yaml\n\n# ---------------------------------------------------------------------------\n# Global settings (can be overridden from CLI)\n# ---------------------------------------------------------------------------\nmode: full         # full | trial\nresults_dir: ./results\ntrial_limited_batches: 2  # how many batches to run in trial mode\n\nwandb:\n  entity: gengaru617-personal\n  project: 251023-test\n  mode: online   # switched to \"disabled\" automatically when mode=trial\n\n# Empty placeholders so OmegaConf structure is defined before composition\nrun: {}\nmethod: {}\nmodel: {}\ndataset: {}\ntraining: {}\noptuna: {}\nresources: {}\n", "evaluate_py": "\"\"\"src/evaluate.py\nIndependent evaluation / visualisation.\nFetches metrics from WandB and writes per-run \u0026 aggregated artefacts.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb  # type: ignore\nfrom scipy import stats\n\nsns.set(style=\"whitegrid\")\n\n# -----------------------------------------------------------------------------\n# Figure helpers\n# -----------------------------------------------------------------------------\n\ndef _plot_learning_curve(path: Path, history: pd.DataFrame, run_id: str) -\u003e str:\n    plt.figure(figsize=(6, 4))\n    if \"train_acc\" in history:\n        sns.lineplot(x=history[\"epoch\"], y=history[\"train_acc\"], label=\"train_acc\")\n    if \"val_acc\" in history:\n        sns.lineplot(x=history[\"epoch\"], y=history[\"val_acc\"], label=\"val_acc\")\n    if \"test_acc\" in history:\n        sns.lineplot(x=history[\"epoch\"], y=history[\"test_acc\"], label=\"test_acc\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.tight_layout()\n    fname = f\"{run_id}_learning_curve.pdf\"\n    plt.savefig(path / fname)\n    plt.close()\n    return str(path / fname)\n\n\ndef _plot_confusion_matrix(cm: np.ndarray, classes: List[str], path: Path, run_id: str) -\u003e str:\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.title(\"Confusion Matrix\")\n    plt.tight_layout()\n    fname = f\"{run_id}_confusion_matrix.pdf\"\n    plt.savefig(path / fname)\n    plt.close()\n    return str(path / fname)\n\n\ndef _bar_chart(path: Path, metric_dict: Dict[str, float], metric_name: str) -\u003e str:\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=list(metric_dict.keys()), y=list(metric_dict.values()))\n    for i, v in enumerate(metric_dict.values()):\n        plt.text(i, v + 0.002, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.ylabel(metric_name)\n    plt.tight_layout()\n    fname = f\"comparison_{metric_name}_bar_chart.pdf\"\n    plt.savefig(path / fname)\n    plt.close()\n    return str(path / fname)\n\n\ndef _box_plot(path: Path, metric_dict: Dict[str, float], metric_name: str) -\u003e str:\n    plt.figure(figsize=(6, 4))\n    df = pd.DataFrame({\"run\": list(metric_dict.keys()), metric_name: list(metric_dict.values())})\n    sns.boxplot(x=\"run\", y=metric_name, data=df)\n    plt.tight_layout()\n    fname = f\"comparison_{metric_name}_boxplot.pdf\"\n    plt.savefig(path / fname)\n    plt.close()\n    return str(path / fname)\n\n# -----------------------------------------------------------------------------\n# Main evaluation pipeline\n# -----------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Evaluate WandB runs \u0026 generate artefacts\")\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory containing experiment metadata\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON list of WandB run IDs\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir).expanduser().resolve()\n    run_ids: List[str] = json.loads(args.run_ids)\n\n    # Load global WandB info ----------------------------------------------------\n    import yaml  # local import to avoid mandatory dependency when not needed\n\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(f\"{cfg_path} not found \u2013 cannot resolve WandB project info\")\n    with cfg_path.open() as f:\n        global_cfg = yaml.safe_load(f)\n    entity = global_cfg[\"wandb\"][\"entity\"]\n    project = global_cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    per_run_metric: Dict[str, float] = {}\n    generated_files: List[str] = []\n\n    for rid in run_ids:\n        print(f\"\\nProcessing run {rid}\")\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        history = run.history(keys=[\"train_acc\", \"val_acc\", \"test_acc\", \"epoch\"], pandas=True)\n        summary = dict(run.summary._json_dict)\n        config = dict(run.config)\n\n        run_dir = results_dir / rid\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save metrics.json -----------------------------------------------------\n        metrics_out = {\n            \"history\": history.to_dict(orient=\"list\"),\n            \"summary\": summary,\n            \"config\": config,\n        }\n        with (run_dir / \"metrics.json\").open(\"w\") as f:\n            json.dump(metrics_out, f, indent=2)\n\n        # Learning curve figure -------------------------------------------------\n        lc_fig = _plot_learning_curve(run_dir, history, rid)\n        generated_files.append(lc_fig)\n\n        # Confusion matrix figure ----------------------------------------------\n        if \"confusion_matrix\" in summary:\n            cm_arr = np.array(summary[\"confusion_matrix\"])  # type: ignore[arg-type]\n            class_names = config.get(\"dataset\", {}).get(\"class_names\", []) or [str(i) for i in range(cm_arr.shape[0])]\n            cm_fig = _plot_confusion_matrix(cm_arr, class_names, run_dir, rid)\n            generated_files.append(cm_fig)\n\n        # Collect metric for aggregation ---------------------------------------\n        per_run_metric[rid] = summary.get(\"final_test_acc\", summary.get(\"best_val_acc\", 0.0))\n\n    # -------------------------------------------------------------------------\n    # Aggregated comparison\n    # -------------------------------------------------------------------------\n    comp_dir = results_dir / \"comparison\"\n    comp_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save aggregated metrics --------------------------------------------------\n    with (comp_dir / \"aggregated_metrics.json\").open(\"w\") as f:\n        json.dump(per_run_metric, f, indent=2)\n\n    # Plot bar \u0026 box charts ----------------------------------------------------\n    bar_fig = _bar_chart(comp_dir, per_run_metric, \"accuracy\")\n    box_fig = _box_plot(comp_dir, per_run_metric, \"accuracy\")\n    generated_files.extend([bar_fig, box_fig])\n\n    # Statistical significance tests ------------------------------------------\n    baseline_key = run_ids[0]\n    sig_results = {}\n    for k, v in per_run_metric.items():\n        if k == baseline_key:\n            continue\n        # Two-sample t-test; with single values will return nan but keeps API consistent\n        try:\n            t_stat, p_val = stats.ttest_ind([per_run_metric[baseline_key]], [v], equal_var=False)\n        except Exception:\n            t_stat, p_val = float(\"nan\"), float(\"nan\")\n        sig_results[k] = {\"t_stat\": t_stat, \"p_val\": p_val}\n    with (comp_dir / \"significance_tests.json\").open(\"w\") as f:\n        json.dump(sig_results, f, indent=2)\n\n    # -------------------------------------------------------------------------\n    # Print generated artefact paths ------------------------------------------\n    print(\"\\nGenerated files:\")\n    for fp in generated_files:\n        print(fp)\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "\"\"\"src/main.py\nTop-level orchestrator \u2013 spawns the actual training run in a subprocess so that\nGitHub Actions (or any scheduler) can launch many runs independently.\n\"\"\"\nfrom __future__ import annotations\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom hydra.utils import get_original_cwd\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg):  # type: ignore\n    # Validate mode ------------------------------------------------------------\n    if cfg.mode not in (\"trial\", \"full\"):\n        raise ValueError(\"mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    # Prepare subprocess command ----------------------------------------------\n    original_cwd = Path(get_original_cwd())\n\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run.run_id}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n\n    print(\"Launching training subprocess:\\n\", \" \".join(cmd))\n    subprocess.run(cmd, cwd=original_cwd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py\nConvolutional network architectures used in the experiments.\n\"\"\"\nfrom __future__ import annotations\n\nimport torch\nimport torch.nn as nn\n\n\ndef _conv_block(in_c: int, out_c: int, k: int, s: int, p: int) -\u003e nn.Sequential:\n    \"\"\"Helper to create Conv-BN-ReLU block.\"\"\"\n    return nn.Sequential(\n        nn.Conv2d(in_c, out_c, k, stride=s, padding=p, bias=False),\n        nn.BatchNorm2d(out_c),\n        nn.ReLU(inplace=True),\n    )\n\n\nclass SmallCNN(nn.Module):\n    \"\"\"Configurable small CNN (~1.2 M parameters).\"\"\"\n\n    def __init__(self, cfg, dropout: float):\n        super().__init__()\n        layers_cfg = cfg.model.conv_layers\n        layers = []\n        in_ch = 3\n        for idx, conv_cfg in enumerate(layers_cfg):\n            layers.append(\n                _conv_block(\n                    in_c=in_ch,\n                    out_c=int(conv_cfg.out_channels),\n                    k=int(conv_cfg.kernel_size),\n                    s=int(conv_cfg.stride),\n                    p=int(conv_cfg.padding),\n                )\n            )\n            in_ch = int(conv_cfg.out_channels)\n            if (idx + 1) % 2 == 0:\n                layers.append(nn.MaxPool2d(2))\n        self.features = nn.Sequential(*layers)\n\n        # Compute flattened feature dimension ----------------------------------\n        with torch.no_grad():\n            dummy = torch.zeros(1, 3, 32, 32)\n            feat_dim = self.features(dummy).view(1, -1).size(1)\n\n        fc_layers = []\n        last = feat_dim\n        for fc_cfg in cfg.model.fc_layers:\n            fc_layers.extend(\n                [\n                    nn.Linear(last, int(fc_cfg.out_features)),\n                    nn.ReLU(inplace=True),\n                    nn.Dropout(p=dropout),\n                ]\n            )\n            last = int(fc_cfg.out_features)\n        fc_layers.append(nn.Linear(last, 10))  # CIFAR-10 has 10 classes\n        self.classifier = nn.Sequential(*fc_layers)\n\n    def forward(self, x):  # type: ignore[override]\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x)\n\n\ndef build_model(cfg, dropout: float):\n    name = cfg.model.name.lower()\n    if name.startswith(\"small-cnn\"):\n        return SmallCNN(cfg, dropout)\n    raise ValueError(f\"Unknown model {cfg.model.name}\")\n", "preprocess_py": "\"\"\"src/preprocess.py\nData loading \u0026 augmentation for CIFAR-10 (cached to .cache/).\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import List, Optional, Tuple\n\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import CIFAR10\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Transform builder\n# -----------------------------------------------------------------------------\n\ndef _build_transforms(transform_cfgs: List[dict]):\n    tfms: List[T.transforms.Compose] = []\n    for cfg in transform_cfgs:\n        if not isinstance(cfg, dict):\n            raise ValueError(\"Each transform entry must be a dict {Name: params}\")\n        name, params = list(cfg.items())[0]\n        params = params or {}\n        cls = getattr(T, name)\n        tfms.append(cls(**params))\n    return T.Compose(tfms)\n\n\n# -----------------------------------------------------------------------------\n# Public loader factory\n# -----------------------------------------------------------------------------\n\ndef get_dataloaders(cfg, batch_size: Optional[int] = None) -\u003e Tuple[DataLoader, DataLoader, DataLoader]:\n    bs = batch_size or int(cfg.training.batch_size)\n\n    transform_train = _build_transforms(cfg.dataset.transforms)\n    # Validation/test \u2013 keep only basic preprocessing (ToTensor + Normalize)\n    basic = [d for d in cfg.dataset.transforms if list(d.keys())[0] in (\"ToTensor\", \"Normalize\")]\n    transform_val = _build_transforms(basic)\n\n    full_train = CIFAR10(root=CACHE_DIR, train=True, download=True, transform=transform_train)\n    test_ds = CIFAR10(root=CACHE_DIR, train=False, download=True, transform=transform_val)\n\n    val_size = int(cfg.dataset.val_split)\n    train_size = len(full_train) - val_size\n    train_ds, val_ds = random_split(full_train, [train_size, val_size])\n\n    loader_kwargs = dict(batch_size=bs, num_workers=4, pin_memory=True)\n    train_loader = DataLoader(train_ds, shuffle=True, **loader_kwargs)\n    val_loader = DataLoader(val_ds, shuffle=False, **loader_kwargs)\n    test_loader = DataLoader(test_ds, shuffle=False, **loader_kwargs)\n    return train_loader, val_loader, test_loader\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"boil_c_experiments\"\nversion = \"0.1.0\"\ndescription = \"Cost-Aware BOIL Experiments with Hydra\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\nhydra-core = \"^1.3.2\"\nomegaconf = \"^2.3.0\"\ntorch = \"^2.1.0\"\ntorchvision = \"^0.16.0\"\nwandb = \"^0.16.0\"\noptuna = \"^3.4.0\"\nseaborn = \"^0.13.0\"\nmatplotlib = \"^3.8.0\"\npandas = \"^2.1.0\"\nscikit-learn = \"^1.3.0\"\nscipy = \"^1.11.0\"\n", "train_py": "\"\"\"src/train.py\nSingle-run training / hyper-parameter optimisation with Hydra, Optuna \u0026 WandB.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nimport time\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nimport hydra\nimport numpy as np\nimport optuna\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import OmegaConf\nfrom sklearn.metrics import confusion_matrix\n\nfrom .model import build_model\nfrom .preprocess import get_dataloaders\n\nimport wandb  # type: ignore\n\n# -----------------------------------------------------------------------------\n# Helper utilities\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int) -\u003e None:\n    \"\"\"Set all relevant random seeds for reproducibility.\"\"\"\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -\u003e float:\n    \"\"\"Return top-1 accuracy for a mini-batch.\"\"\"\n    return (pred.argmax(dim=1) == target).float().mean().item()\n\n\n# -----------------------------------------------------------------------------\n# Cost-aware compression helpers (BOIL-C)\n# -----------------------------------------------------------------------------\n\ndef sigmoid_score(metric: float, m0: float = 0.5, g0: float = 0.1) -\u003e float:\n    return 1.0 / (1.0 + np.exp(-(metric - m0) / g0))\n\n\ndef compress_curve(score: float, cumulative_cost: float, beta: float) -\u003e float:\n    \"\"\"Equation (BOIL-C): penalise by compute cost (seconds).\"\"\"\n    return score - beta * np.log1p(cumulative_cost)\n\n\n# -----------------------------------------------------------------------------\n# Epoch-level routines\n# -----------------------------------------------------------------------------\n\ndef _train_one_epoch(\n    model: nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    criterion: nn.Module,\n    optimiser: optim.Optimizer,\n    device: torch.device,\n    epoch: int,\n    cfg,\n    wdb_run,\n) -\u003e Tuple[float, float]:\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    num_samples = 0\n    for batch_idx, (x, y) in enumerate(dataloader):\n        if cfg.mode == \"trial\" and batch_idx \u003e= cfg.trial_limited_batches:\n            break\n        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n        optimiser.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimiser.step()\n\n        bs = x.size(0)\n        running_loss += loss.item() * bs\n        running_acc += accuracy(out.detach(), y) * bs\n        num_samples += bs\n\n    epoch_loss = running_loss / num_samples\n    epoch_acc = running_acc / num_samples\n    if wdb_run is not None:\n        wdb_run.log({\"train_loss\": epoch_loss, \"train_acc\": epoch_acc, \"epoch\": epoch})\n    return epoch_loss, epoch_acc\n\n\ndef _eval_split(\n    model: nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    criterion: nn.Module,\n    device: torch.device,\n    epoch: int,\n    split: str,\n    wdb_run,\n) -\u003e Tuple[float, float]:\n    model.eval()\n    loss_sum = 0.0\n    acc_sum = 0.0\n    num_samples = 0\n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            bs = x.size(0)\n            loss_sum += loss.item() * bs\n            acc_sum += accuracy(out, y) * bs\n            num_samples += bs\n    loss_avg = loss_sum / num_samples\n    acc_avg = acc_sum / num_samples\n    if wdb_run is not None:\n        wdb_run.log({f\"{split}_loss\": loss_avg, f\"{split}_acc\": acc_avg, \"epoch\": epoch})\n    return loss_avg, acc_avg\n\n\n# -----------------------------------------------------------------------------\n# Optuna objective \u2013 returns best validation accuracy of a trial\n# -----------------------------------------------------------------------------\n\ndef build_objective(cfg, device):\n    beta = float(cfg.method.get(\"beta\", 0.0))\n\n    def objective(trial: optuna.Trial):\n        # Sample hyper-parameters ------------------------------------------------\n        lr = trial.suggest_float(\"learning_rate\", **cfg.optuna.search_space.learning_rate)\n        batch_size = trial.suggest_categorical(\"batch_size\", cfg.optuna.search_space.batch_size.choices)\n        dropout = trial.suggest_float(\"dropout\", **cfg.optuna.search_space.dropout)\n\n        # Data loaders ----------------------------------------------------------\n        train_loader, val_loader, _ = get_dataloaders(cfg, batch_size=batch_size)\n\n        # Model / optimiser -----------------------------------------------------\n        model = build_model(cfg, dropout).to(device)\n        crit = nn.CrossEntropyLoss()\n        opt = optim.SGD(\n            model.parameters(),\n            lr=lr,\n            momentum=cfg.training.momentum,\n            weight_decay=cfg.training.weight_decay,\n        )\n        sch = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.training.epochs)\n\n        best_val_acc = 0.0\n        start_time = time.time()\n        for ep in range(cfg.training.epochs):\n            _train_one_epoch(model, train_loader, crit, opt, device, ep, cfg, wdb_run=None)\n            _, v_acc = _eval_split(model, val_loader, crit, device, ep, \"val\", wdb_run=None)\n            sch.step()\n            best_val_acc = max(best_val_acc, v_acc)\n\n            score = sigmoid_score(v_acc)\n            scalar = (\n                compress_curve(score, time.time() - start_time, beta)\n                if cfg.method.name.lower() == \"boil-c\"\n                else score\n            )\n            trial.report(scalar, step=ep)\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n        return best_val_acc\n\n    return objective\n\n\n# -----------------------------------------------------------------------------\n# Hydra entry-point\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg):  # type: ignore\n    # ------------------------------------------------------------------\n    # Mode switch (trial / full)\n    # ------------------------------------------------------------------\n    if cfg.mode not in (\"trial\", \"full\"):\n        raise ValueError(\"mode must be \u0027trial\u0027 or \u0027full\u0027\")\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    else:\n        cfg.wandb.mode = \"online\"\n\n    # ------------------------------------------------------------------\n    # Directories \u0026 config snapshot\n    # ------------------------------------------------------------------\n    results_dir = Path(cfg.results_dir).expanduser().resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n    OmegaConf.save(cfg, str(results_dir / \"config.yaml\"))\n\n    # ------------------------------------------------------------------\n    # WandB session ----------------------------------------------------\n    # ------------------------------------------------------------------\n    wdb_run = None\n    if cfg.wandb.mode != \"disabled\":\n        wdb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            dir=str(results_dir),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n            config=OmegaConf.to_container(cfg, resolve=True),\n        )\n        print(f\"WandB URL: {wdb_run.url}\")\n\n    # ------------------------------------------------------------------\n    # Reproducibility \u0026 device ----------------------------------------\n    # ------------------------------------------------------------------\n    set_seed(cfg.method.seeds[0] if \"seeds\" in cfg.method else 0)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Data loaders (initial hyper-params from cfg) ---------------------\n    # ------------------------------------------------------------------\n    train_loader_full, val_loader_full, test_loader = get_dataloaders(cfg)\n\n    # ------------------------------------------------------------------\n    # Optuna hyper-parameter tuning -----------------------------------\n    # ------------------------------------------------------------------\n    best_params = {\n        \"learning_rate\": float(cfg.training.learning_rate),\n        \"batch_size\": int(cfg.training.batch_size),\n        \"dropout\": float(cfg.model.dropout),\n    }\n\n    if int(cfg.optuna.n_trials) \u003e 0:\n        study = optuna.create_study(\n            direction=cfg.optuna.direction,\n            sampler=optuna.samplers.TPESampler(seed=0),\n            pruner=optuna.pruners.MedianPruner(),\n        )\n        study.optimize(build_objective(cfg, device), n_trials=int(cfg.optuna.n_trials))\n        best_params.update(study.best_params)\n        if wdb_run is not None:\n            wdb_run.summary.update({f\"optuna/best_{k}\": v for k, v in study.best_params.items()})\n\n    # ------------------------------------------------------------------\n    # Final training with best hyper-params ----------------------------\n    # ------------------------------------------------------------------\n    train_loader, val_loader, _ = get_dataloaders(cfg, batch_size=int(best_params[\"batch_size\"]))\n    model = build_model(cfg, dropout=float(best_params[\"dropout\"])).to(device)\n    crit = nn.CrossEntropyLoss()\n    opt = optim.SGD(\n        model.parameters(),\n        lr=float(best_params[\"learning_rate\"]),\n        momentum=cfg.training.momentum,\n        weight_decay=cfg.training.weight_decay,\n    )\n    sch = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.training.epochs)\n\n    best_val_acc = 0.0\n    for ep in range(cfg.training.epochs):\n        _train_one_epoch(model, train_loader, crit, opt, device, ep, cfg, wdb_run)\n        _, v_acc = _eval_split(model, val_loader, crit, device, ep, \"val\", wdb_run)\n        best_val_acc = max(best_val_acc, v_acc)\n        sch.step()\n\n    # ------------------------------------------------------------------\n    # Final test evaluation \u0026 confusion matrix ------------------------\n    # ------------------------------------------------------------------\n    test_loss, test_acc = _eval_split(model, test_loader, crit, device, cfg.training.epochs, \"test\", wdb_run)\n\n    all_preds: List[int] = []\n    all_targets: List[int] = []\n    model.eval()\n    with torch.no_grad():\n        for x, y in test_loader:\n            x = x.to(device)\n            preds = model(x).argmax(dim=1)\n            all_preds.extend(preds.cpu().tolist())\n            all_targets.extend(y.tolist())\n    cm = confusion_matrix(all_targets, all_preds)\n    class_names = test_loader.dataset.classes  # type: ignore[attr-defined]\n\n    if wdb_run is not None:\n        cm_plot = wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=class_names,\n        )\n        wdb_run.log({\"confusion_matrix\": cm_plot})\n        wdb_run.summary.update(\n            {\n                \"best_val_acc\": best_val_acc,\n                \"final_test_loss\": test_loss,\n                \"final_test_acc\": test_acc,\n                \"confusion_matrix\": cm.tolist(),\n            }\n        )\n        wdb_run.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "BOIL transforms every partial learning curve into a single scalar via a fixed-shape sigmoid. The score is independent of how much compute was spent to obtain the curve: a run that reaches 90% accuracy after 200 epochs receives the same utility as one that reaches 90% in 20 epochs. Consequently BOIL may keep sampling hyper-parameters that learn slowly but ultimately perform well, wasting wall-clock time.",
    "Methods": "Cost–Aware Learning-Curve Compression (BOIL-C).\nModification (one line change in the compression routine):\n    u(x,t) = s( r(x,t); m0,g0 )  –  β · log( 1 + C(x,t) )\nwhere\n• s(·) is BOIL’s original sigmoid compression,  \n• C(x,t)=∑_{i=1}^{t} c(x,i) is the cumulative observed training cost (in seconds),\n• β∈[0,1] is a small constant or learned alongside m0,g0 by marginal-likelihood maximisation.\n\nInterpretation: we keep BOIL’s performance-based score but subtract a logarithmic penalty that grows with consumed compute, favouring hyper-params that reach good scores quickly.  Only the single scalar fed to the GP changes; the surrogate, acquisition function and optimisation loop are untouched.",
    "Experimental Setup": "Datasets: CIFAR-10 image classification with a small CNN; CartPole-v0 reinforcement learning with DQN (same as BOIL).\nHyper-parameters to tune: learning-rate, batch-size, and dropout for CNN; lr and target-update for DQN.\nMethods compared:\n1) BOIL (original)\n2) BOIL-C (ours, β=0.25)\n3) Hyperband (strong cost-aware baseline)\nBudget: 8 GPU hours per method, 5 independent seeds.\nMetric: best validation accuracy (CNN) / average return (RL) reached versus wall-clock time.  Report area-under-curve (AUC) of best-so-far metric w.r.t. time.",
    "Experimental Code": "# --- key modification only ----------------------------------------------------\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# inside BOIL class, replace original compression -----------------------------\n\ndef compress_curve(sigmoid_score, cumulative_cost, beta=0.25):\n    \"\"\"Return cost-aware scalar for GP.  Inputs are scalars.\"\"\"\n    return sigmoid_score - beta * np.log1p(cumulative_cost)\n\n# example usage ---------------------------------------------------------------\n# r_t: current accuracy at epoch t, m0,g0 learned as in BOIL\nsigmoid_score = 1 / (1 + np.exp(-(r_t - m0)/g0))\nscalar_for_gp = compress_curve(sigmoid_score, cumulative_cost)\n# everything else in BOIL (GP update, acquisition, etc.) stays unchanged.",
    "Expected Result": "Across both tasks BOIL-C achieves the same final accuracy/return as BOIL but reaches it 30-40% faster in wall-clock time.  The AUC-time metric improves by ≈25% over BOIL and is on par or slightly better than Hyperband, while requiring far fewer total runs.",
    "Expected Conclusion": "Penalising training cost directly in the learning-curve compression gives BOIL the missing notion of time-efficiency with just one extra term.  The change is trivial to implement (one extra subtraction) yet shifts the search toward hyper-parameters that learn quickly, saving compute without sacrificing quality.  This demonstrates how a minimal, well-motivated modification can translate into meaningful practical gains for hyper-parameter optimisation."
}

# Experimental Design
- Summary: Purpose: Demonstrate that the proposed Cost-Aware Learning-Curve Compression (BOIL-C) accelerates Bayesian Optimisation for Hyper-parameter Tuning without hurting final performance.

Components & workflow:
1. Task: Image classification on CIFAR-10 with a 4-layer convolutional neural network (~1.2 M params).
2. Hyper-parameter search space: learning-rate, batch-size, dropout.
3. Methods: (a) BOIL-C (proposed) – modifies BOIL’s scalar learning-curve compression with a log-cost penalty; (b) BOIL (original) – comparative baseline.
4. Each optimiser receives an identical budget of 8 physical GPU-hours and is run with 5 independent random seeds. During the search every partial training curve of the CNN is compressed to a scalar and fed to a Gaussian-process surrogate; the acquisition function chooses the next configuration.
5. Hardware: single NVIDIA A100; multiple seeds run in parallel across the 8 available GPUs to exhaust the budget efficiently.
6. Logging: For every wall-clock second we record the best-so-far validation accuracy; these traces are later integrated to obtain the AUC-Time metric.
7. Evaluation: Compare (i) AUC of best validation-accuracy versus time, and (ii) final validation accuracy at budget exhaustion.

Overall, the experiment quantifies how much faster BOIL-C reaches high accuracy relative to BOIL while maintaining the same final score.
- Evaluation metrics: ['AUC_Time (Best Accuracy vs Wall-Clock Time)', 'Final Validation Accuracy']

# Experiment Runs

- Run ID: proposed-Small-CNN-1.2M-CIFAR-10
  Method: proposed
  Model: Small-CNN-1.2M
  Dataset: CIFAR-10
  Config File: config/run/proposed-Small-CNN-1.2M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-Small-CNN-1.2M-CIFAR-10
method:
  name: BOIL-C
  type: proposed
  beta: 0.25
  compression_formula: "u(x,t) = s(r(x,t); m0,g0) - beta * log(1 + C(x,t))"
  surrogate:
    type: gaussian_process
    kernel: matern52
    noise: 1e-3
  acquisition_function: expected_improvement
  seeds: [0, 1, 2, 3, 4]
model:
  name: Small-CNN-1.2M
  conv_layers:
    - out_channels: 64
      kernel_size: 3
      stride: 1
      padding: 1
    - out_channels: 128
      kernel_size: 3
      stride: 1
      padding: 1
    - out_channels: 256
      kernel_size: 3
      stride: 1
      padding: 1
    - out_channels: 256
      kernel_size: 3
      stride: 1
      padding: 1
  fc_layers:
    - out_features: 512
  activation: relu
  dropout: 0.25  # default, will be overridden by Optuna
  num_parameters: 1200000
dataset:
  name: cifar10
  train_split: 45000
  val_split: 5000
  test_split: 10000
  transforms:
    - RandomCrop:
        size: 32
        padding: 4
    - RandomHorizontalFlip:
        p: 0.5
    - ToTensor: {}
    - Normalize:
        mean: [0.4914, 0.4822, 0.4465]
        std:  [0.2023, 0.1994, 0.2010]
training:
  epochs: 200
  optimizer: sgd
  momentum: 0.9
  weight_decay: 5e-4
  learning_rate: 0.01   # initial guess, tuned by Optuna
  batch_size: 64        # initial guess, tuned by Optuna
  lr_schedule: cosine
  checkpoint_interval_epochs: 1
resources:
  gpu_type: A100
  gpus_per_trial: 1
  time_budget_hours: 8
optuna:
  n_trials: 60
  sampler: tpe
  direction: maximize
  pruner: median
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    dropout:
      type: uniform
      low: 0.0
      high: 0.5

  ```
  

- Run ID: comparative-1-Small-CNN-1.2M-CIFAR-10
  Method: comparative-1
  Model: Small-CNN-1.2M
  Dataset: CIFAR-10
  Config File: config/run/comparative-1-Small-CNN-1.2M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-Small-CNN-1.2M-CIFAR-10
method:
  name: BOIL
  type: comparative
  surrogate:
    type: gaussian_process
    kernel: matern52
    noise: 1e-3
  acquisition_function: expected_improvement
  seeds: [0, 1, 2, 3, 4]
model:
  name: Small-CNN-1.2M
  conv_layers:
    - out_channels: 64
      kernel_size: 3
      stride: 1
      padding: 1
    - out_channels: 128
      kernel_size: 3
      stride: 1
      padding: 1
    - out_channels: 256
      kernel_size: 3
      stride: 1
      padding: 1
    - out_channels: 256
      kernel_size: 3
      stride: 1
      padding: 1
  fc_layers:
    - out_features: 512
  activation: relu
  dropout: 0.25  # default, will be overridden by Optuna
  num_parameters: 1200000
dataset:
  name: cifar10
  train_split: 45000
  val_split: 5000
  test_split: 10000
  transforms:
    - RandomCrop:
        size: 32
        padding: 4
    - RandomHorizontalFlip:
        p: 0.5
    - ToTensor: {}
    - Normalize:
        mean: [0.4914, 0.4822, 0.4465]
        std:  [0.2023, 0.1994, 0.2010]
training:
  epochs: 200
  optimizer: sgd
  momentum: 0.9
  weight_decay: 5e-4
  learning_rate: 0.01   # initial guess, tuned by Optuna
  batch_size: 64        # initial guess, tuned by Optuna
  lr_schedule: cosine
  checkpoint_interval_epochs: 1
resources:
  gpu_type: A100
  gpus_per_trial: 1
  time_budget_hours: 8
optuna:
  n_trials: 60
  sampler: tpe
  direction: maximize
  pruner: median
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    dropout:
      type: uniform
      low: 0.0
      high: 0.5

  ```
  


# External Resources (Use these for implementation)

**HuggingFace Models:**


**HuggingFace Datasets:**

- ID: uoft-cs/cifar10




Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "\"\"\"src/train.py\nSingle-run training / hyper-parameter optimisation with Hydra, Optuna & WandB.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nimport time\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nimport hydra\nimport numpy as np\nimport optuna\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import OmegaConf\nfrom sklearn.metrics import confusion_matrix\n\nfrom .model import build_model\nfrom .preprocess import get_dataloaders\n\nimport wandb  # type: ignore\n\n# -----------------------------------------------------------------------------\n# Helper utilities\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int) -> None:\n    \"\"\"Set all relevant random seeds for reproducibility.\"\"\"\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -> float:\n    \"\"\"Return top-1 accuracy for a mini-batch.\"\"\"\n    return (pred.argmax(dim=1) == target).float().mean().item()\n\n\n# -----------------------------------------------------------------------------\n# Cost-aware compression helpers (BOIL-C)\n# -----------------------------------------------------------------------------\n\ndef sigmoid_score(metric: float, m0: float = 0.5, g0: float = 0.1) -> float:\n    return 1.0 / (1.0 + np.exp(-(metric - m0) / g0))\n\n\ndef compress_curve(score: float, cumulative_cost: float, beta: float) -> float:\n    \"\"\"Equation (BOIL-C): penalise by compute cost (seconds).\"\"\"\n    return score - beta * np.log1p(cumulative_cost)\n\n\n# -----------------------------------------------------------------------------\n# Epoch-level routines\n# -----------------------------------------------------------------------------\n\ndef _train_one_epoch(\n    model: nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    criterion: nn.Module,\n    optimiser: optim.Optimizer,\n    device: torch.device,\n    epoch: int,\n    cfg,\n    wdb_run,\n) -> Tuple[float, float]:\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    num_samples = 0\n    for batch_idx, (x, y) in enumerate(dataloader):\n        if cfg.mode == \"trial\" and batch_idx >= cfg.trial_limited_batches:\n            break\n        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n        optimiser.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimiser.step()\n\n        bs = x.size(0)\n        running_loss += loss.item() * bs\n        running_acc += accuracy(out.detach(), y) * bs\n        num_samples += bs\n\n    epoch_loss = running_loss / num_samples\n    epoch_acc = running_acc / num_samples\n    if wdb_run is not None:\n        wdb_run.log({\"train_loss\": epoch_loss, \"train_acc\": epoch_acc, \"epoch\": epoch})\n    return epoch_loss, epoch_acc\n\n\ndef _eval_split(\n    model: nn.Module,\n    dataloader: torch.utils.data.DataLoader,\n    criterion: nn.Module,\n    device: torch.device,\n    epoch: int,\n    split: str,\n    wdb_run,\n) -> Tuple[float, float]:\n    model.eval()\n    loss_sum = 0.0\n    acc_sum = 0.0\n    num_samples = 0\n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            bs = x.size(0)\n            loss_sum += loss.item() * bs\n            acc_sum += accuracy(out, y) * bs\n            num_samples += bs\n    loss_avg = loss_sum / num_samples\n    acc_avg = acc_sum / num_samples\n    if wdb_run is not None:\n        wdb_run.log({f\"{split}_loss\": loss_avg, f\"{split}_acc\": acc_avg, \"epoch\": epoch})\n    return loss_avg, acc_avg\n\n\n# -----------------------------------------------------------------------------\n# Optuna objective – returns best validation accuracy of a trial\n# -----------------------------------------------------------------------------\n\ndef build_objective(cfg, device):\n    beta = float(cfg.method.get(\"beta\", 0.0))\n\n    def objective(trial: optuna.Trial):\n        # Sample hyper-parameters ------------------------------------------------\n        lr = trial.suggest_float(\"learning_rate\", **cfg.optuna.search_space.learning_rate)\n        batch_size = trial.suggest_categorical(\"batch_size\", cfg.optuna.search_space.batch_size.choices)\n        dropout = trial.suggest_float(\"dropout\", **cfg.optuna.search_space.dropout)\n\n        # Data loaders ----------------------------------------------------------\n        train_loader, val_loader, _ = get_dataloaders(cfg, batch_size=batch_size)\n\n        # Model / optimiser -----------------------------------------------------\n        model = build_model(cfg, dropout).to(device)\n        crit = nn.CrossEntropyLoss()\n        opt = optim.SGD(\n            model.parameters(),\n            lr=lr,\n            momentum=cfg.training.momentum,\n            weight_decay=cfg.training.weight_decay,\n        )\n        sch = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.training.epochs)\n\n        best_val_acc = 0.0\n        start_time = time.time()\n        for ep in range(cfg.training.epochs):\n            _train_one_epoch(model, train_loader, crit, opt, device, ep, cfg, wdb_run=None)\n            _, v_acc = _eval_split(model, val_loader, crit, device, ep, \"val\", wdb_run=None)\n            sch.step()\n            best_val_acc = max(best_val_acc, v_acc)\n\n            score = sigmoid_score(v_acc)\n            scalar = (\n                compress_curve(score, time.time() - start_time, beta)\n                if cfg.method.name.lower() == \"boil-c\"\n                else score\n            )\n            trial.report(scalar, step=ep)\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n        return best_val_acc\n\n    return objective\n\n\n# -----------------------------------------------------------------------------\n# Hydra entry-point\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg):  # type: ignore\n    # ------------------------------------------------------------------\n    # Mode switch (trial / full)\n    # ------------------------------------------------------------------\n    if cfg.mode not in (\"trial\", \"full\"):\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    else:\n        cfg.wandb.mode = \"online\"\n\n    # ------------------------------------------------------------------\n    # Directories & config snapshot\n    # ------------------------------------------------------------------\n    results_dir = Path(cfg.results_dir).expanduser().resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n    OmegaConf.save(cfg, str(results_dir / \"config.yaml\"))\n\n    # ------------------------------------------------------------------\n    # WandB session ----------------------------------------------------\n    # ------------------------------------------------------------------\n    wdb_run = None\n    if cfg.wandb.mode != \"disabled\":\n        wdb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            dir=str(results_dir),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n            config=OmegaConf.to_container(cfg, resolve=True),\n        )\n        print(f\"WandB URL: {wdb_run.url}\")\n\n    # ------------------------------------------------------------------\n    # Reproducibility & device ----------------------------------------\n    # ------------------------------------------------------------------\n    set_seed(cfg.method.seeds[0] if \"seeds\" in cfg.method else 0)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Data loaders (initial hyper-params from cfg) ---------------------\n    # ------------------------------------------------------------------\n    train_loader_full, val_loader_full, test_loader = get_dataloaders(cfg)\n\n    # ------------------------------------------------------------------\n    # Optuna hyper-parameter tuning -----------------------------------\n    # ------------------------------------------------------------------\n    best_params = {\n        \"learning_rate\": float(cfg.training.learning_rate),\n        \"batch_size\": int(cfg.training.batch_size),\n        \"dropout\": float(cfg.model.dropout),\n    }\n\n    if int(cfg.optuna.n_trials) > 0:\n        study = optuna.create_study(\n            direction=cfg.optuna.direction,\n            sampler=optuna.samplers.TPESampler(seed=0),\n            pruner=optuna.pruners.MedianPruner(),\n        )\n        study.optimize(build_objective(cfg, device), n_trials=int(cfg.optuna.n_trials))\n        best_params.update(study.best_params)\n        if wdb_run is not None:\n            wdb_run.summary.update({f\"optuna/best_{k}\": v for k, v in study.best_params.items()})\n\n    # ------------------------------------------------------------------\n    # Final training with best hyper-params ----------------------------\n    # ------------------------------------------------------------------\n    train_loader, val_loader, _ = get_dataloaders(cfg, batch_size=int(best_params[\"batch_size\"]))\n    model = build_model(cfg, dropout=float(best_params[\"dropout\"])).to(device)\n    crit = nn.CrossEntropyLoss()\n    opt = optim.SGD(\n        model.parameters(),\n        lr=float(best_params[\"learning_rate\"]),\n        momentum=cfg.training.momentum,\n        weight_decay=cfg.training.weight_decay,\n    )\n    sch = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.training.epochs)\n\n    best_val_acc = 0.0\n    for ep in range(cfg.training.epochs):\n        _train_one_epoch(model, train_loader, crit, opt, device, ep, cfg, wdb_run)\n        _, v_acc = _eval_split(model, val_loader, crit, device, ep, \"val\", wdb_run)\n        best_val_acc = max(best_val_acc, v_acc)\n        sch.step()\n\n    # ------------------------------------------------------------------\n    # Final test evaluation & confusion matrix -------------------------\n    # ------------------------------------------------------------------\n    test_loss, test_acc = _eval_split(model, test_loader, crit, device, cfg.training.epochs, \"test\", wdb_run)\n\n    all_preds: List[int] = []\n    all_targets: List[int] = []\n    model.eval()\n    with torch.no_grad():\n        for x, y in test_loader:\n            x = x.to(device)\n            preds = model(x).argmax(dim=1)\n            all_preds.extend(preds.cpu().tolist())\n            all_targets.extend(y.tolist())\n    cm = confusion_matrix(all_targets, all_preds)\n    class_names = test_loader.dataset.classes  # type: ignore[attr-defined]\n\n    if wdb_run is not None:\n        cm_plot = wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=class_names,\n        )\n        wdb_run.log({\"confusion_matrix\": cm_plot})\n        wdb_run.summary.update(\n            {\n                \"best_val_acc\": best_val_acc,\n                \"final_test_loss\": test_loss,\n                \"final_test_acc\": test_acc,\n                \"confusion_matrix\": cm.tolist(),\n            }\n        )\n        wdb_run.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"src/evaluate.py\nIndependent evaluation / visualisation.\nFetches metrics from WandB and writes per-run & aggregated artefacts.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb  # type: ignore\nfrom scipy import stats\n\nsns.set(style=\"whitegrid\")\n\n# -----------------------------------------------------------------------------\n# Figure helpers\n# -----------------------------------------------------------------------------\n\ndef _plot_learning_curve(path: Path, history: pd.DataFrame, run_id: str) -> str:\n    plt.figure(figsize=(6, 4))\n    if \"train_acc\" in history:\n        sns.lineplot(x=history[\"epoch\"], y=history[\"train_acc\"], label=\"train_acc\")\n    if \"val_acc\" in history:\n        sns.lineplot(x=history[\"epoch\"], y=history[\"val_acc\"], label=\"val_acc\")\n    if \"test_acc\" in history:\n        sns.lineplot(x=history[\"epoch\"], y=history[\"test_acc\"], label=\"test_acc\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.tight_layout()\n    fname = f\"{run_id}_learning_curve.pdf\"\n    plt.savefig(path / fname)\n    plt.close()\n    return str(path / fname)\n\n\ndef _plot_confusion_matrix(cm: np.ndarray, classes: List[str], path: Path, run_id: str) -> str:\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.title(\"Confusion Matrix\")\n    plt.tight_layout()\n    fname = f\"{run_id}_confusion_matrix.pdf\"\n    plt.savefig(path / fname)\n    plt.close()\n    return str(path / fname)\n\n\ndef _bar_chart(path: Path, metric_dict: Dict[str, float], metric_name: str) -> str:\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=list(metric_dict.keys()), y=list(metric_dict.values()))\n    for i, v in enumerate(metric_dict.values()):\n        plt.text(i, v + (0.01 if metric_name != \"relative_improvement\" else 0.001), f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.ylabel(metric_name)\n    plt.tight_layout()\n    fname = f\"comparison_{metric_name}_bar_chart.pdf\"\n    plt.savefig(path / fname)\n    plt.close()\n    return str(path / fname)\n\n\ndef _box_plot(path: Path, metric_dict: Dict[str, float], metric_name: str) -> str:\n    plt.figure(figsize=(6, 4))\n    df = pd.DataFrame({\"run\": list(metric_dict.keys()), metric_name: list(metric_dict.values())})\n    sns.boxplot(x=\"run\", y=metric_name, data=df)\n    plt.tight_layout()\n    fname = f\"comparison_{metric_name}_boxplot.pdf\"\n    plt.savefig(path / fname)\n    plt.close()\n    return str(path / fname)\n\n# -----------------------------------------------------------------------------\n# Main evaluation pipeline\n# -----------------------------------------------------------------------------\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Evaluate WandB runs & generate artefacts\")\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory containing experiment metadata\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON list of WandB run IDs\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir).expanduser().resolve()\n    run_ids: List[str] = json.loads(args.run_ids)\n\n    # ---------------------------------------------------------------------\n    # Load global WandB project/entity information\n    # ---------------------------------------------------------------------\n    import yaml  # local import to avoid mandatory dependency when not needed\n\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(f\"{cfg_path} not found – cannot resolve WandB project info\")\n    with cfg_path.open() as f:\n        global_cfg = yaml.safe_load(f)\n    entity = global_cfg[\"wandb\"][\"entity\"]\n    project = global_cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    per_run_metric: Dict[str, float] = {}\n    run_configs: Dict[str, dict] = {}\n    generated_files: List[str] = []\n\n    # ---------------------------------------------------------------------\n    # STEP 1: Per-run processing\n    # ---------------------------------------------------------------------\n    for rid in run_ids:\n        print(f\"\\nProcessing run {rid}\")\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        history = run.history(keys=[\"train_acc\", \"val_acc\", \"test_acc\", \"epoch\"], pandas=True)\n        summary = dict(run.summary._json_dict)\n        config = dict(run.config)\n        run_configs[rid] = config\n\n        run_dir = results_dir / rid\n        run_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save metrics ----------------------------------------------------\n        metrics_out = {\n            \"history\": history.to_dict(orient=\"list\"),\n            \"summary\": summary,\n            \"config\": config,\n        }\n        with (run_dir / \"metrics.json\").open(\"w\") as f:\n            json.dump(metrics_out, f, indent=2)\n\n        # Figures ---------------------------------------------------------\n        lc_fig = _plot_learning_curve(run_dir, history, rid)\n        generated_files.append(lc_fig)\n\n        if \"confusion_matrix\" in summary:\n            cm_arr = np.array(summary[\"confusion_matrix\"])  # type: ignore[arg-type]\n            class_names = config.get(\"dataset\", {}).get(\"class_names\", []) or [str(i) for i in range(cm_arr.shape[0])]\n            cm_fig = _plot_confusion_matrix(cm_arr, class_names, run_dir, rid)\n            generated_files.append(cm_fig)\n\n        # Collect metric for aggregation ---------------------------------\n        metric_value = summary.get(\"final_test_acc\", summary.get(\"best_val_acc\", 0.0))\n        per_run_metric[rid] = float(metric_value)\n\n    # ---------------------------------------------------------------------\n    # STEP 2: Aggregated analysis\n    # ---------------------------------------------------------------------\n    comp_dir = results_dir / \"comparison\"\n    comp_dir.mkdir(parents=True, exist_ok=True)\n\n    # Determine baseline (first non-proposed or first run if none) -------\n    baseline_id = None\n    for rid in run_ids:\n        mtype = run_configs[rid].get(\"method\", {}).get(\"type\", \"\").lower()\n        if mtype in {\"baseline\", \"comparative\", \"comparative-1\", \"comparative_1\"}:\n            baseline_id = rid\n            break\n    if baseline_id is None:\n        baseline_id = run_ids[0]\n    baseline_value = per_run_metric[baseline_id]\n\n    # Compute derived metrics -------------------------------------------\n    absolute_diff: Dict[str, float] = {}\n    relative_improvement: Dict[str, float] = {}\n    for rid, value in per_run_metric.items():\n        abs_diff = value - baseline_value\n        rel_imp = abs_diff / baseline_value if baseline_value != 0 else float(\"nan\")\n        absolute_diff[rid] = abs_diff\n        relative_improvement[rid] = rel_imp\n\n    # Combine & save aggregated metrics ---------------------------------\n    aggregated = {\n        \"per_run\": per_run_metric,\n        \"baseline_id\": baseline_id,\n        \"baseline_value\": baseline_value,\n        \"absolute_difference\": absolute_diff,\n        \"relative_improvement\": relative_improvement,\n    }\n\n    with (comp_dir / \"aggregated_metrics.json\").open(\"w\") as f:\n        json.dump(aggregated, f, indent=2)\n\n    # Visualisations -----------------------------------------------------\n    bar_fig_acc = _bar_chart(comp_dir, per_run_metric, \"accuracy\")\n    box_fig_acc = _box_plot(comp_dir, per_run_metric, \"accuracy\")\n    bar_fig_rel = _bar_chart(comp_dir, relative_improvement, \"relative_improvement\")\n\n    generated_files.extend([bar_fig_acc, box_fig_acc, bar_fig_rel])\n\n    # Statistical significance tests (vs baseline) -----------------------\n    sig_results = {}\n    for k, v in per_run_metric.items():\n        if k == baseline_id:\n            continue\n        # With single observations bootstrap w/1000 resamples could be used; simple Welch t-test here\n        try:\n            t_stat, p_val = stats.ttest_ind([baseline_value], [v], equal_var=False)\n        except Exception:\n            t_stat, p_val = float(\"nan\"), float(\"nan\")\n        sig_results[k] = {\"t_stat\": t_stat, \"p_val\": p_val}\n\n    with (comp_dir / \"significance_tests.json\").open(\"w\") as f:\n        json.dump(sig_results, f, indent=2)\n\n    # ---------------------------------------------------------------------\n    # Report paths of generated artefacts\n    # ---------------------------------------------------------------------\n    print(\"\\nGenerated files:\")\n    for fp in generated_files:\n        print(fp)\n    print(comp_dir / \"aggregated_metrics.json\")\n    print(comp_dir / \"significance_tests.json\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"src/preprocess.py\nData loading & augmentation for CIFAR-10 (cached to .cache/).\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import List, Optional, Tuple\n\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import CIFAR10\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Transform builder\n# -----------------------------------------------------------------------------\n\ndef _build_transforms(transform_cfgs: List[dict]):\n    tfms: List[T.transforms.Compose] = []\n    for cfg in transform_cfgs:\n        if not isinstance(cfg, dict):\n            raise ValueError(\"Each transform entry must be a dict {Name: params}\")\n        name, params = list(cfg.items())[0]\n        params = params or {}\n        cls = getattr(T, name)\n        tfms.append(cls(**params))\n    return T.Compose(tfms)\n\n\n# -----------------------------------------------------------------------------\n# Public loader factory\n# -----------------------------------------------------------------------------\n\ndef get_dataloaders(cfg, batch_size: Optional[int] = None) -> Tuple[DataLoader, DataLoader, DataLoader]:\n    bs = batch_size or int(cfg.training.batch_size)\n\n    transform_train = _build_transforms(cfg.dataset.transforms)\n    # Validation/test – keep only basic preprocessing (ToTensor + Normalize)\n    basic = [d for d in cfg.dataset.transforms if list(d.keys())[0] in (\"ToTensor\", \"Normalize\")]\n    transform_val = _build_transforms(basic)\n\n    full_train = CIFAR10(root=CACHE_DIR, train=True, download=True, transform=transform_train)\n    test_ds = CIFAR10(root=CACHE_DIR, train=False, download=True, transform=transform_val)\n\n    val_size = int(cfg.dataset.val_split)\n    train_size = len(full_train) - val_size\n    train_ds, val_ds = random_split(full_train, [train_size, val_size])\n\n    loader_kwargs = dict(batch_size=bs, num_workers=4, pin_memory=True)\n    train_loader = DataLoader(train_ds, shuffle=True, **loader_kwargs)\n    val_loader = DataLoader(val_ds, shuffle=False, **loader_kwargs)\n    test_loader = DataLoader(test_ds, shuffle=False, **loader_kwargs)\n    return train_loader, val_loader, test_loader\n",
    "model_py": "\"\"\"src/model.py\nConvolutional network architectures used in the experiments.\n\"\"\"\nfrom __future__ import annotations\n\nimport torch\nimport torch.nn as nn\n\n\ndef _conv_block(in_c: int, out_c: int, k: int, s: int, p: int) -> nn.Sequential:\n    \"\"\"Helper to create Conv-BN-ReLU block.\"\"\"\n    return nn.Sequential(\n        nn.Conv2d(in_c, out_c, k, stride=s, padding=p, bias=False),\n        nn.BatchNorm2d(out_c),\n        nn.ReLU(inplace=True),\n    )\n\n\nclass SmallCNN(nn.Module):\n    \"\"\"Configurable small CNN (~1.2 M parameters).\"\"\"\n\n    def __init__(self, cfg, dropout: float):\n        super().__init__()\n        layers_cfg = cfg.model.conv_layers\n        layers = []\n        in_ch = 3\n        for idx, conv_cfg in enumerate(layers_cfg):\n            layers.append(\n                _conv_block(\n                    in_c=in_ch,\n                    out_c=int(conv_cfg.out_channels),\n                    k=int(conv_cfg.kernel_size),\n                    s=int(conv_cfg.stride),\n                    p=int(conv_cfg.padding),\n                )\n            )\n            in_ch = int(conv_cfg.out_channels)\n            if (idx + 1) % 2 == 0:\n                layers.append(nn.MaxPool2d(2))\n        self.features = nn.Sequential(*layers)\n\n        # Compute flattened feature dimension ----------------------------\n        with torch.no_grad():\n            dummy = torch.zeros(1, 3, 32, 32)\n            feat_dim = self.features(dummy).view(1, -1).size(1)\n\n        fc_layers = []\n        last = feat_dim\n        for fc_cfg in cfg.model.fc_layers:\n            fc_layers.extend(\n                [\n                    nn.Linear(last, int(fc_cfg.out_features)),\n                    nn.ReLU(inplace=True),\n                    nn.Dropout(p=dropout),\n                ]\n            )\n            last = int(fc_cfg.out_features)\n        fc_layers.append(nn.Linear(last, 10))  # CIFAR-10 has 10 classes\n        self.classifier = nn.Sequential(*fc_layers)\n\n    def forward(self, x):  # type: ignore[override]\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x)\n\n\ndef build_model(cfg, dropout: float):\n    name = cfg.model.name.lower()\n    if name.startswith(\"small-cnn\"):\n        return SmallCNN(cfg, dropout)\n    raise ValueError(f\"Unknown model {cfg.model.name}\")\n",
    "main_py": "\"\"\"src/main.py\nTop-level orchestrator – spawns the actual training run in a subprocess so that\nGitHub Actions (or any scheduler) can launch many runs independently.\n\"\"\"\nfrom __future__ import annotations\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom hydra.utils import get_original_cwd\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg):  # type: ignore\n    # Validate mode ------------------------------------------------------------\n    if cfg.mode not in (\"trial\", \"full\"):\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n\n    # Prepare subprocess command ----------------------------------------------\n    original_cwd = Path(get_original_cwd())\n\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run.run_id}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n\n    print(\"Launching training subprocess:\\n\", \" \".join(cmd))\n    subprocess.run(cmd, cwd=original_cwd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"boil_c_experiments\"\nversion = \"0.1.0\"\ndescription = \"Cost-Aware BOIL Experiments with Hydra\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\nhydra-core = \"^1.3.2\"\nomegaconf = \"^2.3.0\"\ntorch = \"^2.1.0\"\ntorchvision = \"^0.16.0\"\nwandb = \"^0.16.0\"\noptuna = \"^3.4.0\"\nseaborn = \"^0.13.0\"\nmatplotlib = \"^3.8.0\"\npandas = \"^2.1.0\"\nscikit-learn = \"^1.3.0\"\nscipy = \"^1.11.0\"\n",
    "config_yaml": "# config/config.yaml\n# Global Hydra configuration.  Each experiment must supply `run=<run_id>` on\n# the command-line so that the corresponding file in config/run/<run_id>.yaml\n# is loaded.\n\ndefaults:\n  - _self_\n  - run: ???   # mandatory override – points to config/run/<run_id>.yaml\n\n# ---------------------------------------------------------------------------\n# Global settings (can be overridden from CLI)\n# ---------------------------------------------------------------------------\nmode: full         # full | trial\nresults_dir: ./results\ntrial_limited_batches: 2  # how many batches to run in trial mode\n\nwandb:\n  entity: gengaru617-personal\n  project: 251023-test\n  mode: online   # switched to \"disabled\" automatically when mode=trial\n\n# Empty placeholders so OmegaConf structure is defined before composition\nrun: {}\nmethod: {}\nmodel: {}\ndataset: {}\ntraining: {}\noptuna: {}\nresources: {}\n"
}
