
LLM Name: o3-2025-04-16
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training (full): `uv run python -u -m src.main run={run_id} results_dir={path} mode=full`
     * Training (trial): `uv run python -u -m src.main run={run_id} results_dir={path} mode=trial`
     * Evaluation: `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'` (independent execution)
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics comprehensively:
     * Use `wandb.log()` at each training step/batch/epoch with ALL relevant time-series metrics
     * Log as frequently as possible (per-batch or per-epoch) to capture complete training dynamics
     * Use `wandb.summary["key"] = value` to save final/best metrics (best_val_acc, final_test_acc, best_epoch, etc.)
   - Optuna Integration: If using Optuna, DO NOT log intermediate trial results to WandB - only log the final run with best hyperparameters
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"` before any WandB operations
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` before any WandB operations
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)
   - `WANDB_API_KEY` environment variable is available for authentication

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/run/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
   - Accepts `run_ids` parameter as JSON string list (parse with `json.loads(args.run_ids)`)
   - main.py DOES NOT call evaluate.py
   - evaluate.py retrieves comprehensive data from WandB API:
     * Use `wandb.Api()` to get run data: `run = api.run(f"{entity}/{project}/{run_id}")`
     * Retrieve: `history = run.history()`, `summary = run.summary._json_dict`, `config = dict(run.config)`
   - **STEP 1: Per-Run Processing** (for each run_id):
     * Export comprehensive run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
     * Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
     * Each run should have its own subdirectory with its metrics and figures
   - **STEP 2: Aggregated Analysis** (after processing all runs):
     * Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
     * Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
     * Generate comparison figures to: `{results_dir}/comparison/`
     * Cross-run comparison charts (bar charts, box plots)
     * Performance metrics tables
     * Statistical significance tests
   - Proper figure quality: legends, annotations, tight_layout
   - Follows GLOBALLY UNIQUE naming convention to prevent collisions:
     * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
     * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Mode-Based Implementation**:
   - `mode` parameter controls experiment behavior (required parameter)
   - When `cfg.mode == "trial"`:
     * Properly reduces computational load: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
     * Automatically sets `cfg.wandb.mode = "disabled"`
     * Purpose: Fast validation that code runs without errors
   - When `cfg.mode == "full"`:
     * Automatically sets `cfg.wandb.mode = "online"`
     * Uses full configuration (full epochs, full Optuna trials, etc.)

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
{
    "Open Problems": "BOIL transforms every partial learning curve into a single scalar via a fixed-shape sigmoid. The score is independent of how much compute was spent to obtain the curve: a run that reaches 90% accuracy after 200 epochs receives the same utility as one that reaches 90% in 20 epochs. Consequently BOIL may keep sampling hyper-parameters that learn slowly but ultimately perform well, wasting wall-clock time.",
    "Methods": "Cost–Aware Learning-Curve Compression (BOIL-C).\nModification (one line change in the compression routine):\n    u(x,t) = s( r(x,t); m0,g0 )  –  β · log( 1 + C(x,t) )\nwhere\n• s(·) is BOIL’s original sigmoid compression,  \n• C(x,t)=∑_{i=1}^{t} c(x,i) is the cumulative observed training cost (in seconds),\n• β∈[0,1] is a small constant or learned alongside m0,g0 by marginal-likelihood maximisation.\n\nInterpretation: we keep BOIL’s performance-based score but subtract a logarithmic penalty that grows with consumed compute, favouring hyper-params that reach good scores quickly.  Only the single scalar fed to the GP changes; the surrogate, acquisition function and optimisation loop are untouched.",
    "Experimental Setup": "Datasets: CIFAR-10 image classification with a small CNN; CartPole-v0 reinforcement learning with DQN (same as BOIL).\nHyper-parameters to tune: learning-rate, batch-size, and dropout for CNN; lr and target-update for DQN.\nMethods compared:\n1) BOIL (original)\n2) BOIL-C (ours, β=0.25)\n3) Hyperband (strong cost-aware baseline)\nBudget: 8 GPU hours per method, 5 independent seeds.\nMetric: best validation accuracy (CNN) / average return (RL) reached versus wall-clock time.  Report area-under-curve (AUC) of best-so-far metric w.r.t. time.",
    "Experimental Code": "# --- key modification only ----------------------------------------------------\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# inside BOIL class, replace original compression -----------------------------\n\ndef compress_curve(sigmoid_score, cumulative_cost, beta=0.25):\n    \"\"\"Return cost-aware scalar for GP.  Inputs are scalars.\"\"\"\n    return sigmoid_score - beta * np.log1p(cumulative_cost)\n\n# example usage ---------------------------------------------------------------\n# r_t: current accuracy at epoch t, m0,g0 learned as in BOIL\nsigmoid_score = 1 / (1 + np.exp(-(r_t - m0)/g0))\nscalar_for_gp = compress_curve(sigmoid_score, cumulative_cost)\n# everything else in BOIL (GP update, acquisition, etc.) stays unchanged.",
    "Expected Result": "Across both tasks BOIL-C achieves the same final accuracy/return as BOIL but reaches it 30-40% faster in wall-clock time.  The AUC-time metric improves by ≈25% over BOIL and is on par or slightly better than Hyperband, while requiring far fewer total runs.",
    "Expected Conclusion": "Penalising training cost directly in the learning-curve compression gives BOIL the missing notion of time-efficiency with just one extra term.  The change is trivial to implement (one extra subtraction) yet shifts the search toward hyper-parameters that learn quickly, saving compute without sacrificing quality.  This demonstrates how a minimal, well-motivated modification can translate into meaningful practical gains for hyper-parameter optimisation."
}

# Experimental Design
- Strategy: Purpose: Demonstrate that the proposed Cost-Aware Learning-Curve Compression (BOIL-C) accelerates Bayesian Optimisation for Hyper-parameter Tuning without hurting final performance.

Components & workflow:
1. Task: Image classification on CIFAR-10 with a 4-layer convolutional neural network (~1.2 M params).
2. Hyper-parameter search space: learning-rate, batch-size, dropout.
3. Methods: (a) BOIL-C (proposed) – modifies BOIL’s scalar learning-curve compression with a log-cost penalty; (b) BOIL (original) – comparative baseline.
4. Each optimiser receives an identical budget of 8 physical GPU-hours and is run with 5 independent random seeds. During the search every partial training curve of the CNN is compressed to a scalar and fed to a Gaussian-process surrogate; the acquisition function chooses the next configuration.
5. Hardware: single NVIDIA A100; multiple seeds run in parallel across the 8 available GPUs to exhaust the budget efficiently.
6. Logging: For every wall-clock second we record the best-so-far validation accuracy; these traces are later integrated to obtain the AUC-Time metric.
7. Evaluation: Compare (i) AUC of best validation-accuracy versus time, and (ii) final validation accuracy at budget exhaustion.

Overall, the experiment quantifies how much faster BOIL-C reaches high accuracy relative to BOIL while maintaining the same final score.
- Proposed Method: BOIL-C augments BOIL’s learning-curve compression with a cost term. For a run x after observing t training checkpoints we compute:
    u(x,t) = s(r(x,t); m0,g0) – β · log(1 + C(x,t))
where s(·) is BOIL’s sigmoid of the current validation accuracy r(x,t), C(x,t) is the cumulative training cost in seconds, and β∈[0,1] (fixed to 0.25 in the main experiment). Only this scalar fed to the Gaussian-process surrogate changes; GP training, acquisition optimisation (e.g., Expected Improvement), and the outer BO loop remain untouched. The subtraction biases the search toward configurations that obtain high accuracy rapidly, thereby reducing wall-clock time consumption. Implementation requires adding one line in the compression routine:
    scalar = sigmoid_score – beta * np.log1p(cumulative_cost)
- Evaluation Metrics: ['AUC_Time (Best Accuracy vs Wall-Clock Time)', 'Final Validation Accuracy']

# Experiment Runs

- Run ID: proposed-Small-CNN-1.2M-CIFAR-10
  Method: proposed
  Model: Small-CNN-1.2M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: proposed-Small-CNN-1.2M-CIFAR-10
method:
  name: BOIL-C
  type: proposed
  beta: 0.25
  compression_formula: "u(x,t) = s(r(x,t); m0,g0) - beta * log(1 + C(x,t))"
  surrogate:
    type: gaussian_process
    kernel: matern52
    noise: 1e-3
  acquisition_function: expected_improvement
  seeds: [0, 1, 2, 3, 4]
model:
  name: Small-CNN-1.2M
  conv_layers:
    - out_channels: 64
      kernel_size: 3
      stride: 1
      padding: 1
    - out_channels: 128
      kernel_size: 3
      stride: 1
      padding: 1
    - out_channels: 256
      kernel_size: 3
      stride: 1
      padding: 1
    - out_channels: 256
      kernel_size: 3
      stride: 1
      padding: 1
  fc_layers:
    - out_features: 512
  activation: relu
  dropout: 0.25  # default, will be overridden by Optuna
  num_parameters: 1200000
dataset:
  name: cifar10
  train_split: 45000
  val_split: 5000
  test_split: 10000
  transforms:
    - RandomCrop:
        size: 32
        padding: 4
    - RandomHorizontalFlip:
        p: 0.5
    - ToTensor: {}
    - Normalize:
        mean: [0.4914, 0.4822, 0.4465]
        std:  [0.2023, 0.1994, 0.2010]
training:
  epochs: 200
  optimizer: sgd
  momentum: 0.9
  weight_decay: 5e-4
  learning_rate: 0.01   # initial guess, tuned by Optuna
  batch_size: 64        # initial guess, tuned by Optuna
  lr_schedule: cosine
  checkpoint_interval_epochs: 1
resources:
  gpu_type: A100
  gpus_per_trial: 1
  time_budget_hours: 8
optuna:
  n_trials: 60
  sampler: tpe
  direction: maximize
  pruner: median
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    dropout:
      type: uniform
      low: 0.0
      high: 0.5

  ```
  

- Run ID: comparative-1-Small-CNN-1.2M-CIFAR-10
  Method: comparative-1
  Model: Small-CNN-1.2M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-1-Small-CNN-1.2M-CIFAR-10
method:
  name: BOIL
  type: comparative
  surrogate:
    type: gaussian_process
    kernel: matern52
    noise: 1e-3
  acquisition_function: expected_improvement
  seeds: [0, 1, 2, 3, 4]
model:
  name: Small-CNN-1.2M
  conv_layers:
    - out_channels: 64
      kernel_size: 3
      stride: 1
      padding: 1
    - out_channels: 128
      kernel_size: 3
      stride: 1
      padding: 1
    - out_channels: 256
      kernel_size: 3
      stride: 1
      padding: 1
    - out_channels: 256
      kernel_size: 3
      stride: 1
      padding: 1
  fc_layers:
    - out_features: 512
  activation: relu
  dropout: 0.25  # default, will be overridden by Optuna
  num_parameters: 1200000
dataset:
  name: cifar10
  train_split: 45000
  val_split: 5000
  test_split: 10000
  transforms:
    - RandomCrop:
        size: 32
        padding: 4
    - RandomHorizontalFlip:
        p: 0.5
    - ToTensor: {}
    - Normalize:
        mean: [0.4914, 0.4822, 0.4465]
        std:  [0.2023, 0.1994, 0.2010]
training:
  epochs: 200
  optimizer: sgd
  momentum: 0.9
  weight_decay: 5e-4
  learning_rate: 0.01   # initial guess, tuned by Optuna
  batch_size: 64        # initial guess, tuned by Optuna
  lr_schedule: cosine
  checkpoint_interval_epochs: 1
resources:
  gpu_type: A100
  gpus_per_trial: 1
  time_budget_hours: 8
optuna:
  n_trials: 60
  sampler: tpe
  direction: maximize
  pruner: median
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    dropout:
      type: uniform
      low: 0.0
      high: 0.5

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "# config/config.yaml\n# Main Hydra configuration.\n# Individual runs are provided via config/run/*.yaml and selected at runtime\n# with the CLI override `run=\u003crun_id\u003e`.\n\ndefaults:\n  - _self_\n  # run group override MUST be supplied on the command line.\n\n# -----------------------------------------------------------------------------\n# Global (overridable) settings\n# -----------------------------------------------------------------------------\nmode: full  # trial | full\nresults_dir: ./results\ntrial_limited_batches: 2\n\nwandb:\n  entity: gengaru617-personal\n  project: 251023-test\n  mode: online  # overwritten automatically when mode=trial\n\n# Empty placeholders so that OmegaConf structure is defined even before run file\nrun: {}\nmethod: {}\nmodel: {}\ndataset: {}\ntraining: {}\noptuna: {}\nresources: {}\n", "evaluate_py": "\"\"\"src/evaluate.py\nIndependent evaluation \u0026 visualisation script.\nFetches data from WandB, stores metrics/figures to results_dir.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport wandb  # type: ignore\n\nsns.set(style=\"whitegrid\")\n\n\n# -----------------------------------------------------------------------------\n# Helper functions\n# -----------------------------------------------------------------------------\n\ndef export_metrics(path: Path, history: pd.DataFrame, summary: Dict, config: Dict) -\u003e None:\n    path.mkdir(parents=True, exist_ok=True)\n    # time-series metrics\n    history.to_json(path / \"history.json\", orient=\"records\", lines=True)\n    # summary \u0026 config\n    with (path / \"summary.json\").open(\"w\") as f:\n        json.dump(summary, f, indent=2)\n    with (path / \"config.json\").open(\"w\") as f:\n        json.dump(config, f, indent=2)\n\n\ndef plot_learning_curve(path: Path, history: pd.DataFrame, run_id: str) -\u003e str:\n    plt.figure(figsize=(6, 4))\n    if \"train_acc\" in history:\n        sns.lineplot(x=history.index, y=history[\"train_acc\"], label=\"train_acc\")\n    if \"val_acc\" in history:\n        sns.lineplot(x=history.index, y=history[\"val_acc\"], label=\"val_acc\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.tight_layout()\n    fname = f\"{run_id}_learning_curve.pdf\"\n    plt.savefig(path / fname)\n    plt.close()\n    return str(path / fname)\n\n\ndef aggregated_bar_chart(path: Path, metric_dict: Dict[str, float], metric_name: str) -\u003e str:\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=list(metric_dict.keys()), y=list(metric_dict.values()))\n    for i, v in enumerate(metric_dict.values()):\n        plt.text(i, v + 0.001, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.ylabel(metric_name)\n    plt.tight_layout()\n    fname = f\"comparison_{metric_name}_bar_chart.pdf\"\n    plt.savefig(path / fname)\n    plt.close()\n    return str(path / fname)\n\n\n# -----------------------------------------------------------------------------\n# Main entry\n# -----------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Evaluate multiple WandB runs and generate figures.\")\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory to save outputs\")\n    parser.add_argument(\"run_ids\", type=str, help=\u0027JSON string list of run IDs, e.g. \"[\\\"run-1\\\", \\\"run-2\\\"]\"\u0027)\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir).expanduser().resolve()\n    run_ids: List[str] = json.loads(args.run_ids)\n\n    # Load global WandB config saved during training\n    cfg_path = results_dir / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(f\"{cfg_path} not found\")\n    import yaml  # local import to avoid hard dependency\n\n    with cfg_path.open() as f:\n        global_cfg = yaml.safe_load(f)\n\n    entity = global_cfg[\"wandb\"][\"entity\"]\n    project = global_cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    per_run_best: Dict[str, float] = {}\n\n    generated_files: List[str] = []\n\n    for run_id in run_ids:\n        print(f\"Processing run {run_id} \u2026\")\n        run = api.run(f\"{entity}/{project}/{run_id}\")\n        history = run.history(keys=[\"train_acc\", \"val_acc\", \"test_acc\", \"epoch\"], pandas=True)\n        summary = run.summary._json_dict\n        config = dict(run.config)\n\n        run_path = results_dir / run_id\n        export_metrics(run_path, history, summary, config)\n        fig_path = plot_learning_curve(run_path, history, run_id)\n        generated_files.append(fig_path)\n\n        per_run_best[run_id] = summary.get(\"best_val_acc\", summary.get(\"final_test_acc\", 0.0))\n\n    # Aggregated comparison ----------------------------------------------------\n    comparison_dir = results_dir / \"comparison\"\n    comparison_dir.mkdir(parents=True, exist_ok=True)\n    with (comparison_dir / \"aggregated_metrics.json\").open(\"w\") as f:\n        json.dump(per_run_best, f, indent=2)\n\n    bar_fig = aggregated_bar_chart(comparison_dir, per_run_best, \"best_val_acc\")\n    generated_files.append(bar_fig)\n\n    # Improvement relative to first run ---------------------------------------\n    baseline_key = run_ids[0]\n    baseline_val = per_run_best[baseline_key]\n    improvement = {k: (v - baseline_val) / baseline_val for k, v in per_run_best.items() if k != baseline_key}\n    with (comparison_dir / \"improvement_rate.json\").open(\"w\") as f:\n        json.dump(improvement, f, indent=2)\n\n    # Print generated file paths ----------------------------------------------\n    print(\"\\nGenerated files:\")\n    for fp in generated_files:\n        print(fp)\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "\"\"\"src/main.py\nTop-level orchestrator. Launches src.train in a subprocess.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import OmegaConf\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg):  # type: ignore\n    # ------------------------------------------------------------------\n    # Mode adjustments (trial / full)\n    # ------------------------------------------------------------------\n    if cfg.mode not in [\"trial\", \"full\"]:\n        raise ValueError(\"mode must be either \u0027trial\u0027 or \u0027full\u0027\")\n\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    else:\n        cfg.wandb.mode = \"online\"\n\n    # ------------------------------------------------------------------\n    # Spawn training subprocess with identical overrides\n    # ------------------------------------------------------------------\n    original_cwd = Path(get_original_cwd())\n\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run.run_id}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n\n    print(\"Launching training subprocess:\\n\", \" \".join(cmd))\n    subprocess.run(cmd, cwd=original_cwd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py\nModel definitions.\nImplements a configurable small CNN for CIFAR-10.\n\"\"\"\nfrom __future__ import annotations\n\nimport torch.nn as nn\n\n\ndef conv_block(in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n    )\n\n\nclass SmallCNN(nn.Module):\n    def __init__(self, cfg, dropout: float):\n        super().__init__()\n        layers_cfg = cfg.model.conv_layers\n\n        layers = []\n        in_ch = 3\n        for idx, layer in enumerate(layers_cfg):\n            layers.append(conv_block(in_ch, layer.out_channels, layer.kernel_size, layer.stride, layer.padding))\n            in_ch = layer.out_channels\n            if (idx + 1) % 2 == 0:  # add pooling every 2 convs\n                layers.append(nn.MaxPool2d(2))\n        self.features = nn.Sequential(*layers)\n\n        # compute flatten size with a dummy forward\n        with torch.no_grad():\n            import torch\n\n            dummy = torch.zeros(1, 3, 32, 32)\n            feat_dim = self.features(dummy).view(1, -1).size(1)\n\n        fc_layers = []\n        last_dim = feat_dim\n        for fc in cfg.model.fc_layers:\n            fc_layers.append(nn.Linear(last_dim, fc.out_features))\n            fc_layers.append(nn.ReLU(inplace=True))\n            fc_layers.append(nn.Dropout(dropout))\n            last_dim = fc.out_features\n        fc_layers.append(nn.Linear(last_dim, 10))  # CIFAR-10 \u2192 10 classes\n        self.classifier = nn.Sequential(*fc_layers)\n\n    def forward(self, x):  # type: ignore[override]\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n\ndef build_model(cfg, dropout: float):\n    name = cfg.model.name.lower()\n    if name == \"small-cnn-1.2m\":\n        return SmallCNN(cfg, dropout)\n    raise ValueError(f\"Unknown model name {cfg.model.name}\")\n", "preprocess_py": "\"\"\"src/preprocess.py\nData loading \u0026 preprocessing utilities.\nCurrently supports CIFAR-10 via torchvision.\nDatasets are cached to .cache/ by default.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Tuple\n\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import CIFAR10\n\nCACHE_DIR = \".cache/\"\n\n\n# -----------------------------------------------------------------------------\n# Build transforms from Hydra config\n# -----------------------------------------------------------------------------\n\ndef _build_transforms(transform_cfgs):\n    transform_list = []\n    for cfg in transform_cfgs:\n        if not isinstance(cfg, dict):\n            raise ValueError(\"Each transform entry must be a dict {Name: params}\")\n        name, params = list(cfg.items())[0]\n        params = params or {}\n        cls = getattr(T, name)\n        transform_list.append(cls(**params))\n    return T.Compose(transform_list)\n\n\n# -----------------------------------------------------------------------------\n# Public API\n# -----------------------------------------------------------------------------\n\ndef get_dataloaders(cfg, batch_size: int | None = None) -\u003e Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"Return train/val/test DataLoaders adhering to cfg.\n    Optionally override batch_size.\n    \"\"\"\n    bs = batch_size or cfg.training.batch_size\n\n    transform_train = _build_transforms(cfg.dataset.transforms)\n    # Validation / test: remove augmentations (only ToTensor + Normalize)\n    basic_transforms = [t for t in cfg.dataset.transforms if list(t.keys())[0] in [\"ToTensor\", \"Normalize\"]]\n    transform_val = _build_transforms(basic_transforms)\n\n    full_train_ds = CIFAR10(root=CACHE_DIR, train=True, download=True, transform=transform_train)\n    test_ds = CIFAR10(root=CACHE_DIR, train=False, download=True, transform=transform_val)\n\n    val_size = cfg.dataset.val_split\n    train_size = len(full_train_ds) - val_size\n    train_ds, val_ds = random_split(full_train_ds, [train_size, val_size])\n\n    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_ds, batch_size=bs, shuffle=False, num_workers=4, pin_memory=True)\n\n    return train_loader, val_loader, test_loader\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"boil_c_experiments\"\nversion = \"0.1.0\"\ndescription = \"Cost-Aware BOIL Experiments with Hydra\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\nhydra-core = \"^1.3.2\"\nomegaconf = \"^2.3.0\"\ntorch = \"^2.1.0\"\ntorchvision = \"^0.16.0\"\nwandb = \"^0.16.0\"\noptuna = \"^3.4.0\"\nseaborn = \"^0.13.0\"\nmatplotlib = \"^3.8.0\"\npandas = \"^2.1.0\"\nscikit-learn = \"^1.3.0\"\n", "train_py": "\"\"\"src/train.py\nSingle-run training / hyper-parameter optimisation script.\nFully integrates Hydra, Optuna and Weights-and-Biases.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport subprocess\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple\n\nimport hydra\nimport numpy as np\nimport optuna\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import OmegaConf\n\n# local imports (relative so Hydra-spawned sub-processes work regardless of cwd)\nfrom .model import build_model\nfrom .preprocess import get_dataloaders\n\ntry:\n    import wandb  # pylint: disable=import-error\nexcept ModuleNotFoundError:  # fallback in environments without WANDB\n    wandb = None  # type: ignore\n\n\n# -----------------------------------------------------------------------------\n# Utility functions\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int) -\u003e None:\n    \"\"\"Reproducibility helper.\"\"\"\n    import random\n\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef accuracy(pred: torch.Tensor, target: torch.Tensor) -\u003e float:\n    \"\"\"Top-1 accuracy.\"\"\"\n    pred_classes = pred.argmax(dim=1)\n    return (pred_classes == target).float().mean().item()\n\n\n# -----------------------------------------------------------------------------\n# BOIL / BOIL-C compression utilities\n# -----------------------------------------------------------------------------\n\ndef sigmoid_score(metric: float, m0: float = 0.5, g0: float = 0.1) -\u003e float:\n    \"\"\"Sigmoid mapping used in BOIL.\"\"\"\n    return 1.0 / (1.0 + np.exp(-(metric - m0) / g0))\n\n\ndef compress_curve(score: float, cumulative_cost: float, beta: float = 0.25) -\u003e float:\n    \"\"\"Cost-aware compression (BOIL-C).\"\"\"\n    return score - beta * np.log1p(cumulative_cost)\n\n\n# -----------------------------------------------------------------------------\n# Main optimisation / training logic\n# -----------------------------------------------------------------------------\n\ndef _train_one_epoch(model: nn.Module, dataloader: torch.utils.data.DataLoader, criterion, optimiser, device, epoch: int, cfg, wdb_run) -\u003e Tuple[float, float]:\n    model.train()\n    running_loss, running_acc = 0.0, 0.0\n    for batch_idx, (inputs, targets) in enumerate(dataloader):\n        if cfg.mode == \"trial\" and batch_idx \u003e= cfg.trial_limited_batches:\n            break  # tiny subset in trial mode\n        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n        optimiser.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimiser.step()\n\n        batch_acc = accuracy(outputs.detach(), targets)\n        running_loss += loss.item() * inputs.size(0)\n        running_acc += batch_acc * inputs.size(0)\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_acc = running_acc / len(dataloader.dataset)\n    if wdb_run is not None:\n        wdb_run.log({\"train_loss\": epoch_loss, \"train_acc\": epoch_acc, \"epoch\": epoch})\n    return epoch_loss, epoch_acc\n\n\ndef _validate(model: nn.Module, dataloader: torch.utils.data.DataLoader, criterion, device, epoch: int, split: str, wdb_run):\n    model.eval()\n    val_loss, val_acc = 0.0, 0.0\n    with torch.no_grad():\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item() * inputs.size(0)\n            val_acc += accuracy(outputs, targets) * inputs.size(0)\n    val_loss /= len(dataloader.dataset)\n    val_acc /= len(dataloader.dataset)\n    if wdb_run is not None:\n        wdb_run.log({f\"{split}_loss\": val_loss, f\"{split}_acc\": val_acc, \"epoch\": epoch})\n    return val_loss, val_acc\n\n\n# -----------------------------------------------------------------------------\n# Optuna objective\n# -----------------------------------------------------------------------------\n\ndef get_objective(cfg, device, train_loader_full, val_loader_full):\n    method_name = cfg.method.name.lower()\n    beta = cfg.method.get(\"beta\", 0.0)\n\n    def objective(trial: optuna.Trial):\n        # Sample hyper-parameters ------------------------------------------------\n        lr = trial.suggest_float(\"learning_rate\", **cfg.optuna.search_space.learning_rate)\n        batch_size = trial.suggest_categorical(\"batch_size\", cfg.optuna.search_space.batch_size.choices)\n        dropout = trial.suggest_float(\"dropout\", **cfg.optuna.search_space.dropout)\n\n        # Re-build data-loaders if batch size differs --------------------------\n        train_loader, val_loader, _ = get_dataloaders(cfg, batch_size=batch_size)\n\n        # Build \u0026 train ---------------------------------------------------------\n        model = build_model(cfg, dropout=dropout).to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimiser = optim.SGD(model.parameters(), lr=lr, momentum=cfg.training.momentum, weight_decay=cfg.training.weight_decay)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=cfg.training.epochs)\n\n        cumulative_cost = 0.0\n        best_val_metric = 0.0\n        t_start = time.time()\n        for epoch in range(cfg.training.epochs):\n            _train_one_epoch(model, train_loader, criterion, optimiser, device, epoch, cfg, wdb_run=None)\n            _, val_acc = _validate(model, val_loader, criterion, device, epoch, \"val\", wdb_run=None)\n            scheduler.step()\n            # compress ---------------------------------------------------------\n            score = sigmoid_score(val_acc)\n            cumulative_cost = time.time() - t_start  # seconds\n            if method_name == \"boil-c\":\n                scalar = compress_curve(score, cumulative_cost, beta)\n            else:  # BOIL baseline\n                scalar = score\n\n            trial.report(scalar, step=epoch)\n            best_val_metric = max(best_val_metric, val_acc)\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n        return best_val_metric  # note: Optuna direction=maximise\n\n    return objective\n\n\n# -----------------------------------------------------------------------------\n# Hydra entry point\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg) -\u003e None:  # type: ignore\n    # ---------------------------------------------------------------------\n    # Mode / trial adjustments -------------------------------------------\n    # ---------------------------------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    else:  # full\n        cfg.wandb.mode = \"online\"\n\n    # Create results directory -------------------------------------------\n    results_dir = Path(cfg.results_dir).expanduser().resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n    # Save resolved cfg so evaluation can reload -------------------------\n    with (results_dir / \"config.yaml\").open(\"w\") as f:\n        OmegaConf.save(cfg, f)\n\n    # Initialise Weights \u0026 Biases ----------------------------------------\n    wdb_run = None\n    if wandb is not None and cfg.wandb.mode != \"disabled\":\n        wdb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            dir=str(results_dir),\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n        )\n        print(f\"WandB URL: {wdb_run.url}\")\n\n    # Reproducibility -----------------------------------------------------\n    seed = int(cfg.method.seeds[0]) if \"seeds\" in cfg.method else 0\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Data ----------------------------------------------------------------\n    train_loader_full, val_loader_full, test_loader = get_dataloaders(cfg)\n\n    # Optuna hyper-parameter optimisation --------------------------------\n    best_params = {}\n    if cfg.optuna.n_trials \u003e 0:\n        study = optuna.create_study(direction=cfg.optuna.direction, sampler=optuna.samplers.TPESampler(seed=seed), pruner=optuna.pruners.MedianPruner())\n        objective = get_objective(cfg, device, train_loader_full, val_loader_full)\n        study.optimize(objective, n_trials=cfg.optuna.n_trials, timeout=int(cfg.resources.time_budget_hours * 3600))\n        best_params = study.best_params\n        if wdb_run is not None:\n            wdb_run.summary.update({f\"optuna/best_{k}\": v for k, v in best_params.items()})\n    else:\n        # use defaults from cfg.training\n        best_params = {\n            \"learning_rate\": cfg.training.learning_rate,\n            \"batch_size\": cfg.training.batch_size,\n            \"dropout\": cfg.model.dropout,\n        }\n\n    # ---------------------------------------------------------------------\n    # Final training with best hyper-parameters ---------------------------\n    # ---------------------------------------------------------------------\n    train_loader, val_loader, _ = get_dataloaders(cfg, batch_size=int(best_params[\"batch_size\"]))\n    model = build_model(cfg, dropout=float(best_params[\"dropout\"])).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimiser = optim.SGD(model.parameters(), lr=float(best_params[\"learning_rate\"]), momentum=cfg.training.momentum, weight_decay=cfg.training.weight_decay)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=cfg.training.epochs)\n\n    best_val_acc = 0.0\n    for epoch in range(cfg.training.epochs):\n        train_loss, train_acc = _train_one_epoch(model, train_loader, criterion, optimiser, device, epoch, cfg, wdb_run)\n        val_loss, val_acc = _validate(model, val_loader, criterion, device, epoch, \"val\", wdb_run)\n        scheduler.step()\n        best_val_acc = max(best_val_acc, val_acc)\n\n    # Final test evaluation ----------------------------------------------\n    test_loss, test_acc = _validate(model, test_loader, criterion, device, epoch=cfg.training.epochs, split=\"test\", wdb_run=wdb_run)\n\n    if wdb_run is not None:\n        wdb_run.summary.update({\n            \"best_val_acc\": best_val_acc,\n            \"final_test_loss\": test_loss,\n            \"final_test_acc\": test_acc,\n        })\n        wdb_run.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "Hydra configuration & evaluation script do not satisfy mandatory requirements:\n1. config/config.yaml lacks a `run` default group (e.g. `- run: ??`) so the CLI override `run=<run_id>` raises `ConfigCompositionException`, causing the whole pipeline to crash.  Because of this `cfg.run.run_id` cannot be accessed in main/train.\n2. evaluate.py only generates learning-curve figures; it omits required confusion-matrix plots, statistical tests and does not save per-run metrics under the mandatory name `{run_id}/metrics.json`.  Therefore it violates Evaluation Script Independence criteria.\n3. Minor but blocking: aggregated significance tests and comparison figures demanded by spec are missing.\n\nCode must be fixed before it can be executed in production experiments."
}
