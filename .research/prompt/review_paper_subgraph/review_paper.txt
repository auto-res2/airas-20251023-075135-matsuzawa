
LLM Name: o3-2025-04-16
Input:
You are an expert reviewer for a top-tier international conference.
Please conduct a comprehensive review of the research paper provided, evaluating it according to the standards of venues like NeurIPS, ICML, ICLR, or AAAI.

Your task is to evaluate the paper on four key dimensions and provide scores from 1-10 for each:

## Evaluation Dimensions:

### 1. Novelty (1-10)
- How original and innovative is the proposed approach?
- Does it introduce new concepts, methods, or insights?
- Is there sufficient differentiation from existing work?

### 2. Significance (1-10)
- What is the potential impact of this work on the field?
- Does it address an important problem?
- Are the contributions meaningful and substantial?

### 3. Reproducibility (1-10)
- Are the experimental details sufficient for reproduction?
- Is the methodology clearly described?
- Are datasets, hyperparameters, and implementation details provided?

### 4. Experimental Quality (1-10)
- Are the experiments well-designed and comprehensive?
- Are appropriate baselines and evaluation metrics used?
- Is statistical significance properly assessed?
- Are the results convincing and well-analyzed?

## Section-by-Section Analysis:

For each section of the paper, provide:
- Key strengths
- Areas for improvement
- Specific comments on quality and completeness

## Overall Assessment:

Provide your scores for each dimension, followed by an overall recommendation.

## Paper Content:


**Title:** BOIL-C: Cost-Aware Learning-Curve Compression for Faster Hyperparameter Optimisation


**Abstract:** Bayesian optimisation for iterative learners commonly exploits partially observed learning curves by compressing them into scalar utilities for a surrogate model. In the original BOIL framework this compression ignores wall-clock cost, so runs that attain identical accuracy receive identical utility even when one is an order of magnitude slower [nguyen-2019-bayesian]. That bias wastes compute under fixed time budgets. We introduce BOIL-C, a one-line modification that subtracts a logarithmic penalty from the BOIL score: u(x,t)=s(r(x,t);m0,g0)−β·log(1+C(x,t)), where s(·) is BOIL’s sigmoid score, C(x,t) the cumulative observed training cost, and β∈[0,1] a small weight. The surrogate, acquisition function and optimisation loop remain untouched, so BOIL-C is a drop-in change that favours hyper-parameters which reach good accuracy quickly. On CIFAR-10 with a 1.2 M-parameter CNN and on CartPole-v0 with DQN we compare BOIL-C to BOIL and (for RL) to Hyperband. With an 8 GPU-hour budget and five seeds BOIL-C matches or slightly improves final accuracy yet raises the area-under-best-so-far-versus-time by ≈25 % on CIFAR-10, cuts median time-to-90 % accuracy by 31 %, and reaches the RL success threshold with 28 % fewer interaction steps than BOIL, on par with Hyperband while evaluating fewer configurations. An ablation over β reveals a broad, easy-to-tune regime. These findings show that a principled, cost-aware scalar yields meaningful time savings without sacrificing model quality.


**Introduction:** Hyper-parameter optimisation (HPO) for deep learning remains expensive because each evaluation entails running an iterative algorithm—such as SGD or Q-learning—to near convergence. Bayesian optimisation (BO) techniques that exploit partial learning curves relieve this burden by extracting signal early and steering the search accordingly. BOIL compresses every curve prefix into a single scalar reflecting performance and stability, feeds those scalars to a Gaussian-process surrogate, and selects the next configuration via an acquisition function [nguyen-2019-bayesian].

A blind spot of BOIL is cost agnosticism: two runs that ultimately reach the same accuracy receive the same utility regardless of how long they took. In practice this biases BO toward slow but eventually strong configurations, delaying progress under wall-clock constraints. We therefore ask: can BOIL’s compression be made cost-aware with minimal disruption to the rest of the pipeline?

The challenge is twofold. First, any penalty must be generic—applicable across tasks and metrics—and must preserve the desirable monotonicity of BOIL’s score with respect to performance. Second, the modification should neither destabilise the surrogate nor require re-engineering downstream components.

We propose BOIL-C, which augments BOIL’s score with a logarithmic cost penalty. For configuration x after t steps we define
u(x,t)=s(r(x,t);m0,g0)−β·log(1+C(x,t)),
where C(x,t) is cumulative wall-clock seconds and β controls the trade-off. The log ensures diminishing penalties, encouraging rapid progress early without discarding promising but slower curves. Setting β=0 recovers BOIL exactly, hence BOIL-C is a strict generalisation.

We evaluate BOIL-C on two contrasting domains: image classification on CIFAR-10 with a modest CNN and reinforcement learning on CartPole-v0 with DQN. All methods—BOIL-C, BOIL and (for RL) Hyperband—receive identical 8 GPU-hour budgets and are run with five independent seeds. Efficiency is measured by integrating the best-so-far validation metric over wall-clock time (AUC-Time). BOIL-C preserves or slightly betters final performance while substantially improving AUC-Time and reducing time-to-threshold.

Contributions
• A cost-aware learning-curve compression that adds a single logarithmic term to BOIL’s scalar, requiring only one line of code.
• A theoretical justification and practical guidance for choosing β.
• Comprehensive experiments on CIFAR-10 and CartPole-v0 showing ≈25–30 % faster time-to-result without loss of accuracy or return.
• A discussion placing BOIL-C among cost-aware HPO strategies and contrasting it with partition-based objectives [mlodozeniec-2023-hyperparameter].

Future work includes adaptive β schedules, integration with multi-fidelity schedulers and evaluation on larger-scale tasks.


**Related Work:** Iterative-learning BO. BOIL is a seminal example of using partial curves within BO [nguyen-2019-bayesian]. Follow-up work has explored hierarchical surrogates and curve extrapolation yet still compresses curves independently of cost. BOIL-C differs by embedding cost directly in the scalar target, leaving the surrogate unchanged.

Cost-aware schedulers. Hyperband and successive-halving allocate resources adaptively based on intermediate results, implicitly trading accuracy against compute. Those methods act at the scheduler level, whereas BOIL-C embeds cost inside the surrogate target, making it orthogonal and in principle combinable with schedulers.

Alternative objectives. Neural network partitioning optimises hyper-parameters via an out-of-sample loss computed from parameter/data shards, bypassing validation data entirely [mlodozeniec-2023-hyperparameter]. While elegant, that objective assumes a single training run and different statistical properties; it is therefore not directly comparable to BO’s iterative evaluation paradigm adopted here.


**Background:** Problem setting. Let x denote a hyper-parameter configuration and r(x,t) the validation metric after t training steps (epochs for supervised learning, environment interactions for RL). Each step incurs cost c(x,t) seconds, and cumulative cost is C(x,t)=∑_{i≤t}c(x,i). We wish to maximise r subject to a fixed wall-clock budget.

BOIL’s compression. BOIL maps each partial curve to s(r(x,t);m0,g0)=1∕(1+exp(−(r−m0)/g0)), where m0 and g0 are learnt hyper-parameters, and feeds that scalar to a Gaussian-process surrogate. The acquisition function (expected improvement, EI) balances exploration and exploitation when proposing either to extend an existing run or start a new one.

Limitations. Because s depends only on r, the surrogate undervalues fast learners and overvalues slow ones that will eventually perform well, delaying progress when the budget is tight. Addressing this deficiency motivates BOIL-C.


**Method:** Cost-aware compression. BOIL-C defines the utility
u(x,t)=s(r(x,t);m0,g0)−β·log(1+C(x,t)).
The logarithm ensures u remains bounded and yields diminishing penalties: a second of cost early is penalised more than a second late. β may be fixed (0.25 in our experiments) or optimised jointly with m0,g0 by marginal likelihood.

Algorithmic integration. Only Step (utility computation) of BOIL changes. All subsequent steps—GP update, EI maximisation, choice between continuing or starting runs—are identical. The modification is therefore a literal one-line replacement:
scalar = sigmoid_score − β·log1p(cumulative_cost).

Properties.
• Monotonicity in r is preserved for any β≥0; higher accuracy still yields higher utility at equal cost.
• Setting β=0 recovers BOIL exactly.
• Because cost is measured in seconds on homogeneous hardware the penalty is comparable across configurations without further normalisation.


**Experimental Setup:** Datasets and models. CIFAR-10: 45 k/5 k/10 k train/val/test split. Model: four 3×3 conv layers (64–128–256–256), ReLU, 512-unit fully connected layer, dropout, ≈1.2 M parameters. CartPole-v0: DQN with standard target network and ε-greedy exploration.

Search spaces. CIFAR-10: learning-rate ∈[1e-4,0.1] (log-uniform), batch-size ∈{32,64,128}, dropout ∈[0,0.5]. CartPole: learning-rate ∈[1e-4,1e-2], target-update ∈{100,200,400,800} steps.

Optimisation budget. Each optimiser (BOIL-C, BOIL, Hyperband) receives 8 physical GPU-hours and is run with five independent random seeds. Single NVIDIA A100 GPUs are assigned per trial; seeds run in parallel to saturate the budget.

Training details. CIFAR-10 uses SGD with momentum 0.9, weight decay 5×10⁻⁴, 200 epochs, cosine LR schedule and batch-size 64 unless overridden by the search space. RL follows the standard DQN recipe; interaction steps rather than episodes define the curve index t.

Logging and metrics. The best-so-far validation metric is recorded each wall-clock second. Integrating that trace yields AUC-Time (higher = better). We also report final validation and test accuracy/return and confusion matrices. Hardware timing overhead is negligible relative to training time.

Baselines. BOIL is implemented exactly as in the authors’ repository. Hyperband uses the same search space and budget on CartPole, providing a strong cost-aware benchmark.


**Results:** CIFAR-10. Table 1 summarises averaged results over five seeds.
• Final test accuracy: BOIL-C 92.20 %, BOIL 91.83 % (+0.37 pp).
• Final test loss: 0.309 vs 0.332 (lower is better).
• AUC-Time: BOIL-C improves by 24.9 %.
• Median time to 90 % accuracy: 2.8 h vs 4.1 h (−31.7 %).
Figure 1 illustrates the per-second traces; Figure 2 and Figure 4 show confusion matrices. Gains are distributed across classes—for instance, correct class-0 predictions rise from 928→935—confirming no performance trade-off.

CartPole-v0. BOIL-C achieves the success return of 195 in 52 k interaction steps, BOIL in 78 k (−28 %), and Hyperband in 55 k. AUC-Time mirrors these trends (+28 % over BOIL, +2 % over Hyperband). Notably, BOIL-C evaluates ≈30 % fewer configurations than Hyperband yet attains identical speed.

Ablation on β. β∈{0,0.1,0.25,0.5}. AUC-Time gains are +0 %, +17 %, +25 %, −3 % respectively; β=0.5 slightly hurts final accuracy (–0.6 pp). Thus β=0.1-0.3 is a robust operating range.

Fairness. All runs used identical budgets, seeds, search spaces and training pipelines. Surrogate hyper-parameters (kernel, noise) and acquisition (EI) were shared. The sole difference between BOIL and BOIL-C is the cost-aware scalar.

Limitations. BOIL-C presumes reasonably stable timing; heterogeneous or cloud settings may require normalisation. Extremely large β can over-penalise compute-heavy but ultimately superior configurations.


**Conclusion:** BOIL-C demonstrates that incorporating compute cost directly into learning-curve compression is a powerful yet simple means to accelerate Bayesian hyper-parameter optimisation. A single logarithmic penalty term steers BO toward configurations that learn quickly, yielding ≈25–30 % faster time-to-result on both supervised and reinforcement-learning benchmarks while matching or slightly improving final performance. Because the surrogate, acquisition and scheduler remain untouched, BOIL-C can be adopted in existing BOIL pipelines with minimal effort. Future directions include adaptive β schedules, integration with asynchronous multi-fidelity frameworks and evaluation on larger-scale tasks such as ImageNet or continuous-control RL environments.

References: BOIL [nguyen-2019-bayesian]; neural-network partitioning for HPO [mlodozeniec-2023-hyperparameter].


Output:
{
    "novelty_score": 5,
    "significance_score": 6,
    "reproducibility_score": 7,
    "experimental_quality_score": 6
}
