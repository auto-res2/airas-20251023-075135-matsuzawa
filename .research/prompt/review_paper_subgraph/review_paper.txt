
LLM Name: o3-2025-04-16
Input:
You are an expert reviewer for a top-tier international conference.
Please conduct a comprehensive review of the research paper provided, evaluating it according to the standards of venues like NeurIPS, ICML, ICLR, or AAAI.

Your task is to evaluate the paper on four key dimensions and provide scores from 1-10 for each:

## Evaluation Dimensions:

### 1. Novelty (1-10)
- How original and innovative is the proposed approach?
- Does it introduce new concepts, methods, or insights?
- Is there sufficient differentiation from existing work?

### 2. Significance (1-10)
- What is the potential impact of this work on the field?
- Does it address an important problem?
- Are the contributions meaningful and substantial?

### 3. Reproducibility (1-10)
- Are the experimental details sufficient for reproduction?
- Is the methodology clearly described?
- Are datasets, hyperparameters, and implementation details provided?

### 4. Experimental Quality (1-10)
- Are the experiments well-designed and comprehensive?
- Are appropriate baselines and evaluation metrics used?
- Is statistical significance properly assessed?
- Are the results convincing and well-analyzed?

## Section-by-Section Analysis:

For each section of the paper, provide:
- Key strengths
- Areas for improvement
- Specific comments on quality and completeness

## Overall Assessment:

Provide your scores for each dimension, followed by an overall recommendation.

## Paper Content:


**Title:** BOIL-C: Cost-Aware Learning-Curve Compression for Faster Bayesian Hyperparameter Optimisation


**Abstract:** Bayesian Optimisation for Iterative Learning (BOIL) accelerates hyper-parameter search by feeding a Gaussian-process surrogate with single-number summaries of partial learning curves. Unfortunately these summaries ignore the wall-clock time required to reach a given accuracy, so BOIL often oversamples configurations that learn slowly yet eventually perform well, wasting compute under tight budgets. We propose BOIL-C, a one-line modification that subtracts a logarithmic cost penalty from BOIL’s sigmoid score: u(x,t)=s(r(x,t);m0,g0)−β·log(1+C(x,t)), where r is the observed performance at step t and C the cumulative training time. The surrogate, acquisition function and optimisation loop remain unchanged. Experiments on CIFAR-10 with a 1.2 M-parameter CNN and on CartPole-v0 with DQN compare BOIL-C to the original BOIL and to Hyperband under identical eight-hour GPU budgets and five seeds. BOIL-C matches or slightly improves final accuracy/return yet increases the area-under-best-so-far-score versus time by ≈25 % and reaches target quality 30–40 % faster. An ablation shows robust gains for β∈[0.15,0.40]; paired t-tests confirm significant improvements in time-efficiency without harming asymptotic performance. Thus a principled, minimal compute penalty restores time awareness to BOIL and delivers meaningful real-world savings.


**Introduction:** Hyper-parameter optimisation (HPO) is indispensable for state-of-the-art performance in deep learning and reinforcement learning, but its computational cost remains prohibitive. Bayesian Optimisation for Iterative Learning (BOIL) addresses this by compressing partial learning curves into scalars consumed by a Gaussian-process (GP) surrogate, enabling the optimiser to exploit intermediate training signals instead of waiting for full convergence [nguyen-2019-bayesian]. Despite its efficiency, BOIL’s scalar score depends solely on accuracy and stability, ignoring the wall-clock cost spent to achieve that accuracy. Consequently, BOIL may repeatedly select configurations that learn slowly but ultimately perform well, squandering precious compute when a fixed budget is imposed.

We introduce BOIL-C, a cost-aware learning-curve compression scheme that augments BOIL’s score with a logarithmic penalty on cumulative training time. The modification is minimal—a single subtraction—yet it endows the GP surrogate with an explicit preference for configurations that achieve high accuracy quickly. Designing such a penalty is challenging: if the cost term is too strong, the optimiser may discard ultimately superior but slower configurations; if too weak, search behaviour remains unchanged. BOIL-C solves this by using a sub-linear log(1+C) penalty whose magnitude is controlled by one coefficient β.

We validate BOIL-C on two representative tasks: CIFAR-10 image classification with a 1.2 M-parameter convolutional network and CartPole-v0 reinforcement learning with DQN. We compare against BOIL and the cost-aware bandit baseline Hyperband under identical eight-hour GPU budgets and five independent seeds. Performance is measured both at budget exhaustion and throughout the run via the area under the best-so-far curve versus time (AUC-Time). BOIL-C achieves the same or slightly higher final accuracy/return while delivering≈25 % higher AUC-Time and attaining target quality 30–40 % sooner.

Contributions:
• We propose BOIL-C, a cost-aware scalar compression that requires only a one-line change to BOIL.
• We present thorough experiments on vision and RL tasks showing consistent improvements in anytime performance without sacrificing final quality.
• We provide an ablation over β and statistical tests confirming the significance and robustness of the observed gains.
• BOIL-C complements broader efficiency efforts such as partition-based HPO [mlodozeniec-2023-hyperparameter] and demonstrates how minimal, principled modifications can yield tangible compute savings.


**Related Work:** Learning-curve-aware Bayesian optimisation BOIL compresses partial training trajectories into sigmoid scores that quantify accuracy and stability, allowing a GP surrogate to reason about progress and stop unpromising runs early [nguyen-2019-bayesian]. BOIL-C preserves this framework but augments the score with an explicit cost term. Alternative learning-curve models predict future accuracy directly, yet they typically require bespoke surrogates and do not inject cost at the compression stage.

Cost-aware resource allocation Hyperband and successors allocate budgets adaptively, trading early stopping against exploration through non-parametric bandit rules. These schedulers are inherently time-aware, often achieving strong anytime performance, but they forgo parametric surrogates and thus require many full or partial trainings. BOIL-C bridges this gap by embedding compute considerations inside a GP-based BO framework, combining sample efficiency with time awareness.

Alternative HPO objectives Partition-based optimisation approximates marginal likelihood by splitting data and parameters, enabling validation-free tuning [mlodozeniec-2023-hyperparameter]. Although orthogonal to compute penalties, such objectives could be combined with BOIL-C. Work on learning invariances jointly with model parameters [benton-2020-learning] illustrates the wider trend of integrating auxiliary objectives directly into training; BOIL-C follows the same philosophy for wall-clock cost.

Compared to these approaches, BOIL-C is distinguished by its simplicity: one extra term in the compression suffices to convert BOIL into a cost-aware optimiser while leaving the surrogate and acquisition untouched.


**Background:** Problem setting Given a configuration x and training step t, let r(x,t) denote the validation performance and c(x,i) the wall-clock seconds consumed by step i. The cumulative cost is C(x,t)=∑_{i=1}^{t}c(x,i). Under a fixed budget B, the optimiser iteratively (1) trains a chosen configuration, (2) compresses the partial curve to a scalar, (3) updates the GP surrogate, and (4) selects the next configuration via expected improvement.

BOIL compression The original BOIL maps r(x,t) to a sigmoid score s(r(x,t);m0,g0)=1/(1+exp(−(r−m0)/g0)), where m0 and g0 are learned by maximising GP marginal likelihood [nguyen-2019-bayesian]. This score is agnostic to elapsed time, making the optimiser blind to computational efficiency.

Design goal We seek a modified scalar that (i) rewards accuracy, (ii) penalises cost, (iii) grows smoothly, and (iv) preserves BOIL’s GP machinery. A logarithmic penalty satisfies these criteria: it is zero at zero cost, sub-linear, and numerically stable via log(1+C).


**Method:** Cost-aware compression For each partial run we compute
u(x,t)=s(r(x,t);m0,g0)−β·log(1+C(x,t)),
where β∈[0,1] weighs compute against accuracy. The term log(1+C) ensures diminishing penalisation and avoids instability at small costs.

Surrogate and acquisition The GP surrogate, Matérn-5/2 kernel, observation noise, and expected-improvement acquisition remain exactly as in BOIL. Only the target values fed to the surrogate change from s to u. Parameters m0 and g0 continue to be learned by marginal likelihood; β is fixed to 0.25 unless stated, but could also be estimated jointly.

Implementation The modification amounts to replacing one line in the compression routine (see listing in the code appendix). No other code changes are required.


**Experimental Setup:** Datasets and models (1) CIFAR-10 classification using a four-layer CNN with ≈1.2 M parameters; standard random-crop and flip augmentation, 45 k/5 k/10 k train/val/test split. (2) CartPole-v0 reinforcement learning with DQN, following the original BOIL setup.

Search spaces CIFAR-10: learning rate log-uniform in [1e-4,1e-1], batch size ∈{32,64,128}, dropout uniform in [0,0.5]. CartPole: learning rate and target-update frequency as in [nguyen-2019-bayesian].

Budgets and hardware Each optimiser (BOIL-C, BOIL, Hyperband) receives eight physical GPU-hours and is run with five independent seeds on NVIDIA A100 hardware. Seeds are distributed across available GPUs to exhaust the budget. We record the best-so-far validation accuracy (or return) every second and integrate these traces to obtain AUC-Time.

Training protocol CIFAR-10 models train for up to 200 epochs with SGD (momentum 0.9, weight decay 5×10⁻⁴) and a cosine schedule. Checkpoints are saved each epoch. Hyper-parameter trials are orchestrated by Optuna with a TPE sampler and median pruner; BOIL variants share GP hyper-parameters (Matérn-5/2 kernel, noise 10⁻³) and acquisition settings.

Evaluation metrics Primary: AUC-Time (higher is better). Secondary: final validation and test accuracy (CIFAR-10) or return (CartPole) at budget exhaustion. Fairness: identical budgets, seeds, and search spaces across methods.


**Results:** CIFAR-10 BOIL-C achieves a final test accuracy of 0.922 versus 0.9183 for BOIL. More importantly, BOIL-C improves AUC-Time by 25.8 % and reaches 90 % validation accuracy in 27.3 min versus 45.8 min for BOIL. Hyperband attains similar AUC-Time but requires roughly twice as many partial trainings.

CartPole-v0 BOIL-C matches BOIL’s final return (199.2 vs 198.8) yet delivers a 32 % higher AUC-Time and converges 30 % sooner. It slightly outperforms Hyperband while evaluating 43 % fewer full training runs.

Ablation on β Sweeping β on CIFAR-10 yields AUC-Time values of 5.9×10⁴ (β=0, BOIL), 6.8×10⁴ (β=0.15), 7.45×10⁴ (β=0.25), and 7.3×10⁴ (β=0.40), demonstrating robustness for β∈[0.15,0.40].

Statistical significance Paired t-tests over five seeds show significant AUC-Time improvements: t=5.12, p=0.003 (CIFAR-10) and t=4.41, p=0.005 (CartPole). No significant difference appears in final accuracy/return (p>0.4), confirming that BOIL-C accelerates convergence without loss of peak quality.

Limitations BOIL-C introduces one hyper-parameter β that may need tuning across domains; extremely long plateaus might benefit from alternative penalty shapes. Compute measurements are assumed reliable; excessive measurement noise could dilute the penalty.

Figures
Figure 1: Confusion matrix of the final BOIL-C CIFAR-10 model (filename: proposed-Small-CNN-1.2M-CIFAR-10_confusion_matrix.pdf). Higher diagonal entries indicate better classification.
Figure 2: Best-so-far validation accuracy versus time for BOIL-C on CIFAR-10 (filename: proposed-Small-CNN-1.2M-CIFAR-10_learning_curve.pdf). Higher curves indicate better and earlier performance.
Figure 3: Confusion matrix of the final BOIL CIFAR-10 model (filename: comparative-1-Small-CNN-1.2M-CIFAR-10_confusion_matrix.pdf). Higher diagonal entries indicate better classification.
Figure 4: Best-so-far validation accuracy versus time for BOIL on CIFAR-10 (filename: comparative-1-Small-CNN-1.2M-CIFAR-10_learning_curve.pdf). Higher curves indicate better and earlier performance.
Figure 5: Final test accuracy across methods (filename: comparison_accuracy_bar_chart.pdf). Higher bars are better.
Figure 6: Distribution of final test accuracy across seeds (filename: comparison_accuracy_boxplot.pdf). Higher boxes are better.
Figure 7: Relative AUC-Time improvement of BOIL-C over BOIL (filename: comparison_relative_improvement_bar_chart.pdf). Higher bars indicate greater efficiency gains.


**Conclusion:** We presented BOIL-C, a cost-aware extension of BOIL that subtracts a logarithmic penalty in cumulative compute from the learning-curve scalar fed to the GP surrogate. This one-line change preserves BOIL’s modelling and acquisition logic while introducing a principled preference for fast-learning configurations. Across CIFAR-10 and CartPole-v0, BOIL-C maintains or slightly improves final accuracy/return yet delivers≈25 % higher AUC-Time and 30–40 % faster time-to-target quality, outperforming the original BOIL and matching or surpassing Hyperband with far fewer trainings. These gains are statistically significant and robust across β∈[0.15,0.40].

Future work includes learning β jointly with the sigmoid parameters, exploring alternative cost penalties, and applying BOIL-C to larger parameter spaces and tasks. Because BOIL-C is orthogonal to objectives such as partition-based marginal-likelihood optimisation [mlodozeniec-2023-hyperparameter] and retains compatibility with BOIL’s GP framework [nguyen-2019-bayesian], it offers a practical drop-in upgrade for time-efficient hyper-parameter search in compute-constrained settings.


Output:
{
    "novelty_score": 5,
    "significance_score": 6,
    "reproducibility_score": 7,
    "experimental_quality_score": 6
}
